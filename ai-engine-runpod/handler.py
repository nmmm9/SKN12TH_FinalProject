"""
TtalKkak AI Engine - Runpod Serverless Handler
WhisperX (STT) + Qwen3-14B 4bit (LLM) í†µí•© ì²˜ë¦¬
"""

import runpod
import torch
import json
import base64
import tempfile
import os
import logging
from typing import Dict, Any, Optional

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Global model variables (for cold start optimization)
whisper_model = None
qwen_model = None
qwen_tokenizer = None

def load_whisperx():
    """WhisperX ëª¨ë¸ ë¡œë”©"""
    global whisper_model
    
    if whisper_model is None:
        logger.info("ğŸ¤ Loading WhisperX large-v3...")
        try:
            import whisperx
            device = "cuda" if torch.cuda.is_available() else "cpu"
            compute_type = "float16" if device == "cuda" else "int8"
            
            whisper_model = whisperx.load_model(
                "large-v3", 
                device, 
                compute_type=compute_type,
                language="ko"  # í•œêµ­ì–´ ìµœì í™”
            )
            logger.info("âœ… WhisperX loaded successfully")
            
        except Exception as e:
            logger.error(f"âŒ WhisperX loading failed: {e}")
            raise e
    
    return whisper_model

def load_qwen3():
    """Qwen3-14B 4bit ëª¨ë¸ ë¡œë”©"""
    global qwen_model, qwen_tokenizer
    
    if qwen_model is None or qwen_tokenizer is None:
        logger.info("ğŸ§  Loading Qwen3-14B 4bit...")
        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
            
            # 4ë¹„íŠ¸ ì–‘ìí™” ì„¤ì •
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
            
            model_name = "unsloth/Qwen3-14B-unsloth-bnb-4bit"
            
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            qwen_tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True
            )
            
            # ëª¨ë¸ ë¡œë“œ
            qwen_model = AutoModelForCausalLM.from_pretrained(
                model_name,
                quantization_config=bnb_config,
                device_map="auto",
                torch_dtype=torch.float16,
                trust_remote_code=True
            )
            
            logger.info("âœ… Qwen3-14B loaded successfully")
            
        except Exception as e:
            logger.error(f"âŒ Qwen3 loading failed: {e}")
            raise e
    
    return qwen_model, qwen_tokenizer

def transcribe_audio(audio_base64: str) -> Dict[str, Any]:
    """ìŒì„± ì „ì‚¬ ì²˜ë¦¬"""
    try:
        logger.info("ğŸ¤ Starting audio transcription...")
        
        # ëª¨ë¸ ë¡œë“œ
        whisper_model = load_whisperx()
        
        # Base64 ë””ì½”ë”© ë° ì„ì‹œ íŒŒì¼ ì €ì¥
        audio_data = base64.b64decode(audio_base64)
        
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_file:
            temp_file.write(audio_data)
            temp_path = temp_file.name
        
        try:
            # WhisperX ì „ì‚¬ ì‹¤í–‰
            result = whisper_model.transcribe(temp_path, batch_size=16)
            
            # ê²°ê³¼ ì²˜ë¦¬
            segments = result.get("segments", [])
            full_text = " ".join([seg.get("text", "") for seg in segments])
            
            logger.info(f"âœ… Transcription completed: {len(full_text)} characters")
            
            return {
                "success": True,
                "transcription": {
                    "segments": segments,
                    "full_text": full_text,
                    "language": result.get("language", "ko"),
                    "duration": len(segments) * 2  # ëŒ€ëµì ì¸ ì‹œê°„
                }
            }
            
        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if os.path.exists(temp_path):
                os.unlink(temp_path)
                
    except Exception as e:
        logger.error(f"âŒ Transcription error: {e}")
        return {
            "success": False,
            "error": f"Transcription failed: {str(e)}"
        }

def analyze_meeting(transcript_text: str) -> Dict[str, Any]:
    """íšŒì˜ ë‚´ìš© AI ë¶„ì„"""
    try:
        logger.info("ğŸ§  Starting meeting analysis...")
        
        # ëª¨ë¸ ë¡œë“œ
        qwen_model, qwen_tokenizer = load_qwen3()
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""ë‹¤ìŒ íšŒì˜ ì „ì‚¬ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

íšŒì˜ ë‚´ìš©:
{transcript_text}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”:
{{
  "summary": "íšŒì˜ í•µì‹¬ ìš”ì•½ (2-3ì¤„)",
  "action_items": [
    {{"task": "êµ¬ì²´ì ì¸ ì—…ë¬´ ë‚´ìš©", "assignee": "ë‹´ë‹¹ìëª…", "deadline": "ì˜ˆìƒ ë§ˆê°ì¼", "priority": "high/medium/low"}},
    {{"task": "ë˜ ë‹¤ë¥¸ ì—…ë¬´", "assignee": "ë‹´ë‹¹ìëª…", "deadline": "ì˜ˆìƒ ë§ˆê°ì¼", "priority": "high/medium/low"}}
  ],
  "decisions": ["ì¤‘ìš”í•œ ê²°ì •ì‚¬í•­1", "ì¤‘ìš”í•œ ê²°ì •ì‚¬í•­2"],
  "next_steps": ["ë‹¤ìŒ ë‹¨ê³„1", "ë‹¤ìŒ ë‹¨ê³„2"],
  "key_points": ["í•µì‹¬ í¬ì¸íŠ¸1", "í•µì‹¬ í¬ì¸íŠ¸2"]
}}

ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”."""

        # ë©”ì‹œì§€ êµ¬ì„±
        messages = [{"role": "user", "content": prompt}]
        
        # í† í°í™”
        text = qwen_tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        inputs = qwen_tokenizer([text], return_tensors="pt").to(qwen_model.device)
        
        # ì¶”ë¡  ì‹¤í–‰
        with torch.no_grad():
            outputs = qwen_model.generate(
                **inputs,
                max_new_tokens=1024,
                temperature=0.3,
                do_sample=True,
                pad_token_id=qwen_tokenizer.eos_token_id,
                repetition_penalty=1.1
            )
        
        # ê²°ê³¼ ë””ì½”ë”©
        response = qwen_tokenizer.decode(
            outputs[0][len(inputs["input_ids"][0]):], 
            skip_special_tokens=True
        )
        
        logger.info(f"âœ… Analysis completed: {len(response)} characters")
        
        # JSON íŒŒì‹± ì‹œë„
        try:
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°)
            if "```json" in response:
                json_start = response.find("```json") + 7
                json_end = response.find("```", json_start)
                response = response[json_start:json_end].strip()
            elif "{" in response and "}" in response:
                json_start = response.find("{")
                json_end = response.rfind("}") + 1
                response = response[json_start:json_end]
            
            analysis_json = json.loads(response)
            
            return {
                "success": True,
                "analysis": analysis_json,
                "raw_response": response
            }
            
        except json.JSONDecodeError as e:
            logger.warning(f"âš ï¸ JSON parsing failed, returning raw response: {e}")
            return {
                "success": True,
                "analysis": {"raw_text": response},
                "raw_response": response,
                "json_error": str(e)
            }
            
    except Exception as e:
        logger.error(f"âŒ Analysis error: {e}")
        return {
            "success": False,
            "error": f"Analysis failed: {str(e)}"
        }

def handler(event):
    """Runpod ë©”ì¸ í•¸ë“¤ëŸ¬"""
    try:
        job_input = event.get("input", {})
        action = job_input.get("action")
        
        logger.info(f"ğŸš€ Processing action: {action}")
        
        if action == "transcribe":
            # ìŒì„± ì „ì‚¬ë§Œ
            audio_base64 = job_input.get("audio_base64")
            if not audio_base64:
                return {"success": False, "error": "Missing audio_base64"}
            
            result = transcribe_audio(audio_base64)
            
        elif action == "analyze":
            # í…ìŠ¤íŠ¸ ë¶„ì„ë§Œ
            transcript = job_input.get("transcript")
            if not transcript:
                return {"success": False, "error": "Missing transcript"}
            
            result = analyze_meeting(transcript)
            
        elif action == "pipeline":
            # ì „ì²´ íŒŒì´í”„ë¼ì¸: ìŒì„± â†’ ì „ì‚¬ â†’ ë¶„ì„
            audio_base64 = job_input.get("audio_base64")
            if not audio_base64:
                return {"success": False, "error": "Missing audio_base64"}
            
            # 1ë‹¨ê³„: ì „ì‚¬
            logger.info("ğŸ“ Step 1: Transcription")
            transcribe_result = transcribe_audio(audio_base64)
            if not transcribe_result["success"]:
                return transcribe_result
                
            # 2ë‹¨ê³„: ë¶„ì„
            logger.info("ğŸ“Š Step 2: Analysis")
            analysis_result = analyze_meeting(
                transcribe_result["transcription"]["full_text"]
            )
            
            result = {
                "success": True,
                "transcription": transcribe_result["transcription"],
                "analysis": analysis_result.get("analysis", {}),
                "raw_analysis": analysis_result.get("raw_response", ""),
                "analysis_success": analysis_result["success"]
            }
            
        elif action == "health":
            # í—¬ìŠ¤ ì²´í¬
            result = {
                "success": True,
                "status": "healthy",
                "gpu_available": torch.cuda.is_available(),
                "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0,
                "models_loaded": {
                    "whisperx": whisper_model is not None,
                    "qwen3": qwen_model is not None
                }
            }
            
        else:
            result = {
                "success": False, 
                "error": f"Unknown action: {action}. Available: transcribe, analyze, pipeline, health"
            }
            
        logger.info(f"âœ… Action {action} completed successfully")
        return result
        
    except Exception as e:
        logger.error(f"âŒ Handler error: {e}")
        return {
            "success": False,
            "error": f"Handler failed: {str(e)}"
        }

# Runpod ì‹œì‘
if __name__ == "__main__":
    logger.info("ğŸš€ Starting TtalKkak AI Engine on Runpod...")
    runpod.serverless.start({"handler": handler})