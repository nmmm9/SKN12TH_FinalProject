"""
TtalKkak AI ì„œë²„ - Runpod ê°œë°œìš©
WhisperX (STT) + Qwen3-14B 4bit (LLM)
ë¡œì»¬ ë°±ì—”ë“œì™€ ì—°ë™í•˜ì—¬ ì‚¬ìš©
"""

import os
import io
import tempfile
import logging
from typing import Optional, Dict, Any, List
from contextlib import asynccontextmanager

import torch
import numpy as np
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ê¸€ë¡œë²Œ ëª¨ë¸ ë³€ìˆ˜
whisper_model = None
qwen_model = None
qwen_tokenizer = None

class TranscriptionResponse(BaseModel):
    success: bool
    transcription: Optional[Dict[str, Any]] = None
    error: Optional[str] = None

class AnalysisRequest(BaseModel):
    transcript: str

class AnalysisResponse(BaseModel):
    success: bool
    analysis: Optional[Dict[str, Any]] = None
    error: Optional[str] = None

class HealthResponse(BaseModel):
    status: str
    gpu_available: bool
    gpu_count: int
    models_loaded: Dict[str, bool]
    memory_info: Optional[Dict[str, float]] = None

def load_whisperx():
    """WhisperX ëª¨ë¸ ë¡œë”©"""
    global whisper_model
    
    if whisper_model is None:
        logger.info("ğŸ¤ Loading WhisperX large-v3...")
        try:
            import whisperx
            device = "cuda" if torch.cuda.is_available() else "cpu"
            compute_type = "float16" if device == "cuda" else "int8"
            
            whisper_model = whisperx.load_model(
                "large-v3", 
                device, 
                compute_type=compute_type,
                language="ko"
            )
            logger.info("âœ… WhisperX loaded successfully")
            
        except Exception as e:
            logger.error(f"âŒ WhisperX loading failed: {e}")
            raise e
    
    return whisper_model

def load_qwen3():
    """Qwen3-14B 4bit ëª¨ë¸ ë¡œë”©"""
    global qwen_model, qwen_tokenizer
    
    if qwen_model is None or qwen_tokenizer is None:
        logger.info("ğŸ§  Loading Qwen3-14B 4bit...")
        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
            
            # 4ë¹„íŠ¸ ì–‘ìí™” ì„¤ì •
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
            
            model_name = "unsloth/Qwen3-14B-unsloth-bnb-4bit"
            
            qwen_tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True
            )
            
            qwen_model = AutoModelForCausalLM.from_pretrained(
                model_name,
                quantization_config=bnb_config,
                device_map="auto",
                torch_dtype=torch.float16,
                trust_remote_code=True
            )
            
            logger.info("âœ… Qwen3-14B loaded successfully")
            
        except Exception as e:
            logger.error(f"âŒ Qwen3 loading failed: {e}")
            raise e
    
    return qwen_model, qwen_tokenizer

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì•± ì‹œì‘/ì¢…ë£Œ ì‹œ ëª¨ë¸ ë¡œë”©/ì •ë¦¬"""
    logger.info("ğŸš€ Starting TtalKkak AI Server...")
    
    # ëª¨ë¸ë“¤ì„ ë¯¸ë¦¬ ë¡œë”© (ì„ íƒì‚¬í•­)
    try:
        if os.getenv("PRELOAD_MODELS", "false").lower() == "true":
            logger.info("ğŸ“¦ Preloading models...")
            load_whisperx()
            load_qwen3()
            logger.info("âœ… Models preloaded successfully")
    except Exception as e:
        logger.warning(f"âš ï¸ Model preloading failed: {e}")
    
    yield
    
    logger.info("ğŸ›‘ Shutting down TtalKkak AI Server...")

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="TtalKkak AI Server",
    description="WhisperX + Qwen3-14B AI ì—”ì§„",
    version="1.0.0",
    lifespan=lifespan
)

# CORS ì„¤ì • (ë¡œì»¬ ë°±ì—”ë“œ ì—°ë™)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ê°œë°œìš©, í”„ë¡œë•ì…˜ì—ì„œëŠ” ì œí•œ í•„ìš”
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/", response_model=Dict[str, str])
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "TtalKkak AI Server",
        "version": "1.0.0",
        "docs": "/docs"
    }

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    gpu_available = torch.cuda.is_available()
    gpu_count = torch.cuda.device_count() if gpu_available else 0
    
    memory_info = None
    if gpu_available:
        try:
            memory_info = {
                "allocated_gb": torch.cuda.memory_allocated() / 1024**3,
                "reserved_gb": torch.cuda.memory_reserved() / 1024**3,
                "total_gb": torch.cuda.get_device_properties(0).total_memory / 1024**3
            }
        except:
            pass
    
    return HealthResponse(
        status="healthy",
        gpu_available=gpu_available,
        gpu_count=gpu_count,
        models_loaded={
            "whisperx": whisper_model is not None,
            "qwen3": qwen_model is not None
        },
        memory_info=memory_info
    )

@app.post("/transcribe", response_model=TranscriptionResponse)
async def transcribe_audio(audio: UploadFile = File(...)):
    """ìŒì„± íŒŒì¼ ì „ì‚¬"""
    try:
        logger.info(f"ğŸ¤ Transcribing audio: {audio.filename}")
        
        # ëª¨ë¸ ë¡œë”©
        whisper_model = load_whisperx()
        
        # ì˜¤ë””ì˜¤ íŒŒì¼ ì„ì‹œ ì €ì¥
        audio_content = await audio.read()
        
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_file:
            temp_file.write(audio_content)
            temp_path = temp_file.name
        
        try:
            # WhisperX ì „ì‚¬ ì‹¤í–‰
            result = whisper_model.transcribe(temp_path, batch_size=16)
            
            segments = result.get("segments", [])
            full_text = " ".join([seg.get("text", "") for seg in segments])
            
            logger.info(f"âœ… Transcription completed: {len(full_text)} characters")
            
            return TranscriptionResponse(
                success=True,
                transcription={
                    "segments": segments,
                    "full_text": full_text,
                    "language": result.get("language", "ko"),
                    "duration": sum([seg.get("end", 0) - seg.get("start", 0) for seg in segments])
                }
            )
            
        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            try:
                os.unlink(temp_path)
            except:
                pass
                
    except Exception as e:
        logger.error(f"âŒ Transcription error: {e}")
        return TranscriptionResponse(
            success=False,
            error=str(e)
        )

@app.post("/analyze", response_model=AnalysisResponse)
async def analyze_meeting(request: AnalysisRequest):
    """íšŒì˜ ë‚´ìš© ë¶„ì„"""
    try:
        logger.info("ğŸ§  Analyzing meeting content...")
        
        # ëª¨ë¸ ë¡œë”©
        qwen_model, qwen_tokenizer = load_qwen3()
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""ë‹¤ìŒ íšŒì˜ ì „ì‚¬ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

íšŒì˜ ë‚´ìš©:
{request.transcript}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”:
{{
  "summary": "íšŒì˜ í•µì‹¬ ìš”ì•½ (2-3ì¤„)",
  "action_items": [
    {{"task": "êµ¬ì²´ì ì¸ ì—…ë¬´ ë‚´ìš©", "assignee": "ë‹´ë‹¹ìëª…", "deadline": "ì˜ˆìƒ ë§ˆê°ì¼", "priority": "high/medium/low"}},
    {{"task": "ë˜ ë‹¤ë¥¸ ì—…ë¬´", "assignee": "ë‹´ë‹¹ìëª…", "deadline": "ì˜ˆìƒ ë§ˆê°ì¼", "priority": "high/medium/low"}}
  ],
  "decisions": ["ì¤‘ìš”í•œ ê²°ì •ì‚¬í•­1", "ì¤‘ìš”í•œ ê²°ì •ì‚¬í•­2"],
  "next_steps": ["ë‹¤ìŒ ë‹¨ê³„1", "ë‹¤ìŒ ë‹¨ê³„2"],
  "key_points": ["í•µì‹¬ í¬ì¸íŠ¸1", "í•µì‹¬ í¬ì¸íŠ¸2"]
}}

ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”."""

        # ë©”ì‹œì§€ êµ¬ì„±
        messages = [{"role": "user", "content": prompt}]
        
        # í† í°í™”
        text = qwen_tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        inputs = qwen_tokenizer([text], return_tensors="pt").to(qwen_model.device)
        
        # ì¶”ë¡  ì‹¤í–‰
        with torch.no_grad():
            outputs = qwen_model.generate(
                **inputs,
                max_new_tokens=1024,
                temperature=0.3,
                do_sample=True,
                pad_token_id=qwen_tokenizer.eos_token_id,
                repetition_penalty=1.1
            )
        
        # ê²°ê³¼ ë””ì½”ë”©
        response = qwen_tokenizer.decode(
            outputs[0][len(inputs["input_ids"][0]):], 
            skip_special_tokens=True
        )
        
        logger.info(f"âœ… Analysis completed: {len(response)} characters")
        
        # JSON íŒŒì‹± ì‹œë„
        import json
        try:
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            if "```json" in response:
                json_start = response.find("```json") + 7
                json_end = response.find("```", json_start)
                response = response[json_start:json_end].strip()
            elif "{" in response and "}" in response:
                json_start = response.find("{")
                json_end = response.rfind("}") + 1
                response = response[json_start:json_end]
            
            analysis_json = json.loads(response)
            
            return AnalysisResponse(
                success=True,
                analysis=analysis_json
            )
            
        except json.JSONDecodeError as e:
            logger.warning(f"âš ï¸ JSON parsing failed: {e}")
            return AnalysisResponse(
                success=True,
                analysis={"raw_text": response}
            )
            
    except Exception as e:
        logger.error(f"âŒ Analysis error: {e}")
        return AnalysisResponse(
            success=False,
            error=str(e)
        )

@app.post("/pipeline", response_model=Dict[str, Any])
async def full_pipeline(audio: UploadFile = File(...)):
    """ì „ì²´ íŒŒì´í”„ë¼ì¸: ìŒì„± â†’ ì „ì‚¬ â†’ ë¶„ì„"""
    try:
        logger.info("ğŸš€ Starting full pipeline...")
        
        # 1ë‹¨ê³„: ì „ì‚¬
        transcribe_result = await transcribe_audio(audio)
        if not transcribe_result.success:
            return {
                "success": False,
                "step": "transcription",
                "error": transcribe_result.error
            }
        
        # 2ë‹¨ê³„: ë¶„ì„
        analysis_request = AnalysisRequest(
            transcript=transcribe_result.transcription["full_text"]
        )
        analysis_result = await analyze_meeting(analysis_request)
        
        return {
            "success": True,
            "transcription": transcribe_result.transcription,
            "analysis": analysis_result.analysis,
            "analysis_success": analysis_result.success
        }
        
    except Exception as e:
        logger.error(f"âŒ Pipeline error: {e}")
        return {
            "success": False,
            "error": str(e)
        }

if __name__ == "__main__":
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    host = os.getenv("HOST", "0.0.0.0")
    port = int(os.getenv("PORT", "8000"))
    workers = int(os.getenv("WORKERS", "1"))
    
    logger.info(f"ğŸš€ Starting server on {host}:{port}")
    
    uvicorn.run(
        "ai_server:app",
        host=host,
        port=port,
        workers=workers,
        reload=False,  # Runpodì—ì„œëŠ” reload ë¹„í™œì„±í™”
        log_level="info"
    )