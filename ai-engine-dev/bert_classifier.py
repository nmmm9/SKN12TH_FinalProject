"""
TtalKkak BERT ë¶„ë¥˜ ëª¨ë¸
WhisperX Triplet ë°ì´í„°ë¥¼ ì¤‘ìš”ë„ë³„ë¡œ ë¶„ë¥˜í•˜ëŠ” BERT ê¸°ë°˜ í•„í„°ë§ ì‹œìŠ¤í…œ
"""

import os
import torch
import numpy as np
from typing import List, Dict, Any, Optional
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import logging

logger = logging.getLogger(__name__)

class TtalkkakBERTClassifier:
    """
    íšŒì˜ ë°œí™” ì¤‘ìš”ë„ ë¶„ë¥˜ë¥¼ ìœ„í•œ BERT ëª¨ë¸
    - Label 0: ì¤‘ìš”í•œ ì—…ë¬´ ê´€ë ¨ ë°œí™” (ìœ ì§€)
    - Label 1: ì¡ë‹´, ì¸ì‚¬ë§ ë“± ë¶ˆí•„ìš”í•œ ë°œí™” (ì œê±°)
    """
    
    def __init__(self, model_path: Optional[str] = None):
        self.model_path = model_path or "klue/bert-base"
        self.tokenizer = None
        self.model = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        logger.info(f"ğŸ§  BERT ë¶„ë¥˜ê¸° ì´ˆê¸°í™” - Device: {self.device}")
    
    def load_model(self):
        """BERT ëª¨ë¸ ë¡œë”©"""
        try:
            logger.info(f"ğŸ“¦ BERT ëª¨ë¸ ë¡œë”©: {self.model_path}")
            
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
            
            # ëª¨ë¸ ë¡œë“œ (íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì´ ìˆë‹¤ë©´ í•´ë‹¹ ê²½ë¡œ ì‚¬ìš©)
            local_model_path = "./Bertëª¨ë¸/Ttalkkak_model_v2"
            if os.path.exists(local_model_path):
                logger.info("ğŸ¯ ë¡œì»¬ íŒŒì¸íŠœë‹ ëª¨ë¸ ì‚¬ìš©")
                from transformers import AutoConfig
                
                # config ë¡œë“œ (num_labels=2 í™•ì¸)
                config = AutoConfig.from_pretrained(local_model_path, num_labels=2)
                
                # ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„±
                self.model = AutoModelForSequenceClassification.from_config(config)
                
                # .pt íŒŒì¼ ë¡œë“œ (ì‚¬ìš©ìê°€ ì—…ë¡œë“œí•  ì˜ˆì •)
                pt_file_path = os.path.join(local_model_path, "Ttalkkak_model_v2.pt")
                if os.path.exists(pt_file_path):
                    state_dict = torch.load(pt_file_path, map_location=self.device)
                    self.model.load_state_dict(state_dict)
                    logger.info("âœ… íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                else:
                    logger.warning("âš ï¸ .pt íŒŒì¼ ì—†ìŒ, ê¸°ë³¸ BERT ëª¨ë¸ ì‚¬ìš©")
                    self.model = AutoModelForSequenceClassification.from_pretrained(
                        self.model_path, num_labels=2
                    )
            else:
                # ê¸°ë³¸ BERT ëª¨ë¸ ì‚¬ìš©
                logger.info("ğŸ“– ê¸°ë³¸ BERT ëª¨ë¸ ì‚¬ìš©")
                self.model = AutoModelForSequenceClassification.from_pretrained(
                    self.model_path, num_labels=2
                )
            
            # GPUë¡œ ì´ë™
            self.model.to(self.device)
            self.model.eval()
            
            logger.info("âœ… BERT ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ BERT ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise e
    
    def classify_triplet(self, triplet: Dict[str, Any]) -> int:
        """ë‹¨ì¼ Triplet ë¶„ë¥˜"""
        if not self.model or not self.tokenizer:
            self.load_model()
        
        try:
            # Triplet í…ìŠ¤íŠ¸ ê²°í•© (ë§¥ë½ í¬í•¨)
            prev_text = triplet.get("prev", "")
            target_text = triplet.get("target", "")
            next_text = triplet.get("next", "")
            
            # [TGT] íƒœê·¸ ì œê±° í›„ ê²°í•©
            target_clean = target_text.replace("[TGT]", "").replace("[/TGT]", "").strip()
            combined_text = f"{prev_text} {target_clean} {next_text}".strip()
            
            # í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer(
                combined_text,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            )
            
            # GPUë¡œ ì´ë™
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # ì¶”ë¡ 
            with torch.no_grad():
                outputs = self.model(**inputs)
                prediction = torch.argmax(outputs.logits, dim=-1)
                confidence = torch.softmax(outputs.logits, dim=-1)
            
            label = prediction.item()
            conf_score = confidence[0][label].item()
            
            return {
                "label": label,  # 0: ì¤‘ìš”, 1: ë…¸ì´ì¦ˆ
                "confidence": conf_score,
                "text_length": len(combined_text)
            }
            
        except Exception as e:
            logger.error(f"âŒ Triplet ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ê°’: ì¤‘ìš”í•œ ë°œí™”ë¡œ ë¶„ë¥˜ (ì•ˆì „ì¥ì¹˜)
            return {"label": 0, "confidence": 0.5, "text_length": 0}
    
    def classify_triplets_batch(self, triplets: List[Dict[str, Any]], batch_size: int = 32) -> List[Dict[str, Any]]:
        """ì§„ì§œ ë°°ì¹˜ ì²˜ë¦¬ - GPU íš¨ìœ¨ì  ì¶”ë¡ """
        if not triplets:
            return []
        
        import time
        total_start_time = time.time()
        logger.info(f"ğŸš€ ì§„ì§œ ë°°ì¹˜ BERT ë¶„ë¥˜ ì‹œì‘: {len(triplets)}ê°œ Triplet (ë°°ì¹˜ í¬ê¸°: {batch_size})")
        
        # ëª¨ë¸ ë¡œë”© í™•ì¸
        if not self.model or not self.tokenizer:
            self.load_model()
        
        classified_triplets = []
        important_count = 0
        noise_count = 0
        
        try:
            # 1. ëª¨ë“  í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            texts = []
            for triplet in triplets:
                prev_text = triplet.get("prev", "")
                target_text = triplet.get("target", "")
                next_text = triplet.get("next", "")
                
                # [TGT] íƒœê·¸ ì œê±° í›„ ê²°í•©
                target_clean = target_text.replace("[TGT]", "").replace("[/TGT]", "").strip()
                combined_text = f"{prev_text} {target_clean} {next_text}".strip()
                texts.append(combined_text)
            
            # 2. ë°°ì¹˜ë³„ ì²˜ë¦¬
            all_predictions = []
            all_confidences = []
            preprocessing_time = time.time() - total_start_time
            
            batch_start_time = time.time()
            num_batches = (len(texts) + batch_size - 1) // batch_size
            
            for batch_idx, i in enumerate(range(0, len(texts), batch_size)):
                batch_processing_start = time.time()
                batch_end = min(i + batch_size, len(texts))
                batch_texts = texts[i:batch_end]
                
                logger.info(f"ğŸ“Š ë°°ì¹˜ {batch_idx+1}/{num_batches} ì²˜ë¦¬ ì¤‘: {i+1}-{batch_end}/{len(texts)}")
                
                # ë°°ì¹˜ í† í¬ë‚˜ì´ì§•
                inputs = self.tokenizer(
                    batch_texts,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=512
                )
                
                # GPUë¡œ ì´ë™
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                # ë°°ì¹˜ ì¶”ë¡  ì‹¤í–‰ (GPU ë©”ëª¨ë¦¬ ìµœì í™”)
                with torch.no_grad():
                    # Mixed Precisionìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
                    if hasattr(torch.cuda, 'amp') and torch.cuda.is_available():
                        with torch.cuda.amp.autocast():
                            outputs = self.model(**inputs)
                    else:
                        outputs = self.model(**inputs)
                    
                    predictions = torch.argmax(outputs.logits, dim=-1)
                    confidences = torch.softmax(outputs.logits, dim=-1)
                
                # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                del inputs
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                # ê²°ê³¼ ìˆ˜ì§‘
                batch_predictions = predictions.cpu().numpy()
                batch_confidences = confidences.cpu().numpy()
                
                all_predictions.extend(batch_predictions)
                for j, conf_scores in enumerate(batch_confidences):
                    label = batch_predictions[j]
                    confidence = conf_scores[label]
                    all_confidences.append(confidence)
                
                batch_elapsed = time.time() - batch_processing_start
                logger.info(f"   â±ï¸  ë°°ì¹˜ {batch_idx+1} ì™„ë£Œ: {batch_elapsed:.3f}ì´ˆ ({len(batch_texts)}ê°œ ì²˜ë¦¬)")
            
            # 3. ê²°ê³¼ í†µí•©
            for i, triplet in enumerate(triplets):
                try:
                    triplet_with_label = triplet.copy()
                    triplet_with_label["label"] = int(all_predictions[i])
                    triplet_with_label["confidence"] = float(all_confidences[i])
                    triplet_with_label["text_length"] = len(texts[i])
                    
                    classified_triplets.append(triplet_with_label)
                    
                    # í†µê³„ ìˆ˜ì§‘
                    if all_predictions[i] == 0:
                        important_count += 1
                    else:
                        noise_count += 1
                        
                except Exception as e:
                    logger.error(f"âŒ Triplet {i} ê²°ê³¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
                    triplet["label"] = 0
                    triplet["confidence"] = 0.5
                    classified_triplets.append(triplet)
                    important_count += 1
            
            # 4. í†µê³„ ì¶œë ¥
            total = len(triplets)
            noise_ratio = (noise_count / total) * 100 if total > 0 else 0
            
            total_elapsed = time.time() - total_start_time
            batch_processing_time = time.time() - batch_start_time
            
            logger.info(f"ğŸ‰ ì§„ì§œ ë°°ì¹˜ BERT ë¶„ë¥˜ ì™„ë£Œ!")
            logger.info(f"ğŸ“ˆ ë¶„ë¥˜ í†µê³„:")
            logger.info(f"   - ì „ì²´: {total}ê°œ")
            logger.info(f"   - ë°°ì¹˜ í¬ê¸°: {batch_size}")
            logger.info(f"   - ì´ ë°°ì¹˜ ìˆ˜: {num_batches}")
            logger.info(f"   - ì¤‘ìš” ë°œí™”: {important_count}ê°œ ({100-noise_ratio:.1f}%)")
            logger.info(f"   - ë…¸ì´ì¦ˆ ë°œí™”: {noise_count}ê°œ ({noise_ratio:.1f}%)")
            logger.info(f"â±ï¸  ì„±ëŠ¥ í†µê³„:")
            logger.info(f"   - ì „ì²˜ë¦¬ ì‹œê°„: {preprocessing_time:.3f}ì´ˆ")
            logger.info(f"   - ë°°ì¹˜ ì²˜ë¦¬ ì‹œê°„: {batch_processing_time:.3f}ì´ˆ")
            logger.info(f"   - ì´ ì²˜ë¦¬ ì‹œê°„: {total_elapsed:.3f}ì´ˆ")
            logger.info(f"   - ì²˜ë¦¬ ì†ë„: {total/total_elapsed:.1f} triplets/sec")
            logger.info(f"   - ê°œë³„ ì²˜ë¦¬ ì˜ˆìƒ ì‹œê°„: {total * 0.5:.1f}ì´ˆ (ê°€ì •)")
            logger.info(f"   - ì˜ˆìƒ ì‹œê°„ ì ˆì•½: {(total * 0.5) - total_elapsed:.1f}ì´ˆ")
            
            return classified_triplets
            
        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ ë¶„ë¥˜ ì‹¤íŒ¨, ê°œë³„ ì²˜ë¦¬ë¡œ ëŒ€ì²´: {e}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ê°œë³„ ì²˜ë¦¬ ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´
            return self._classify_triplets_individual_fallback(triplets)
    
    def _classify_triplets_individual_fallback(self, triplets: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ìš© ê°œë³„ ì²˜ë¦¬"""
        logger.warning("âš ï¸ ê°œë³„ ì²˜ë¦¬ ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´")
        
        classified_triplets = []
        important_count = 0
        noise_count = 0
        
        for i, triplet in enumerate(triplets):
            try:
                classification = self.classify_triplet(triplet)
                
                triplet_with_label = triplet.copy()
                triplet_with_label["label"] = classification["label"]
                triplet_with_label["confidence"] = classification["confidence"]
                
                classified_triplets.append(triplet_with_label)
                
                if classification["label"] == 0:
                    important_count += 1
                else:
                    noise_count += 1
                    
                if (i + 1) % 20 == 0 or (i + 1) == len(triplets):
                    logger.info(f"ğŸ“Š ê°œë³„ ì²˜ë¦¬ ì§„í–‰ë¥ : {i+1}/{len(triplets)}")
                    
            except Exception as e:
                logger.error(f"âŒ Triplet {i} ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
                triplet["label"] = 0
                triplet["confidence"] = 0.5
                classified_triplets.append(triplet)
                important_count += 1
        
        total = len(triplets)
        noise_ratio = (noise_count / total) * 100 if total > 0 else 0
        
        logger.info(f"âœ… ê°œë³„ BERT ë¶„ë¥˜ ì™„ë£Œ")
        logger.info(f"ğŸ“ˆ ë¶„ë¥˜ í†µê³„:")
        logger.info(f"   - ì „ì²´: {total}ê°œ")
        logger.info(f"   - ì¤‘ìš” ë°œí™”: {important_count}ê°œ ({100-noise_ratio:.1f}%)")
        logger.info(f"   - ë…¸ì´ì¦ˆ ë°œí™”: {noise_count}ê°œ ({noise_ratio:.1f}%)")
        
        return classified_triplets
    
    def get_classification_stats(self, classified_triplets: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ë¶„ë¥˜ í†µê³„ ìƒì„±"""
        total = len(classified_triplets)
        important = sum(1 for t in classified_triplets if t.get("label") == 0)
        noise = total - important
        
        return {
            "total_triplets": total,
            "important_triplets": important,
            "noise_triplets": noise,
            "noise_reduction_ratio": (noise / total) if total > 0 else 0,
            "avg_confidence": np.mean([t.get("confidence", 0.5) for t in classified_triplets]),
            "method": "BERT-based classification"
        }

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
bert_classifier = None

def get_bert_classifier() -> TtalkkakBERTClassifier:
    """BERT ë¶„ë¥˜ê¸° ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global bert_classifier
    
    if bert_classifier is None:
        bert_classifier = TtalkkakBERTClassifier()
        bert_classifier.load_model()
    
    return bert_classifier