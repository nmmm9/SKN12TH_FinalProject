"""
TtalKkak í–¥ìƒëœ AI ì„œë²„ - Task Master ê¸°ëŠ¥ í†µí•©
WhisperX (STT) + Qwen3-14B (LLM) + Task Master í”„ë¡¬í”„íŠ¸ ì‹œìŠ¤í…œ
"""

import os
import io
import json
import tempfile
import logging
from typing import Optional, Dict, Any, List
from contextlib import asynccontextmanager
import time

import torch
import numpy as np
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# ë¡œì»¬ ëª¨ë“ˆ ìž„í¬íŠ¸
from task_schemas import (
    TaskItem, MeetingAnalysisResult, ComplexityAnalysis, 
    TaskExpansionRequest, TaskExpansionResult, PipelineRequest, PipelineResult,
    calculate_task_complexity_advanced, validate_task_dependencies, 
    generate_task_id_sequence, TASK_SCHEMA_EXAMPLE
)
from meeting_analysis_prompts import (
    generate_meeting_analysis_system_prompt,
    generate_meeting_analysis_user_prompt,
    generate_task_expansion_system_prompt,
    generate_complexity_analysis_prompt,
    validate_meeting_analysis
)

import uvicorn

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ê¸€ë¡œë²Œ ëª¨ë¸ ë³€ìˆ˜
whisper_model = None
qwen_model = None
qwen_tokenizer = None

# ê¸°ë³¸ ì‘ë‹µ ëª¨ë¸ë“¤
class TranscriptionResponse(BaseModel):
    success: bool
    transcription: Optional[Dict[str, Any]] = None
    error: Optional[str] = None

class AnalysisRequest(BaseModel):
    transcript: str
    num_tasks: int = 5
    additional_context: Optional[str] = None

class AnalysisResponse(BaseModel):
    success: bool
    analysis: Optional[MeetingAnalysisResult] = None
    error: Optional[str] = None

class HealthResponse(BaseModel):
    status: str
    gpu_available: bool
    gpu_count: int
    models_loaded: Dict[str, bool]
    memory_info: Optional[Dict[str, float]] = None

def load_whisperx():
    """WhisperX ëª¨ë¸ ë¡œë”©"""
    global whisper_model
    
    if whisper_model is None:
        logger.info("ðŸŽ¤ Loading WhisperX large-v3...")
        try:
            import whisperx
            device = "cuda" if torch.cuda.is_available() else "cpu"
            compute_type = "float16" if device == "cuda" else "int8"
            
            whisper_model = whisperx.load_model(
                "large-v3", 
                device, 
                compute_type=compute_type,
                language="ko"
            )
            logger.info("âœ… WhisperX loaded successfully")
            
        except Exception as e:
            logger.error(f"âŒ WhisperX loading failed: {e}")
            raise e
    
    return whisper_model

def load_qwen3():
    """Qwen3-14B 4bit ëª¨ë¸ ë¡œë”©"""
    global qwen_model, qwen_tokenizer
    
    if qwen_model is None or qwen_tokenizer is None:
        logger.info("ðŸ§  Loading Qwen3-14B 4bit...")
        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
            
            # 4ë¹„íŠ¸ ì–‘ìží™” ì„¤ì •
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
            
            model_name = "unsloth/Qwen3-14B-unsloth-bnb-4bit"
            
            qwen_tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True
            )
            
            qwen_model = AutoModelForCausalLM.from_pretrained(
                model_name,
                quantization_config=bnb_config,
                device_map="auto",
                torch_dtype=torch.float16,
                trust_remote_code=True
            )
            
            logger.info("âœ… Qwen3-14B loaded successfully")
            
        except Exception as e:
            logger.error(f"âŒ Qwen3 loading failed: {e}")
            raise e
    
    return qwen_model, qwen_tokenizer

def generate_structured_response(
    system_prompt: str, 
    user_prompt: str, 
    response_schema: Dict[str, Any],
    temperature: float = 0.3
) -> Dict[str, Any]:
    """êµ¬ì¡°í™”ëœ ì‘ë‹µ ìƒì„± (Task Master ìŠ¤íƒ€ì¼)"""
    
    # ëª¨ë¸ ë¡œë”©
    qwen_model, qwen_tokenizer = load_qwen3()
    
    # ìŠ¤í‚¤ë§ˆ ì˜ˆì‹œ í¬í•¨ í”„ë¡¬í”„íŠ¸
    schema_prompt = f"""
{system_prompt}

**Response Schema:**
You must respond with a JSON object following this exact structure:
```json
{json.dumps(response_schema, indent=2, ensure_ascii=False)}
```

**Important Rules:**
1. Always return valid JSON format
2. Use Korean for all text content unless technical terms require English
3. Follow the exact schema structure
4. Include all required fields
5. Use appropriate data types for each field

{user_prompt}

**Response:**
```json
"""
    
    # ë©”ì‹œì§€ êµ¬ì„±
    messages = [{"role": "user", "content": schema_prompt}]
    
    # í† í°í™”
    text = qwen_tokenizer.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True
    )
    
    inputs = qwen_tokenizer([text], return_tensors="pt").to(qwen_model.device)
    
    # ì¶”ë¡  ì‹¤í–‰
    with torch.no_grad():
        outputs = qwen_model.generate(
            **inputs,
            max_new_tokens=2048,
            temperature=temperature,
            do_sample=True,
            pad_token_id=qwen_tokenizer.eos_token_id,
            repetition_penalty=1.1,
            top_p=0.9
        )
    
    # ê²°ê³¼ ë””ì½”ë”©
    response = qwen_tokenizer.decode(
        outputs[0][len(inputs["input_ids"][0]):], 
        skip_special_tokens=True
    )
    
    # JSON ì¶”ì¶œ ë° íŒŒì‹±
    try:
        # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
        if "```json" in response:
            json_start = response.find("```json") + 7
            json_end = response.find("```", json_start)
            if json_end == -1:
                json_end = len(response)
            json_text = response[json_start:json_end].strip()
        elif "{" in response and "}" in response:
            json_start = response.find("{")
            json_end = response.rfind("}") + 1
            json_text = response[json_start:json_end]
        else:
            json_text = response
        
        # JSON íŒŒì‹±
        parsed_result = json.loads(json_text)
        return parsed_result
        
    except json.JSONDecodeError as e:
        logger.warning(f"âš ï¸ JSON parsing failed: {e}")
        logger.warning(f"Raw response: {response}")
        
        # ê¸°ë³¸ êµ¬ì¡° ë°˜í™˜
        return {
            "summary": "AI ì‘ë‹µì„ íŒŒì‹±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            "action_items": [],
            "decisions": [],
            "next_steps": [],
            "key_points": [],
            "raw_response": response
        }

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì•± ì‹œìž‘/ì¢…ë£Œ ì‹œ ëª¨ë¸ ë¡œë”©/ì •ë¦¬"""
    logger.info("ðŸš€ Starting TtalKkak Enhanced AI Server...")
    
    # ëª¨ë¸ë“¤ì„ ë¯¸ë¦¬ ë¡œë”© (ì„ íƒì‚¬í•­)
    try:
        if os.getenv("PRELOAD_MODELS", "false").lower() == "true":
            logger.info("ðŸ“¦ Preloading models...")
            load_whisperx()
            load_qwen3()
            logger.info("âœ… Models preloaded successfully")
    except Exception as e:
        logger.warning(f"âš ï¸ Model preloading failed: {e}")
    
    yield
    
    logger.info("ðŸ›‘ Shutting down TtalKkak Enhanced AI Server...")

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="TtalKkak Enhanced AI Server",
    description="WhisperX + Qwen3-14B + Task Master Features",
    version="2.0.0",
    lifespan=lifespan
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/", response_model=Dict[str, str])
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "TtalKkak Enhanced AI Server",
        "version": "2.0.0",
        "features": [
            "WhisperX Speech-to-Text",
            "Qwen3-14B Meeting Analysis", 
            "Task Master Prompt System",
            "Complex Task Generation",
            "Complexity Analysis"
        ],
        "docs": "/docs"
    }

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    gpu_available = torch.cuda.is_available()
    gpu_count = torch.cuda.device_count() if gpu_available else 0
    
    memory_info = None
    if gpu_available:
        try:
            memory_info = {
                "allocated_gb": torch.cuda.memory_allocated() / 1024**3,
                "reserved_gb": torch.cuda.memory_reserved() / 1024**3,
                "total_gb": torch.cuda.get_device_properties(0).total_memory / 1024**3
            }
        except:
            pass
    
    return HealthResponse(
        status="healthy",
        gpu_available=gpu_available,
        gpu_count=gpu_count,
        models_loaded={
            "whisperx": whisper_model is not None,
            "qwen3": qwen_model is not None
        },
        memory_info=memory_info
    )

@app.post("/transcribe", response_model=TranscriptionResponse)
async def transcribe_audio(audio: UploadFile = File(...)):
    """ìŒì„± íŒŒì¼ ì „ì‚¬ (WhisperX)"""
    try:
        logger.info(f"ðŸŽ¤ Transcribing audio: {audio.filename}")
        
        # ëª¨ë¸ ë¡œë”©
        whisper_model = load_whisperx()
        
        # ì˜¤ë””ì˜¤ íŒŒì¼ ìž„ì‹œ ì €ìž¥
        audio_content = await audio.read()
        
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_file:
            temp_file.write(audio_content)
            temp_path = temp_file.name
        
        try:
            # WhisperX ì „ì‚¬ ì‹¤í–‰
            result = whisper_model.transcribe(temp_path, batch_size=16)
            
            segments = result.get("segments", [])
            full_text = " ".join([seg.get("text", "") for seg in segments])
            
            logger.info(f"âœ… Transcription completed: {len(full_text)} characters")
            
            return TranscriptionResponse(
                success=True,
                transcription={
                    "segments": segments,
                    "full_text": full_text,
                    "language": result.get("language", "ko"),
                    "duration": sum([seg.get("end", 0) - seg.get("start", 0) for seg in segments])
                }
            )
            
        finally:
            # ìž„ì‹œ íŒŒì¼ ì •ë¦¬
            try:
                os.unlink(temp_path)
            except:
                pass
                
    except Exception as e:
        logger.error(f"âŒ Transcription error: {e}")
        return TranscriptionResponse(
            success=False,
            error=str(e)
        )

@app.post("/analyze", response_model=AnalysisResponse)
async def analyze_meeting(request: AnalysisRequest):
    """íšŒì˜ ë‚´ìš© ë¶„ì„ (Task Master ìŠ¤íƒ€ì¼)"""
    try:
        logger.info(f"ðŸ§  Analyzing meeting content... (generating {request.num_tasks} tasks)")
        
        start_time = time.time()
        
        # ì‹œìŠ¤í…œ ë° ì‚¬ìš©ìž í”„ë¡¬í”„íŠ¸ ìƒì„±
        system_prompt = generate_meeting_analysis_system_prompt(request.num_tasks)
        user_prompt = generate_meeting_analysis_user_prompt(
            request.transcript, 
            request.additional_context or ""
        )
        
        # êµ¬ì¡°í™”ëœ ì‘ë‹µ ìƒì„±
        result = generate_structured_response(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            response_schema=TASK_SCHEMA_EXAMPLE,
            temperature=0.3
        )
        
        # ê²°ê³¼ í›„ì²˜ë¦¬ ë° ê²€ì¦
        validated_result = validate_meeting_analysis(result)
        
        # TaskItem ê°ì²´ë¡œ ë³€í™˜
        task_items = []
        for i, item in enumerate(validated_result.get("action_items", [])):
            task_item = TaskItem(
                id=i + 1,
                title=item.get("task", f"Task {i+1}"),
                description=item.get("task", ""),
                priority=item.get("priority", "medium"),
                assignee=item.get("assignee", "ë¯¸ì§€ì •"),
                deadline=item.get("deadline", "ë¯¸ì •"),
                complexity=calculate_task_complexity_advanced(
                    TaskItem(
                        id=i + 1,
                        title=item.get("task", ""),
                        description=item.get("task", ""),
                        priority=item.get("priority", "medium")
                    )
                ),
                status="pending"
            )
            task_items.append(task_item)
        
        # ì˜ì¡´ì„± ê²€ì¦
        task_items = validate_task_dependencies(task_items)
        
        # ìµœì¢… ê²°ê³¼ êµ¬ì„±
        meeting_analysis = MeetingAnalysisResult(
            summary=validated_result.get("summary", ""),
            action_items=task_items,
            decisions=validated_result.get("decisions", []),
            next_steps=validated_result.get("next_steps", []),
            key_points=validated_result.get("key_points", []),
            participants=validated_result.get("participants", []),
            follow_up=validated_result.get("follow_up", {}),
            metadata={
                "processing_time": time.time() - start_time,
                "total_tasks": len(task_items),
                "complexity_distribution": {
                    "high": len([t for t in task_items if t.complexity >= 7]),
                    "medium": len([t for t in task_items if 4 <= t.complexity < 7]),
                    "low": len([t for t in task_items if t.complexity < 4])
                }
            }
        )
        
        logger.info(f"âœ… Analysis completed in {time.time() - start_time:.2f}s")
        
        return AnalysisResponse(
            success=True,
            analysis=meeting_analysis
        )
        
    except Exception as e:
        logger.error(f"âŒ Analysis error: {e}")
        return AnalysisResponse(
            success=False,
            error=str(e)
        )

@app.post("/expand-task", response_model=TaskExpansionResult)
async def expand_task(request: TaskExpansionRequest):
    """íƒœìŠ¤í¬ í™•ìž¥ (ì„œë¸ŒíƒœìŠ¤í¬ ìƒì„±)"""
    try:
        logger.info(f"ðŸ” Expanding task {request.task_id} into {request.num_subtasks} subtasks")
        
        # íƒœìŠ¤í¬ ì •ë³´ (ì‹¤ì œë¡œëŠ” DBì—ì„œ ì¡°íšŒ)
        task_description = f"Task ID {request.task_id}: {request.additional_context or 'Sample task'}"
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        system_prompt = generate_task_expansion_system_prompt(request.num_subtasks)
        user_prompt = f"""
        Expand the following task into {request.num_subtasks} detailed subtasks:
        
        **Task:** {task_description}
        **Focus Areas:** {', '.join(request.focus_areas) if request.focus_areas else 'General implementation'}
        **Additional Context:** {request.additional_context or 'None'}
        
        Generate {request.num_subtasks} specific, actionable subtasks that together accomplish the main task.
        """
        
        # ì„œë¸ŒíƒœìŠ¤í¬ ìŠ¤í‚¤ë§ˆ
        subtask_schema = {
            "subtasks": [
                {
                    "id": 1,
                    "title": "ì„œë¸ŒíƒœìŠ¤í¬ ì œëª©",
                    "description": "ì„œë¸ŒíƒœìŠ¤í¬ ìƒì„¸ ì„¤ëª…",
                    "priority": "high/medium/low",
                    "estimated_hours": 4
                }
            ],
            "expansion_reasoning": "íƒœìŠ¤í¬ë¥¼ ì´ë ‡ê²Œ ë¶„í•´í•œ ì´ìœ ì™€ ê·¼ê±°"
        }
        
        # êµ¬ì¡°í™”ëœ ì‘ë‹µ ìƒì„±
        result = generate_structured_response(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            response_schema=subtask_schema,
            temperature=0.4
        )
        
        # ê²°ê³¼ ë°˜í™˜ (ì‹¤ì œë¡œëŠ” DB ì—…ë°ì´íŠ¸ í¬í•¨)
        return TaskExpansionResult(
            original_task=TaskItem(
                id=request.task_id,
                title="Sample Task",
                description=task_description,
                priority="medium"
            ),
            expanded_subtasks=result.get("subtasks", []),
            expansion_reasoning=result.get("expansion_reasoning", ""),
            estimated_total_hours=sum([s.get("estimated_hours", 0) for s in result.get("subtasks", [])])
        )
        
    except Exception as e:
        logger.error(f"âŒ Task expansion error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/pipeline", response_model=PipelineResult)
async def full_pipeline(audio: UploadFile = File(...)):
    """ì „ì²´ íŒŒì´í”„ë¼ì¸: ìŒì„± â†’ ì „ì‚¬ â†’ ë¶„ì„ â†’ íƒœìŠ¤í¬ ìƒì„±"""
    try:
        logger.info("ðŸš€ Starting enhanced full pipeline...")
        
        start_time = time.time()
        
        # 1ë‹¨ê³„: ì „ì‚¬
        logger.info("ðŸ“ Step 1: Transcribing audio...")
        transcribe_result = await transcribe_audio(audio)
        if not transcribe_result.success:
            return PipelineResult(
                success=False,
                step="transcription",
                error=transcribe_result.error
            )
        
        # 2ë‹¨ê³„: ë¶„ì„
        logger.info("ðŸ§  Step 2: Analyzing meeting content...")
        analysis_request = AnalysisRequest(
            transcript=transcribe_result.transcription["full_text"],
            num_tasks=5
        )
        analysis_result = await analyze_meeting(analysis_request)
        
        total_time = time.time() - start_time
        
        return PipelineResult(
            success=True,
            step="completed",
            transcription=transcribe_result.transcription,
            analysis=analysis_result.analysis,
            processing_time=total_time,
            model_info={
                "whisperx": "large-v3",
                "qwen3": "unsloth/Qwen3-14B-unsloth-bnb-4bit",
                "task_master": "integrated"
            }
        )
        
    except Exception as e:
        logger.error(f"âŒ Pipeline error: {e}")
        return PipelineResult(
            success=False,
            step="pipeline",
            error=str(e)
        )

if __name__ == "__main__":
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    host = os.getenv("HOST", "0.0.0.0")
    port = int(os.getenv("PORT", "8000"))
    workers = int(os.getenv("WORKERS", "1"))
    
    logger.info(f"ðŸš€ Starting enhanced server on {host}:{port}")
    
    uvicorn.run(
        "ai_server_enhanced:app",
        host=host,
        port=port,
        workers=workers,
        reload=False,
        log_level="info"
    )