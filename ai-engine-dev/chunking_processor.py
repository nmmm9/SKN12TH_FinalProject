"""
TtalKkak ì²­í‚¹ í”„ë¡œì„¸ì„œ
Qwen3-32B AWQ ëª¨ë¸ (32,768 í† í° ì œí•œ) ëŒ€ì‘
"""

import json
import re
import logging
from typing import List, Dict, Any, Optional, Tuple
import time

logger = logging.getLogger(__name__)

class TtalKkakChunkingProcessor:
    """TtalKkak ì „ìš© í† í° ê¸°ë°˜ ì²­í‚¹ í”„ë¡œì„¸ì„œ"""
    
    def __init__(self, max_context_tokens: int = 32768):
        self.max_context_tokens = max_context_tokens
        # ì•ˆì „ ë§ˆì§„ (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸, ìŠ¤í‚¤ë§ˆ, ì¶œë ¥ í† í° ê³ ë ¤)
        self.safety_margin = 4000
        self.max_input_tokens = max_context_tokens - self.safety_margin
        self.overlap_tokens = 200  # ì²­í¬ ê°„ ê²¹ì¹¨
        
        logger.info(f"ğŸ”§ ì²­í‚¹ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” - ìµœëŒ€ ì…ë ¥ í† í°: {self.max_input_tokens}")
    
    def estimate_tokens(self, text: str) -> int:
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ì¶”ì •"""
        if not text:
            return 0
        
        korean_chars = len(re.findall(r'[ê°€-í£]', text))
        english_words = len(re.findall(r'[a-zA-Z]+', text))
        other_chars = len(text) - korean_chars - sum(len(word) for word in re.findall(r'[a-zA-Z]+', text))
        
        estimated_tokens = int(
            korean_chars * 1.5 + 
            english_words * 1.3 + 
            other_chars * 1.0
        )
        
        return estimated_tokens
    
    def split_by_sentences(self, text: str) -> List[str]:
        """ë¬¸ì¥ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ ë¶„í• """
        # í•œêµ­ì–´ ë¬¸ì¥ ë íŒ¨í„´
        sentence_endings = r'[.!?ã€‚ï¼ï¼Ÿ]\s*'
        sentences = re.split(sentence_endings, text)
        
        # ë¹ˆ ë¬¸ì¥ ì œê±° ë° ì •ë¦¬
        sentences = [s.strip() for s in sentences if s.strip()]
        
        return sentences
    
    def create_chunks_with_overlap(self, text: str) -> List[Dict[str, Any]]:
        """ê²¹ì¹¨ì„ í¬í•¨í•œ ì²­í‚¹"""
        if self.estimate_tokens(text) <= self.max_input_tokens:
            return [{
                "chunk_id": 0,
                "text": text,
                "estimated_tokens": self.estimate_tokens(text),
                "start_sentence": 0,
                "end_sentence": -1,
                "has_overlap": False
            }]
        
        sentences = self.split_by_sentences(text)
        chunks = []
        current_chunk_sentences = []
        current_tokens = 0
        chunk_id = 0
        
        i = 0
        while i < len(sentences):
            sentence = sentences[i]
            sentence_tokens = self.estimate_tokens(sentence)
            
            # ë‹¨ì¼ ë¬¸ì¥ì´ ë„ˆë¬´ ê¸´ ê²½ìš° ê°•ì œ ë¶„í• 
            if sentence_tokens > self.max_input_tokens:
                if current_chunk_sentences:
                    # í˜„ì¬ ì²­í¬ ì €ì¥
                    chunks.append({
                        "chunk_id": chunk_id,
                        "text": " ".join(current_chunk_sentences),
                        "estimated_tokens": current_tokens,
                        "start_sentence": i - len(current_chunk_sentences),
                        "end_sentence": i - 1,
                        "has_overlap": chunk_id > 0
                    })
                    chunk_id += 1
                    current_chunk_sentences = []
                    current_tokens = 0
                
                # ê¸´ ë¬¸ì¥ì„ ê¸€ì ë‹¨ìœ„ë¡œ ë¶„í• 
                long_sentence_chunks = self._split_long_sentence(sentence, chunk_id)
                chunks.extend(long_sentence_chunks)
                chunk_id += len(long_sentence_chunks)
                i += 1
                continue
            
            # í˜„ì¬ ì²­í¬ì— ì¶”ê°€ ê°€ëŠ¥í•œì§€ í™•ì¸
            if current_tokens + sentence_tokens <= self.max_input_tokens:
                current_chunk_sentences.append(sentence)
                current_tokens += sentence_tokens
                i += 1
            else:
                # í˜„ì¬ ì²­í¬ ì €ì¥
                if current_chunk_sentences:
                    chunks.append({
                        "chunk_id": chunk_id,
                        "text": " ".join(current_chunk_sentences),
                        "estimated_tokens": current_tokens,
                        "start_sentence": i - len(current_chunk_sentences),
                        "end_sentence": i - 1,
                        "has_overlap": chunk_id > 0
                    })
                    chunk_id += 1
                
                # ê²¹ì¹¨ ì²˜ë¦¬: ë§ˆì§€ë§‰ ëª‡ ë¬¸ì¥ì„ ë‹¤ìŒ ì²­í¬ì— í¬í•¨
                overlap_sentences = self._get_overlap_sentences(current_chunk_sentences)
                current_chunk_sentences = overlap_sentences + [sentence]
                current_tokens = sum(self.estimate_tokens(s) for s in current_chunk_sentences)
                i += 1
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
        if current_chunk_sentences:
            chunks.append({
                "chunk_id": chunk_id,
                "text": " ".join(current_chunk_sentences),
                "estimated_tokens": current_tokens,
                "start_sentence": len(sentences) - len(current_chunk_sentences),
                "end_sentence": len(sentences) - 1,
                "has_overlap": chunk_id > 0
            })
        
        logger.info(f"ğŸ“Š ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
        return chunks
    
    def _get_overlap_sentences(self, sentences: List[str]) -> List[str]:
        """ê²¹ì¹¨ìš© ë¬¸ì¥ë“¤ ì„ íƒ"""
        if not sentences:
            return []
        
        # ë’¤ì—ì„œë¶€í„° overlap_tokensë§Œí¼ ì„ íƒ
        overlap_sentences = []
        tokens_count = 0
        
        for sentence in reversed(sentences):
            sentence_tokens = self.estimate_tokens(sentence)
            if tokens_count + sentence_tokens <= self.overlap_tokens:
                overlap_sentences.insert(0, sentence)
                tokens_count += sentence_tokens
            else:
                break
        
        return overlap_sentences
    
    def _split_long_sentence(self, sentence: str, start_chunk_id: int) -> List[Dict[str, Any]]:
        """ë„ˆë¬´ ê¸´ ë¬¸ì¥ì„ ê°•ì œë¡œ ë¶„í• """
        max_chars = int(self.max_input_tokens / 1.5)  # í•œê¸€ ê¸°ì¤€
        chunks = []
        
        for i in range(0, len(sentence), max_chars):
            chunk_text = sentence[i:i + max_chars]
            chunks.append({
                "chunk_id": start_chunk_id + i // max_chars,
                "text": chunk_text,
                "estimated_tokens": self.estimate_tokens(chunk_text),
                "start_sentence": -1,  # ë¬¸ì¥ ë‚´ ë¶„í• 
                "end_sentence": -1,
                "has_overlap": False,
                "is_sentence_split": True
            })
        
        return chunks
    
    def merge_chunk_results(self, chunk_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ì²­í¬ë³„ ê²°ê³¼ë¥¼ í†µí•©"""
        if not chunk_results:
            return {"error": "No chunk results to merge"}
        
        # ë‹¨ì¼ ì²­í¬ì¸ ê²½ìš°
        if len(chunk_results) == 1:
            return chunk_results[0]
        
        # ì—¬ëŸ¬ ì²­í¬ í†µí•©
        merged_result = {
            "action_items": [],
            "decisions": [],
            "key_points": [],
            "next_steps": [],
            "participants": set(),
            "summary": "",
            "metadata": {
                "total_chunks": len(chunk_results),
                "processing_method": "chunked"
            }
        }
        
        # ê° ì²­í¬ ê²°ê³¼ í†µí•©
        for i, result in enumerate(chunk_results):
            if isinstance(result, dict):
                # ì•¡ì…˜ ì•„ì´í…œ í†µí•©
                if "action_items" in result and isinstance(result["action_items"], list):
                    for item in result["action_items"]:
                        if isinstance(item, dict):
                            item["source_chunk"] = i
                            merged_result["action_items"].append(item)
                
                # ê¸°íƒ€ í•„ë“œ í†µí•©
                for field in ["decisions", "key_points", "next_steps"]:
                    if field in result and isinstance(result[field], list):
                        merged_result[field].extend(result[field])
                
                # ì°¸ì„ì í†µí•©
                if "participants" in result and isinstance(result["participants"], list):
                    merged_result["participants"].update(result["participants"])
        
        # ì¤‘ë³µ ì œê±°
        merged_result["decisions"] = list(set(merged_result["decisions"]))
        merged_result["key_points"] = list(set(merged_result["key_points"]))
        merged_result["next_steps"] = list(set(merged_result["next_steps"]))
        merged_result["participants"] = list(merged_result["participants"])
        
        # í†µí•© ìš”ì•½ ìƒì„±
        merged_result["summary"] = self._generate_merged_summary(chunk_results)
        
        # ì•¡ì…˜ ì•„ì´í…œ ì¤‘ë³µ ì œê±° ë° ìš°ì„ ìˆœìœ„ ì¬ì •ë ¬
        merged_result["action_items"] = self._deduplicate_action_items(
            merged_result["action_items"]
        )
        
        return merged_result
    
    def _generate_merged_summary(self, chunk_results: List[Dict[str, Any]]) -> str:
        """ì²­í¬ë³„ ìš”ì•½ì„ í†µí•©í•˜ì—¬ ì „ì²´ ìš”ì•½ ìƒì„±"""
        summaries = []
        for result in chunk_results:
            if isinstance(result, dict) and "summary" in result:
                summary = result["summary"]
                if summary and isinstance(summary, str):
                    summaries.append(summary.strip())
        
        if not summaries:
            return "íšŒì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì£¼ìš” ì‚¬í•­ë“¤ì„ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤."
        
        # ê°„ë‹¨í•œ í†µí•© (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ LLM ê¸°ë°˜ í†µí•©ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ)
        return " ".join(summaries)
    
    def _deduplicate_action_items(self, action_items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì•¡ì…˜ ì•„ì´í…œ ì¤‘ë³µ ì œê±°"""
        if not action_items:
            return []
        
        seen_tasks = set()
        deduplicated = []
        
        for item in action_items:
            if isinstance(item, dict) and "task" in item:
                task_key = item["task"].lower().strip()
                if task_key not in seen_tasks:
                    seen_tasks.add(task_key)
                    deduplicated.append(item)
        
        # ìš°ì„ ìˆœìœ„ë³„ ì •ë ¬
        priority_order = {"high": 0, "medium": 1, "low": 2}
        deduplicated.sort(key=lambda x: priority_order.get(x.get("priority", "medium"), 1))
        
        return deduplicated

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_chunking_processor = None

def get_chunking_processor(max_context_tokens: int = 32768) -> TtalKkakChunkingProcessor:
    """ì „ì—­ ì²­í‚¹ í”„ë¡œì„¸ì„œ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _chunking_processor
    if _chunking_processor is None:
        _chunking_processor = TtalKkakChunkingProcessor(max_context_tokens)
    return _chunking_processor 