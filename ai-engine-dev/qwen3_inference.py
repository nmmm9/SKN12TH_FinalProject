import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TtalkkacQwenInference:
    def __init__(self, base_model_name: str = "Qwen/Qwen2.5-14B-Instruct-AWQ", lora_path: str = None):
        self.base_model_name = base_model_name
        self.lora_path = lora_path
        self.tokenizer = None
        self.model = None
        
        # ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” í”„ë¡¬í”„íŠ¸
        self.system_prompt = """ë‹¹ì‹ ì€ íšŒì˜ë¡ì„ ë¶„ì„í•˜ì—¬ ì²´ê³„ì ì¸ ë…¸ì…˜ í”„ë¡œì íŠ¸ ê¸°íšì•ˆì„ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
íšŒì˜ ë‚´ìš©ì„ ì •í™•íˆ ë¶„ì„í•˜ê³  ì‹¤ë¬´ì—ì„œ ë°”ë¡œ í™œìš©í•  ìˆ˜ ìˆëŠ” êµ¬ì¡°í™”ëœ í”„ë¡œì íŠ¸ ê³„íšì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”."""

        self.user_prompt_template = """ë‹¤ìŒ íšŒì˜ë¡ì„ ë°”íƒ•ìœ¼ë¡œ ë…¸ì…˜ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì²´ê³„ì ì¸ í”„ë¡œì íŠ¸ ê¸°íšì•ˆì„ JSON í˜•ì‹ìœ¼ë¡œ ìƒì„±í•´ì£¼ì„¸ìš”.

**ìš”êµ¬ì‚¬í•­:**
1. í”„ë¡œì íŠ¸ëª…ê³¼ ëª©ì ì„ ëª…í™•íˆ ì •ì˜
2. í•µì‹¬ ì•„ì´ë””ì–´ì™€ ì‹¤í–‰ ê³„íš ìˆ˜ë¦½  
3. êµ¬ì²´ì ì¸ ëª©í‘œì™€ ê¸°ëŒ€ íš¨ê³¼ ë„ì¶œ
4. ì‹¤ë¬´ì§„ê³¼ ì¼ì • ê³„íš í¬í•¨
5. JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ

**íšŒì˜ë¡:**
{meeting_content}

**ì‘ë‹µ í˜•ì‹:**
JSON í˜•ì‹ìœ¼ë¡œ í”„ë¡œì íŠ¸ ê¸°íšì•ˆì„ ìƒì„±í•´ì£¼ì„¸ìš”."""
    
    def load_model(self):
        """íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ë¡œë“œ"""
        logger.info(f"ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”©: {self.base_model_name}")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.base_model_name,
            trust_remote_code=True,
            padding_side="right"
        )
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # AWQ ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
        self.model = AutoModelForCausalLM.from_pretrained(
            self.base_model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
            use_cache=True,
            low_cpu_mem_usage=True
        )
        
        # LoRA ì–´ëŒ‘í„° ë¡œë“œ (íŒŒì¸íŠœë‹ëœ ê²½ìš°)
        if self.lora_path:
            logger.info(f"LoRA ì–´ëŒ‘í„° ë¡œë”©: {self.lora_path}")
            self.model = PeftModel.from_pretrained(self.model, self.lora_path)
            self.model = self.model.merge_and_unload()  # LoRA ê°€ì¤‘ì¹˜ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ì— ë³‘í•©
        
        self.model.eval()
        logger.info("ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    
    def generate_project_plan(self, meeting_content: str, max_length: int = 2048, temperature: float = 0.7) -> str:
        """íšŒì˜ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ í”„ë¡œì íŠ¸ ê¸°íšì•ˆ ìƒì„±"""
        
        # ì‹¤ì œ í”„ë¡œì íŠ¸ í”„ë¡¬í”„íŠ¸ í˜•ì‹ìœ¼ë¡œ êµ¬ì„±
        user_message = self.user_prompt_template.format(meeting_content=meeting_content)
        
        # Qwen3 ì±„íŒ… í˜•ì‹ìœ¼ë¡œ êµ¬ì„±
        conversation = f"<|im_start|>system\n{self.system_prompt}<|im_end|>\n<|im_start|>user\n{user_message}<|im_end|>\n<|im_start|>assistant\n"
        
        # í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(conversation, return_tensors="pt", truncation=True, max_length=max_length-512)
        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
        
        # ìƒì„±
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=temperature,
                do_sample=True,
                top_p=0.9,
                top_k=50,
                repetition_penalty=1.1,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )
        
        # ë””ì½”ë”©
        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # assistant ë¶€ë¶„ë§Œ ì¶”ì¶œ
        assistant_start = generated_text.find("<|im_start|>assistant\n")
        if assistant_start != -1:
            response = generated_text[assistant_start + len("<|im_start|>assistant\n"):].strip()
            # <|im_end|> ì œê±°
            if response.endswith("<|im_end|>"):
                response = response[:-len("<|im_end|>")].strip()
            return response
        
        return generated_text
    
    def batch_generate(self, meeting_contents: list, output_file: str = None) -> list:
        """ì—¬ëŸ¬ íšŒì˜ë¡ì— ëŒ€í•´ ë°°ì¹˜ ì²˜ë¦¬"""
        results = []
        
        for i, content in enumerate(meeting_contents):
            logger.info(f"ì²˜ë¦¬ ì¤‘: {i+1}/{len(meeting_contents)}")
            
            try:
                result = self.generate_project_plan(content)
                results.append({
                    "id": i,
                    "meeting_content": content[:200] + "..." if len(content) > 200 else content,
                    "generated_plan": result,
                    "status": "success"
                })
            except Exception as e:
                logger.error(f"ìƒì„± ì‹¤íŒ¨ {i}: {e}")
                results.append({
                    "id": i,
                    "meeting_content": content[:200] + "..." if len(content) > 200 else content,
                    "generated_plan": "",
                    "status": "failed",
                    "error": str(e)
                })
        
        # ê²°ê³¼ ì €ì¥
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            logger.info(f"ê²°ê³¼ ì €ì¥: {output_file}")
        
        return results

def main():
    print("=" * 60)
    print("ğŸ¤– Ttalkkac Qwen3 ì¶”ë¡  í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # ëª¨ë¸ ê²½ë¡œ ì„¤ì • (íŒŒì¸íŠœë‹ëœ LoRA ì–´ëŒ‘í„° ê²½ë¡œ)
    lora_path = input("LoRA ì–´ëŒ‘í„° ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì—†ìœ¼ë©´ ì—”í„°): ").strip()
    if not lora_path:
        lora_path = None
        print("ë² ì´ìŠ¤ ëª¨ë¸ë¡œë§Œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
    
    # ì¶”ë¡ ê¸° ì´ˆê¸°í™”
    inferencer = TtalkkacQwenInference(lora_path=lora_path)
    inferencer.load_model()
    
    # í…ŒìŠ¤íŠ¸ íšŒì˜ë¡ (ì˜ˆì‹œ)
    test_meeting = """[14:00] ê¹€ëŒ€ë¦¬: ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜ íšŒì˜ëŠ” ìƒˆë¡œìš´ ëª¨ë°”ì¼ ì•± í”„ë¡œì íŠ¸ì— ëŒ€í•´ ë…¼ì˜í•˜ê² ìŠµë‹ˆë‹¤.
[14:01] ë°•ê³¼ì¥: íƒ€ê²Ÿ ì‚¬ìš©ìëŠ” 20-30ëŒ€ ì§ì¥ì¸ë“¤ì´ê³ , ì¼ì • ê´€ë¦¬ì™€ í•  ì¼ ê´€ë¦¬ë¥¼ í†µí•©í•œ ì•±ì„ ë§Œë“¤ ì˜ˆì •ì…ë‹ˆë‹¤.
[14:03] ì´íŒ€ì¥: ê°œë°œ ê¸°ê°„ì€ 3ê°œì›” ì •ë„ë¡œ ì˜ˆìƒí•˜ê³ , UI/UX ë””ìì¸ë¶€í„° ì‹œì‘í•´ì„œ MVP ì™„ì„±ê¹Œì§€ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.
[14:05] ê¹€ëŒ€ë¦¬: ì£¼ìš” ê¸°ëŠ¥ìœ¼ë¡œëŠ” ìº˜ë¦°ë” ì—°ë™, ì•Œë¦¼ ê¸°ëŠ¥, í˜‘ì—… ê¸°ëŠ¥ì´ í•„ìš”í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤.
[14:07] ë°•ê³¼ì¥: ì˜ˆì‚°ì€ 1ì–µì› ì •ë„ë¡œ ì±…ì •í–ˆê³ , ê°œë°œíŒ€ 5ëª…, ë””ìì´ë„ˆ 2ëª…ìœ¼ë¡œ êµ¬ì„±í•˜ê² ìŠµë‹ˆë‹¤.
[14:10] ì´íŒ€ì¥: ê·¸ëŸ¼ ë‹¤ìŒ ì£¼ê¹Œì§€ ìƒì„¸ ê¸°íšì„œë¥¼ ì‘ì„±í•´ì„œ ë‹¤ì‹œ ëª¨ì´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤."""
    
    print("\nğŸ“ í…ŒìŠ¤íŠ¸ íšŒì˜ë¡:")
    print(test_meeting[:300] + "...")
    
    print("\nğŸ”„ í”„ë¡œì íŠ¸ ê¸°íšì•ˆ ìƒì„± ì¤‘...")
    result = inferencer.generate_project_plan(test_meeting)
    
    print("\nâœ… ìƒì„±ëœ í”„ë¡œì íŠ¸ ê¸°íšì•ˆ:")
    print("-" * 40)
    print(result)
    
    # JSON íŒŒì‹± ì‹œë„
    try:
        parsed_json = json.loads(result)
        print("\nâœ… JSON íŒŒì‹± ì„±ê³µ!")
        print("ì£¼ìš” í•„ë“œ:", list(parsed_json.keys()))
    except:
        print("\nâš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨ - ì¶”ê°€ íŒŒì¸íŠœë‹ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()