"""
TtalKkak ìµœì¢… AI ì„œë²„ - Triplet + BERT í†µí•©
íšŒì˜ë¡ â†’ Triplet í•„í„°ë§ â†’ ê¸°íšì•ˆ â†’ Task Master PRD â†’ ì—…ë¬´ìƒì„±
"""

import os
import io
import json
import tempfile
import logging
from typing import Optional, Dict, Any, List
from contextlib import asynccontextmanager
import time

import torch
import numpy as np
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
from task_schemas import (
    TaskItem, MeetingAnalysisResult, ComplexityAnalysis, 
    TaskExpansionRequest, TaskExpansionResult, PipelineRequest, PipelineResult,
    calculate_task_complexity_advanced, validate_task_dependencies, 
    generate_task_id_sequence, TASK_SCHEMA_EXAMPLE
)
from meeting_analysis_prompts import (
    generate_meeting_analysis_system_prompt,
    generate_meeting_analysis_user_prompt,
    generate_task_expansion_system_prompt,
    generate_complexity_analysis_prompt,
    validate_meeting_analysis
)
from prd_generation_prompts import (
    generate_notion_project_prompt,
    generate_task_master_prd_prompt,
    format_notion_project,
    format_task_master_prd,
    validate_notion_project,
    validate_task_master_prd,
    NOTION_PROJECT_SCHEMA,
    TASK_MASTER_PRD_SCHEMA
)

# Triplet + BERT ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from triplet_processor import get_triplet_processor
    from bert_classifier import get_bert_classifier
    TRIPLET_AVAILABLE = True
    print("âœ… Triplet + BERT ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    logger.warning(f"âš ï¸ Triplet + BERT ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    TRIPLET_AVAILABLE = False

import uvicorn

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ê¸€ë¡œë²Œ ëª¨ë¸ ë³€ìˆ˜
whisper_model = None
qwen_model = None
qwen_tokenizer = None

# ìƒˆë¡œìš´ ì‘ë‹µ ëª¨ë¸ë“¤
class NotionProjectResponse(BaseModel):
    success: bool
    notion_project: Optional[Dict[str, Any]] = None
    formatted_notion: Optional[str] = None
    error: Optional[str] = None

class TaskMasterPRDResponse(BaseModel):
    success: bool
    prd_data: Optional[Dict[str, Any]] = None
    formatted_prd: Optional[str] = None
    error: Optional[str] = None

class TwoStageAnalysisRequest(BaseModel):
    transcript: str
    generate_notion: bool = True
    generate_tasks: bool = True
    num_tasks: int = 5
    additional_context: Optional[str] = None

class TwoStageAnalysisResponse(BaseModel):
    success: bool
    stage1_notion: Optional[Dict[str, Any]] = None
    stage2_prd: Optional[Dict[str, Any]] = None
    stage3_tasks: Optional[MeetingAnalysisResult] = None
    formatted_notion: Optional[str] = None
    formatted_prd: Optional[str] = None
    processing_time: Optional[float] = None
    error: Optional[str] = None

# Triplet ê´€ë ¨ ìƒˆë¡œìš´ ì‘ë‹µ ëª¨ë¸ë“¤
class EnhancedTranscriptionResponse(BaseModel):
    success: bool
    transcription: Optional[Dict[str, Any]] = None
    triplet_data: Optional[Dict[str, Any]] = None
    filtered_transcript: Optional[str] = None
    processing_stats: Optional[Dict[str, Any]] = None
    error: Optional[str] = None

class EnhancedTwoStageResult(BaseModel):
    success: bool
    triplet_stats: Optional[Dict[str, Any]] = None
    classification_stats: Optional[Dict[str, Any]] = None
    stage1_notion: Optional[Dict[str, Any]] = None
    stage2_prd: Optional[Dict[str, Any]] = None
    stage3_tasks: Optional[MeetingAnalysisResult] = None
    formatted_notion: Optional[str] = None
    formatted_prd: Optional[str] = None
    original_transcript_length: Optional[int] = None
    filtered_transcript_length: Optional[int] = None
    noise_reduction_ratio: Optional[float] = None
    processing_time: Optional[float] = None
    error: Optional[str] = None

# ê¸°ì¡´ ëª¨ë¸ë“¤
class TranscriptionResponse(BaseModel):
    success: bool
    transcription: Optional[Dict[str, Any]] = None
    error: Optional[str] = None

class AnalysisRequest(BaseModel):
    transcript: str
    num_tasks: int = 5
    additional_context: Optional[str] = None

class AnalysisResponse(BaseModel):
    success: bool
    analysis: Optional[MeetingAnalysisResult] = None
    error: Optional[str] = None

class HealthResponse(BaseModel):
    status: str
    gpu_available: bool
    gpu_count: int
    models_loaded: Dict[str, bool]
    memory_info: Optional[Dict[str, float]] = None

def load_whisperx():
    """WhisperX ëª¨ë¸ ë¡œë”©"""
    global whisper_model
    
    if whisper_model is None:
        logger.info("ğŸ¤ Loading WhisperX large-v3...")
        try:
            import whisperx
            device = "cuda" if torch.cuda.is_available() else "cpu"
            compute_type = "float16" if device == "cuda" else "int8"
            
            whisper_model = whisperx.load_model(
                "large-v3", 
                device, 
                compute_type=compute_type,
                language="ko"
            )
            logger.info("âœ… WhisperX loaded successfully")
            
        except Exception as e:
            logger.error(f"âŒ WhisperX loading failed: {e}")
            raise e
    
    return whisper_model

def load_qwen3():
    """Qwen3-32B-AWQ ëª¨ë¸ ë¡œë”© (VLLM ìµœì í™”)"""
    global qwen_model, qwen_tokenizer
    
    if qwen_model is None or qwen_tokenizer is None:
        logger.info("ğŸš€ Loading Qwen3-32B-AWQ with VLLM...")
        try:
            # VLLM ì‚¬ìš© ì—¬ë¶€ ì²´í¬
            use_vllm = os.getenv("USE_VLLM", "true").lower() == "true"
            
            if use_vllm:
                try:
                    logger.info("âš¡ Using VLLM for ultra-fast inference")
                    from vllm import LLM
                    from transformers import AutoTokenizer
                except ImportError as e:
                    logger.warning(f"âš ï¸ VLLM import failed: {e}")
                    logger.info("ğŸ”„ Falling back to Transformers...")
                    use_vllm = False
                
            if use_vllm:
                model_name = "Qwen/Qwen3-32B-AWQ"
                
                try:
                    # VLLM ëª¨ë¸ ë¡œë”©
                    qwen_model = LLM(
                        model=model_name,
                        tensor_parallel_size=1,
                        gpu_memory_utilization=0.7,  # GPU ë©”ëª¨ë¦¬ 70%ë¡œ ë³µì›
                        trust_remote_code=True,
                        quantization="awq",  # AWQ ì–‘ìí™” ëª…ì‹œ
                        max_model_len=16384,  # í† í° ê¸¸ì´ ì›ë˜ëŒ€ë¡œ ë³µì›
                        enforce_eager=True,  # CUDA ê·¸ë˜í”„ ë¹„í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½)
                        swap_space=4,  # 4GB swap spaceë¡œ ë³µì›
                        max_num_seqs=64  # ë™ì‹œ ì‹œí€€ìŠ¤ ìˆ˜ ì›ë˜ëŒ€ë¡œ ë³µì›
                        # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ ë³´ìˆ˜ì  ì„¤ì •
                    )
                    
                    # í† í¬ë‚˜ì´ì €ëŠ” ë³„ë„ ë¡œë”© (í…œí”Œë¦¿ ì ìš©ìš©)
                    qwen_tokenizer = AutoTokenizer.from_pretrained(
                        model_name, trust_remote_code=True
                    )
                    
                    logger.info("ğŸ‰ VLLM Qwen3-32B-AWQ loaded successfully")
                except Exception as e:
                    logger.error(f"âŒ VLLM model loading failed: {e}")
                    logger.info("ğŸ”„ Falling back to Transformers...")
                    use_vllm = False
                
            if not use_vllm:
                # ê¸°ì¡´ Transformers ë°©ì‹ (ë°±ì—…ìš©)
                try:
                    logger.info("ğŸ“š Using Transformers (fallback mode)")
                    from transformers import AutoTokenizer, AutoModelForCausalLM
                except ImportError as e:
                    logger.error(f"âŒ Transformers import failed: {e}")
                    raise RuntimeError("Both VLLM and Transformers unavailable!")
                
                model_name = "Qwen/Qwen3-32B-AWQ"
                
                qwen_tokenizer = AutoTokenizer.from_pretrained(
                    model_name, trust_remote_code=True
                )
                
                qwen_model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    device_map="auto",
                    torch_dtype=torch.float16,
                    trust_remote_code=True
                )
                
                logger.info("âœ… Transformers Qwen3-32B-AWQ loaded successfully")
            
        except Exception as e:
            logger.error(f"âŒ Qwen3-32B-AWQ loading failed: {e}")
            # VLLM ì‹¤íŒ¨ ì‹œ Transformersë¡œ ëŒ€ì²´
            if 'vllm' in str(e).lower():
                logger.warning("ğŸ”„ VLLM failed, falling back to Transformers...")
                os.environ["USE_VLLM"] = "false"
                return load_qwen3()  # ì¬ê·€ í˜¸ì¶œë¡œ Transformers ë¡œë”©
            raise e
    
    return qwen_model, qwen_tokenizer

def generate_structured_response(
    system_prompt: str, 
    user_prompt: str, 
    response_schema: Dict[str, Any],
    temperature: float = 0.3,
    max_input_tokens: int = 28000,  # Qwen3-32B AWQ ì•ˆì „ ë§ˆì§„ ì ìš©
    enable_chunking: bool = True
) -> Dict[str, Any]:
    """êµ¬ì¡°í™”ëœ ì‘ë‹µ ìƒì„± (ì²­í‚¹ ì§€ì›)"""
    
    # ì²­í‚¹ í•„ìš” ì—¬ë¶€ í™•ì¸
    if enable_chunking:
        try:
            from chunking_processor import get_chunking_processor
            chunking_processor = get_chunking_processor(max_context_tokens=32768)
            
            # ì „ì²´ í”„ë¡¬í”„íŠ¸ í† í° ìˆ˜ ì¶”ì •
            total_prompt = f"{system_prompt}\n{user_prompt}"
            estimated_tokens = chunking_processor.estimate_tokens(total_prompt)
            
            if estimated_tokens > max_input_tokens:
                logger.info(f"ğŸ”„ ì²­í‚¹ í•„ìš” ê°ì§€ (í† í°: {estimated_tokens} > {max_input_tokens})")
                return generate_chunked_response(
                    system_prompt, user_prompt, response_schema, 
                    temperature, chunking_processor
                )
            else:
                logger.info(f"ğŸ“ ë‹¨ì¼ ì²˜ë¦¬ (í† í°: {estimated_tokens})")
        except ImportError:
            logger.warning("âš ï¸ ì²­í‚¹ í”„ë¡œì„¸ì„œë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì²˜ë¦¬ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
    
    # ëª¨ë¸ ë¡œë”©
    qwen_model, qwen_tokenizer = load_qwen3()
    
    # ìŠ¤í‚¤ë§ˆ ì˜ˆì‹œ í¬í•¨ í”„ë¡¬í”„íŠ¸
    schema_prompt = f"""
{system_prompt}

**Response Schema:**
You must respond with a JSON object following this exact structure:
```json
{json.dumps(response_schema, indent=2, ensure_ascii=False)}
```

**Important Rules:**
1. Always return valid JSON format
2. Use Korean for all text content unless technical terms require English
3. Follow the exact schema structure
4. Include all required fields
5. Use appropriate data types for each field

{user_prompt}

**Response:**
```json
"""
    
    # ë©”ì‹œì§€ êµ¬ì„±
    messages = [{"role": "user", "content": schema_prompt}]
    
    # ì¶”ë¡  ì‹¤í–‰ (VLLM vs Transformers)
    use_vllm = os.getenv("USE_VLLM", "true").lower() == "true"
    
    if use_vllm and hasattr(qwen_model, 'generate'):
        # VLLM ì¶”ë¡  (ì´ˆê³ ì†!)
        logger.info("âš¡ VLLM ì¶”ë¡  ì‹¤í–‰...")
        start_time = time.time()
        
        from vllm import SamplingParams
        
        # ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° ì„¤ì •
        sampling_params = SamplingParams(
            max_tokens=2048,
            temperature=temperature,
            top_p=0.9,
            repetition_penalty=1.1,
            stop=None
        )
        
        # ë©”ì‹œì§€ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        text = qwen_tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        # VLLM ì¶”ë¡  ì‹¤í–‰
        outputs = qwen_model.generate([text], sampling_params)
        response = outputs[0].outputs[0].text
        
        inference_time = time.time() - start_time
        logger.info(f"ğŸ‰ VLLM ì¶”ë¡  ì™„ë£Œ: {inference_time:.3f}ì´ˆ")
        
    else:
        # Transformers ì¶”ë¡  (ê¸°ì¡´ ë°©ì‹)
        logger.info("ğŸ“š Transformers ì¶”ë¡  ì‹¤í–‰...")
        start_time = time.time()
        
        # í† í°í™”
        text = qwen_tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        inputs = qwen_tokenizer([text], return_tensors="pt").to(qwen_model.device)
        
        # ì¶”ë¡  ì‹¤í–‰
        with torch.no_grad():
            outputs = qwen_model.generate(
                **inputs,
                max_new_tokens=2048,
                temperature=temperature,
                do_sample=True,
                pad_token_id=qwen_tokenizer.eos_token_id,
                repetition_penalty=1.1,
                top_p=0.9
            )
        
        # ê²°ê³¼ ë””ì½”ë”©
        response = qwen_tokenizer.decode(
            outputs[0][len(inputs["input_ids"][0]):], 
            skip_special_tokens=True
        )
        
        inference_time = time.time() - start_time
        logger.info(f"âœ… Transformers ì¶”ë¡  ì™„ë£Œ: {inference_time:.3f}ì´ˆ")
    
    # JSON ì¶”ì¶œ ë° íŒŒì‹±
    try:
        # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
        if "```json" in response:
            json_start = response.find("```json") + 7
            json_end = response.find("```", json_start)
            if json_end == -1:
                json_content = response[json_start:].strip()
            else:
                json_content = response[json_start:json_end].strip()
        else:
            # JSON ë§ˆì»¤ê°€ ì—†ìœ¼ë©´ ì „ì²´ ì‘ë‹µì—ì„œ JSON ì°¾ê¸°
            json_content = response.strip()
        
        # JSON íŒŒì‹±
        parsed_result = json.loads(json_content)
        return parsed_result
        
    except json.JSONDecodeError as e:
        logger.error(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        logger.error(f"Raw response: {response[:500]}...")
        return {
            "error": f"JSON parsing failed: {str(e)}",
            "raw_response": response[:1000]
        }
    except Exception as e:
        logger.error(f"âŒ ì‘ë‹µ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return {
            "error": f"Response processing failed: {str(e)}",
            "raw_response": response[:1000] if 'response' in locals() else "No response"
        }

def generate_chunked_response(
    system_prompt: str,
    user_prompt: str, 
    response_schema: Dict[str, Any],
    temperature: float,
    chunking_processor
) -> Dict[str, Any]:
    """ì²­í‚¹ëœ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬"""
    try:
        logger.info("ğŸš€ ì²­í‚¹ ê¸°ë°˜ ì²˜ë¦¬ ì‹œì‘...")
        start_time = time.time()
        
        # 1. user_promptë¥¼ ì²­í‚¹
        chunks = chunking_processor.create_chunks_with_overlap(user_prompt)
        logger.info(f"ğŸ“Š ì´ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
        
        # 2. ê° ì²­í¬ë³„ë¡œ ì²˜ë¦¬
        chunk_results = []
        for i, chunk in enumerate(chunks):
            logger.info(f"ğŸ”„ ì²­í¬ {i+1}/{len(chunks)} ì²˜ë¦¬ ì¤‘... (í† í°: {chunk['estimated_tokens']})")
            
            # ì²­í¬ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìˆ˜ì •
            chunk_system_prompt = f"""{system_prompt}

**ì²­í‚¹ ì²˜ë¦¬ ì •ë³´:**
- í˜„ì¬ ì²­í¬: {i+1}/{len(chunks)}
- ì´ ì²­í¬ëŠ” ì „ì²´ íšŒì˜ì˜ ì¼ë¶€ì…ë‹ˆë‹¤
- ì´ ì²­í¬ì—ì„œ ë°œê²¬ë˜ëŠ” ë‚´ìš©ë§Œ ë¶„ì„í•˜ì„¸ìš”
- ë‹¤ë¥¸ ì²­í¬ì˜ ë‚´ìš©ì€ ë‚˜ì¤‘ì— í†µí•©ë©ë‹ˆë‹¤"""

            chunk_user_prompt = f"""ë‹¤ìŒì€ ì „ì²´ íšŒì˜ë¡ì˜ ì¼ë¶€ì…ë‹ˆë‹¤:

{chunk['text']}

ìœ„ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì´ ë¶€ë¶„ì—ì„œ ë°œê²¬ë˜ëŠ” ì•¡ì…˜ ì•„ì´í…œ, ê²°ì •ì‚¬í•­, í•µì‹¬ í¬ì¸íŠ¸ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”."""
            
            # ë‹¨ì¼ ì²­í¬ ì²˜ë¦¬
            chunk_result = generate_structured_response(
                system_prompt=chunk_system_prompt,
                user_prompt=chunk_user_prompt,
                response_schema=response_schema,
                temperature=temperature,
                enable_chunking=False  # ì¬ê·€ ë°©ì§€
            )
            
            chunk_results.append(chunk_result)
            logger.info(f"âœ… ì²­í¬ {i+1} ì²˜ë¦¬ ì™„ë£Œ")
        
        # 3. ê²°ê³¼ í†µí•©
        logger.info("ğŸ”„ ì²­í¬ ê²°ê³¼ í†µí•© ì¤‘...")
        merged_result = chunking_processor.merge_chunk_results(chunk_results)
        
        processing_time = time.time() - start_time
        logger.info(f"âœ… ì²­í‚¹ ì²˜ë¦¬ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {processing_time:.2f}ì´ˆ)")
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        merged_result["metadata"] = merged_result.get("metadata", {})
        merged_result["metadata"].update({
            "chunking_applied": True,
            "total_chunks": len(chunks),
            "processing_time": processing_time,
            "original_tokens": chunking_processor.estimate_tokens(user_prompt),
            "chunks_info": [
                {
                    "chunk_id": chunk["chunk_id"],
                    "tokens": chunk["estimated_tokens"],
                    "has_overlap": chunk["has_overlap"]
                }
                for chunk in chunks
            ]
        })
        
        return merged_result
        
    except Exception as e:
        logger.error(f"âŒ ì²­í‚¹ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return {
            "error": f"Chunking processing failed: {str(e)}",
            "fallback_message": "ì²­í‚¹ ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ ì²˜ë¦¬ë¥¼ ì‹œë„í•˜ì„¸ìš”."
        }

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì•± ì‹œì‘/ì¢…ë£Œ ì‹œ ëª¨ë¸ ë¡œë”©/ì •ë¦¬"""
    logger.info("ğŸš€ Starting TtalKkak Final AI Server with Triplets...")
    
    # ëª¨ë¸ë“¤ì„ ë¯¸ë¦¬ ë¡œë”© (ê¸°ë³¸ í™œì„±í™”ë¡œ ë³€ê²½)
    preload_enabled = os.getenv("PRELOAD_MODELS", "true").lower() == "true"
    logger.info(f"ğŸ”§ Model preloading: {'Enabled' if preload_enabled else 'Disabled'}")
    
    if preload_enabled:
        try:
            logger.info("ğŸ“¦ Starting parallel model preloading...")
            import asyncio
            
            # ë³‘ë ¬ ë¡œë”©ì„ ìœ„í•œ ë¹„ë™ê¸° ë˜í¼ í•¨ìˆ˜ë“¤ (ì‹œê°„ ì¸¡ì • í¬í•¨)
            async def load_whisperx_async():
                start_time = time.time()
                logger.info("ğŸ¤ Loading WhisperX...")
                load_whisperx()
                elapsed = time.time() - start_time
                logger.info(f"âœ… WhisperX loaded in {elapsed:.2f} seconds")
                return elapsed
            
            async def load_qwen3_async():
                start_time = time.time()
                logger.info("ğŸ§  Loading Qwen3-32B-AWQ...")
                load_qwen3()
                elapsed = time.time() - start_time
                logger.info(f"âœ… Qwen3-32B-AWQ loaded in {elapsed:.2f} seconds")
                return elapsed
            
            async def load_bert_async():
                if TRIPLET_AVAILABLE:
                    start_time = time.time()
                    logger.info("ğŸ” Loading BERT classifier...")
                    get_bert_classifier()
                    elapsed = time.time() - start_time
                    logger.info(f"âœ… BERT classifier loaded in {elapsed:.2f} seconds")
                    return elapsed
                return 0
            
            # ë³‘ë ¬ ë¡œë”© ì‹¤í–‰ (ì´ ì‹œê°„ ì¸¡ì •)
            total_start_time = time.time()
            
            results = await asyncio.gather(
                load_whisperx_async(),
                load_qwen3_async(), 
                load_bert_async(),
                return_exceptions=True
            )
            
            total_elapsed = time.time() - total_start_time
            
            # ê²°ê³¼ ì •ë¦¬
            whisperx_time, qwen3_time, bert_time = results
            if isinstance(whisperx_time, Exception):
                whisperx_time = 0
                logger.error(f"âŒ WhisperX loading failed: {whisperx_time}")
            if isinstance(qwen3_time, Exception):
                qwen3_time = 0
                logger.error(f"âŒ Qwen3 loading failed: {qwen3_time}")
            if isinstance(bert_time, Exception):
                bert_time = 0
                logger.error(f"âŒ BERT loading failed: {bert_time}")
            
            logger.info("ğŸ‰ All models preloaded successfully!")
            logger.info("â±ï¸  Loading Time Summary:")
            logger.info(f"   - WhisperX: {whisperx_time:.2f}s")
            logger.info(f"   - Qwen3-32B: {qwen3_time:.2f}s") 
            logger.info(f"   - BERT: {bert_time:.2f}s")
            logger.info(f"   - Total (parallel): {total_elapsed:.2f}s")
            logger.info(f"   - Sequential would take: {whisperx_time + qwen3_time + bert_time:.2f}s")
            logger.info(f"   - Time saved: {(whisperx_time + qwen3_time + bert_time) - total_elapsed:.2f}s")
            
        except Exception as e:
            logger.error(f"âŒ Model preloading failed: {e}")
            logger.info("âš ï¸ Server will continue with lazy loading")
    else:
        logger.info("ğŸ“ Using lazy loading (models load on first request)")
    
    yield
    
    logger.info("ğŸ›‘ Shutting down TtalKkak Final AI Server...")

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="TtalKkak Final AI Server with Triplets",
    description="WhisperX + Triplet + BERT + Qwen3-32B + 2-Stage PRD Process",
    version="3.1.0",
    lifespan=lifespan
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "TtalKkak Final AI Server with Triplets",
        "version": "3.1.0",
        "features": [
            "WhisperX Speech-to-Text",
            "Triplet Context Analysis", 
            "BERT Noise Filtering",
            "2-Stage PRD Process",
            "Notion Project Generation", 
            "Task Master PRD Format",
            "Advanced Task Generation"
        ],
        "workflow": "íšŒì˜ë¡ â†’ Triplet í•„í„°ë§ â†’ ê¸°íšì•ˆ â†’ Task Master PRD â†’ ì—…ë¬´ìƒì„±",
        "docs": "/docs"
    }

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    gpu_available = torch.cuda.is_available()
    gpu_count = torch.cuda.device_count() if gpu_available else 0
    
    memory_info = None
    if gpu_available:
        try:
            memory_info = {
                "allocated_gb": torch.cuda.memory_allocated() / 1024**3,
                "reserved_gb": torch.cuda.memory_reserved() / 1024**3,
                "total_gb": torch.cuda.get_device_properties(0).total_memory / 1024**3
            }
        except:
            pass
    
    return HealthResponse(
        status="healthy",
        gpu_available=gpu_available,
        gpu_count=gpu_count,
        models_loaded={
            "whisperx": whisper_model is not None,
            "qwen3": qwen_model is not None,
            "triplet_bert": TRIPLET_AVAILABLE
        },
        memory_info=memory_info
    )

@app.post("/transcribe", response_model=TranscriptionResponse)
async def transcribe_audio(audio: UploadFile = File(...)):
    """ìŒì„± íŒŒì¼ ì „ì‚¬ (WhisperX)"""
    try:
        logger.info(f"ğŸ¤ Transcribing audio: {audio.filename}")
        
        # ëª¨ë¸ ë¡œë”©
        whisper_model = load_whisperx()
        
        # ì˜¤ë””ì˜¤ íŒŒì¼ ì„ì‹œ ì €ì¥
        audio_content = await audio.read()
        
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_file:
            temp_file.write(audio_content)
            temp_path = temp_file.name
        
        try:
            # WhisperX ì „ì‚¬ ì‹¤í–‰
            result = whisper_model.transcribe(temp_path, batch_size=16)
            
            segments = result.get("segments", [])
            full_text = " ".join([seg.get("text", "") for seg in segments])
            
            logger.info(f"âœ… Transcription completed: {len(full_text)} characters")
            
            return TranscriptionResponse(
                success=True,
                transcription={
                    "segments": segments,
                    "full_text": full_text,
                    "language": result.get("language", "ko"),
                    "duration": sum([seg.get("end", 0) - seg.get("start", 0) for seg in segments])
                }
            )
            
        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            try:
                os.unlink(temp_path)
            except:
                pass
                
    except Exception as e:
        logger.error(f"âŒ Transcription error: {e}")
        return TranscriptionResponse(
            success=False,
            error=str(e)
        )

@app.post("/transcribe-enhanced", response_model=EnhancedTranscriptionResponse)
async def transcribe_audio_enhanced(
    audio: UploadFile = File(...),
    enable_bert_filtering: bool = True,
    save_noise_log: bool = True
):
    """í–¥ìƒëœ ìŒì„± íŒŒì¼ ì „ì‚¬ (Triplet + BERT í•„í„°ë§)"""
    try:
        logger.info(f"ğŸ¤ Enhanced transcribing: {audio.filename}")
        
        # 1. ê¸°ë³¸ WhisperX ì „ì‚¬
        basic_result = await transcribe_audio(audio)
        if not basic_result.success:
            return EnhancedTranscriptionResponse(
                success=False,
                error=basic_result.error
            )
        
        # 2. Triplet í”„ë¡œì„¸ì„œë¡œ ì²˜ë¦¬ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if TRIPLET_AVAILABLE and enable_bert_filtering:
            try:
                triplet_processor = get_triplet_processor()
                
                enhanced_result = triplet_processor.process_whisperx_result(
                    whisperx_result=basic_result.transcription,
                    enable_bert_filtering=enable_bert_filtering,
                    save_noise_log=save_noise_log
                )
                
                if enhanced_result["success"]:
                    logger.info("âœ… Enhanced transcription with Triplets completed")
                    
                    return EnhancedTranscriptionResponse(
                        success=True,
                        transcription=basic_result.transcription,
                        triplet_data=enhanced_result.get("triplet_data"),
                        filtered_transcript=enhanced_result.get("filtered_transcript"),
                        processing_stats=enhanced_result.get("processing_stats")
                    )
                else:
                    logger.warning("âš ï¸ Triplet processing failed, using basic transcription")
            except Exception as e:
                logger.warning(f"âš ï¸ Triplet processing error: {e}")
        
        # 3. Triplet ì‚¬ìš© ë¶ˆê°€ëŠ¥ ì‹œ ê¸°ë³¸ ê²°ê³¼ ë°˜í™˜
        return EnhancedTranscriptionResponse(
            success=True,
            transcription=basic_result.transcription,
            filtered_transcript=basic_result.transcription["full_text"],
            processing_stats={"triplet_available": TRIPLET_AVAILABLE}
        )
        
    except Exception as e:
        logger.error(f"âŒ Enhanced transcription error: {e}")
        return EnhancedTranscriptionResponse(
            success=False,
            error=str(e)
        )

@app.post("/generate-notion-project", response_model=NotionProjectResponse)
async def generate_notion_project(request: AnalysisRequest):
    """1ë‹¨ê³„: íšŒì˜ë¡ â†’ ë…¸ì…˜ ê¸°íšì•ˆ ìƒì„±"""
    try:
        logger.info("ğŸ“ Stage 1: Generating Notion project document...")
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        system_prompt = "ë‹¹ì‹ ì€ íšŒì˜ë¡ì„ ë¶„ì„í•˜ì—¬ ì²´ê³„ì ì¸ í”„ë¡œì íŠ¸ ê¸°íšì•ˆì„ ì‘ì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
        user_prompt = generate_notion_project_prompt(request.transcript)
        
        # êµ¬ì¡°í™”ëœ ì‘ë‹µ ìƒì„±
        result = generate_structured_response(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            response_schema=NOTION_PROJECT_SCHEMA,
            temperature=0.3
        )
        
        if "error" in result:
            return NotionProjectResponse(
                success=False,
                error=result["error"]
            )
        
        # ë°ì´í„° ê²€ì¦
        validated_result = validate_notion_project(result)
        
        # ë…¸ì…˜ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…
        formatted_notion = format_notion_project(validated_result)
        
        logger.info("âœ… Stage 1 completed: Notion project generated")
        
        return NotionProjectResponse(
            success=True,
            notion_project=validated_result,
            formatted_notion=formatted_notion
        )
        
    except Exception as e:
        logger.error(f"âŒ Notion project generation error: {e}")
        return NotionProjectResponse(
            success=False,
            error=str(e)
        )

@app.post("/generate-task-master-prd", response_model=TaskMasterPRDResponse)
async def generate_task_master_prd(notion_project: Dict[str, Any]):
    """2ë‹¨ê³„: ë…¸ì…˜ ê¸°íšì•ˆ â†’ Task Master PRD ë³€í™˜"""
    try:
        logger.info("ğŸ”„ Stage 2: Converting to Task Master PRD format...")
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        system_prompt = "ë‹¹ì‹ ì€ ê¸°íšì•ˆì„ Task Master PRD í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
        user_prompt = generate_task_master_prd_prompt(notion_project)
        
        # êµ¬ì¡°í™”ëœ ì‘ë‹µ ìƒì„±
        result = generate_structured_response(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            response_schema=TASK_MASTER_PRD_SCHEMA,
            temperature=0.3
        )
        
        if "error" in result:
            return TaskMasterPRDResponse(
                success=False,
                error=result["error"]
            )
        
        # ë°ì´í„° ê²€ì¦
        validated_result = validate_task_master_prd(result)
        
        # Task Master PRD í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…
        formatted_prd = format_task_master_prd(validated_result)
        
        logger.info("âœ… Stage 2 completed: Task Master PRD generated")
        
        return TaskMasterPRDResponse(
            success=True,
            prd_data=validated_result,
            formatted_prd=formatted_prd
        )
        
    except Exception as e:
        logger.error(f"âŒ Task Master PRD generation error: {e}")
        return TaskMasterPRDResponse(
            success=False,
            error=str(e)
        )

@app.post("/two-stage-analysis", response_model=TwoStageAnalysisResponse)
async def two_stage_analysis(request: TwoStageAnalysisRequest):
    """2ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤ í†µí•© ë¶„ì„"""
    try:
        logger.info("ğŸš€ Starting 2-stage analysis process...")
        
        start_time = time.time()
        
        # 1ë‹¨ê³„: ë…¸ì…˜ ê¸°íšì•ˆ ìƒì„±
        stage1_result = None
        if request.generate_notion:
            logger.info("ğŸ“ Stage 1: Generating Notion project...")
            notion_request = AnalysisRequest(
                transcript=request.transcript,
                additional_context=request.additional_context
            )
            stage1_response = await generate_notion_project(notion_request)
            
            if not stage1_response.success:
                return TwoStageAnalysisResponse(
                    success=False,
                    error=f"Stage 1 failed: {stage1_response.error}"
                )
            
            stage1_result = stage1_response.notion_project
        
        # 2ë‹¨ê³„: Task Master PRD ë³€í™˜
        stage2_result = None
        if request.generate_tasks and stage1_result:
            logger.info("ğŸ”„ Stage 2: Converting to Task Master PRD...")
            stage2_response = await generate_task_master_prd(stage1_result)
            
            if not stage2_response.success:
                return TwoStageAnalysisResponse(
                    success=False,
                    error=f"Stage 2 failed: {stage2_response.error}"
                )
            
            stage2_result = stage2_response.prd_data
        
        # 3ë‹¨ê³„: íƒœìŠ¤í¬ ìƒì„± (Task Master ë°©ì‹)
        stage3_result = None
        if request.generate_tasks and stage2_result:
            logger.info("ğŸ¯ Stage 3: Generating tasks using Task Master approach...")
            
            # Task Master PRDë¥¼ ì‚¬ìš©í•œ íƒœìŠ¤í¬ ìƒì„±
            prd_content = format_task_master_prd(stage2_result)
            system_prompt = generate_meeting_analysis_system_prompt(request.num_tasks)
            user_prompt = f"""
            ë‹¤ìŒ PRD ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ {request.num_tasks}ê°œì˜ êµ¬ì²´ì ì¸ ê°œë°œ íƒœìŠ¤í¬ë¥¼ ìƒì„±í•˜ì„¸ìš”:
            
            {prd_content}
            
            Task Masterì˜ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì„ ì ìš©í•˜ì—¬ ì²´ê³„ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ íƒœìŠ¤í¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.
            """
            
            result = generate_structured_response(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                response_schema=TASK_SCHEMA_EXAMPLE,
                temperature=0.3
            )
            
            # ê²°ê³¼ í›„ì²˜ë¦¬
            validated_result = validate_meeting_analysis(result)
            
            # TaskItem ê°ì²´ë¡œ ë³€í™˜
            task_items = []
            for i, item in enumerate(validated_result.get("action_items", [])):
                task_item = TaskItem(
                    id=i + 1,
                    title=item.get("task", f"Task {i+1}"),
                    description=item.get("task", ""),
                    priority=item.get("priority", "medium"),
                    assignee=item.get("assignee", "ë¯¸ì§€ì •"),
                    deadline=item.get("deadline", "ë¯¸ì •"),
                    complexity=calculate_task_complexity_advanced(
                        TaskItem(
                            id=i + 1,
                            title=item.get("task", ""),
                            description=item.get("task", ""),
                            priority=item.get("priority", "medium")
                        )
                    ),
                    status="pending"
                )
                task_items.append(task_item)
            
            # ì˜ì¡´ì„± ê²€ì¦
            task_items = validate_task_dependencies(task_items)
            
            # ìµœì¢… ê²°ê³¼ êµ¬ì„±
            stage3_result = MeetingAnalysisResult(
                summary=validated_result.get("summary", ""),
                action_items=task_items,
                decisions=validated_result.get("decisions", []),
                next_steps=validated_result.get("next_steps", []),
                key_points=validated_result.get("key_points", []),
                participants=validated_result.get("participants", []),
                follow_up=validated_result.get("follow_up", {}),
                metadata={
                    "processing_time": time.time() - start_time,
                    "total_tasks": len(task_items),
                    "process_type": "2-stage-task-master"
                }
            )
        
        total_time = time.time() - start_time
        
        return TwoStageAnalysisResponse(
            success=True,
            stage1_notion=stage1_result,
            stage2_prd=stage2_result,
            stage3_tasks=stage3_result,
            formatted_notion=format_notion_project(stage1_result) if stage1_result else None,
            formatted_prd=format_task_master_prd(stage2_result) if stage2_result else None,
            processing_time=total_time
        )
        
    except Exception as e:
        logger.error(f"âŒ 2-stage analysis error: {e}")
        return TwoStageAnalysisResponse(
            success=False,
            error=str(e)
        )

@app.post("/two-stage-pipeline-text", response_model=EnhancedTwoStageResult)
async def enhanced_two_stage_pipeline_text(request: dict):
    """í…ìŠ¤íŠ¸ ì…ë ¥ ì „ìš© 2ë‹¨ê³„ íŒŒì´í”„ë¼ì¸: í…ìŠ¤íŠ¸ â†’ Triplet í•„í„°ë§ â†’ 2ë‹¨ê³„ ë¶„ì„"""
    try:
        logger.info("ğŸš€ Starting text-based 2-stage pipeline...")
        
        transcript = request.get("transcript", "")
        if not transcript:
            raise ValueError("transcriptê°€ í•„ìš”í•©ë‹ˆë‹¤")
            
        enable_bert_filtering = request.get("enable_bert_filtering", True)
        
        # VLLM ì‚¬ìš© ì—¬ë¶€ í™•ì¸
        use_vllm = os.getenv("USE_VLLM", "true").lower() == "true"
        
        # í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ì²˜ë¦¬í•˜ì—¬ Triplet ìƒì„± ë° í•„í„°ë§
        if TRIPLET_AVAILABLE and enable_bert_filtering:
            try:
                triplet_processor = get_triplet_processor()
                # í…ìŠ¤íŠ¸ë¥¼ WhisperX í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                mock_whisperx_result = {
                    "segments": [{"text": transcript, "start": 0, "end": 60}],
                    "full_text": transcript,
                    "language": "ko"
                }
                
                enhanced_result = triplet_processor.process_whisperx_result(
                    whisperx_result=mock_whisperx_result,
                    enable_bert_filtering=enable_bert_filtering,
                    save_noise_log=False
                )
                
                if enhanced_result["success"]:
                    filtered_transcript = enhanced_result["filtered_transcript"]
                    triplet_stats = enhanced_result.get("triplet_stats", {})
                    classification_stats = enhanced_result.get("classification_stats", {})
                else:
                    filtered_transcript = transcript
                    triplet_stats = {}
                    classification_stats = {}
                    
            except Exception as e:
                logger.warning(f"Triplet ì²˜ë¦¬ ì‹¤íŒ¨, ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©: {e}")
                filtered_transcript = transcript
                triplet_stats = {}
                classification_stats = {}
        else:
            filtered_transcript = transcript
            triplet_stats = {}
            classification_stats = {}
        
        # Stage 1: Notion í”„ë¡œì íŠ¸ ìƒì„±
        stage1_notion = None
        if request.get("generate_notion", True):
            try:
                # ê¸°ì¡´ generate_notion_project í•¨ìˆ˜ ë¡œì§ ì‚¬ìš©
                system_prompt = generate_notion_project_prompt()
                user_prompt = f"ë‹¤ìŒ íšŒì˜ë¡ì„ ë°”íƒ•ìœ¼ë¡œ ë…¸ì…˜ ê¸°íšì•ˆì„ ì‘ì„±í•´ì£¼ì„¸ìš”:\n\n{filtered_transcript}"
                
                if use_vllm and qwen_model and qwen_tokenizer:
                    from vllm import SamplingParams
                    sampling_params = SamplingParams(
                        temperature=0.3,
                        max_tokens=2048,
                        stop=["<|im_end|>", "<|endoftext|>"]
                    )
                    
                    messages = [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ]
                    
                    formatted_prompt = qwen_tokenizer.apply_chat_template(
                        messages, tokenize=False, add_generation_prompt=True
                    )
                    
                    outputs = qwen_model.generate([formatted_prompt], sampling_params)
                    result_text = outputs[0].outputs[0].text.strip()
                    
                    try:
                        import json
                        stage1_notion = json.loads(result_text)
                    except:
                        stage1_notion = {"title": "AI í”„ë¡œì íŠ¸", "overview": result_text}
                        
            except Exception as e:
                logger.error(f"Notion ìƒì„± ì‹¤íŒ¨: {e}")
                stage1_notion = None
        
        # Stage 2: PRD ìƒì„±
        stage2_prd = None
        if stage1_notion and request.get("generate_prd", True):
            try:
                system_prompt = generate_task_master_prd_prompt()
                user_prompt = f"ë‹¤ìŒ ë…¸ì…˜ í”„ë¡œì íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ Task Master PRDë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:\n\n{json.dumps(stage1_notion, ensure_ascii=False, indent=2)}"
                
                if use_vllm and qwen_model and qwen_tokenizer:
                    from vllm import SamplingParams
                    sampling_params = SamplingParams(
                        temperature=0.3,
                        max_tokens=2048,
                        stop=["<|im_end|>", "<|endoftext|>"]
                    )
                    
                    messages = [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ]
                    
                    formatted_prompt = qwen_tokenizer.apply_chat_template(
                        messages, tokenize=False, add_generation_prompt=True
                    )
                    
                    outputs = qwen_model.generate([formatted_prompt], sampling_params)
                    result_text = outputs[0].outputs[0].text.strip()
                    
                    try:
                        stage2_prd = json.loads(result_text)
                    except:
                        stage2_prd = {"title": "PRD", "overview": result_text}
                        
            except Exception as e:
                logger.error(f"PRD ìƒì„± ì‹¤íŒ¨: {e}")
                stage2_prd = None
        
        # Stage 3: ì—…ë¬´ ìƒì„±
        stage3_tasks = None
        if stage2_prd and request.get("generate_tasks", True):
            try:
                system_prompt = generate_meeting_analysis_system_prompt()
                user_prompt = f"ë‹¤ìŒ PRDë¥¼ ë°”íƒ•ìœ¼ë¡œ ì—…ë¬´ íƒœìŠ¤í¬ë“¤ì„ ìƒì„±í•´ì£¼ì„¸ìš”:\n\n{json.dumps(stage2_prd, ensure_ascii=False, indent=2)}"
                
                if use_vllm and qwen_model and qwen_tokenizer:
                    from vllm import SamplingParams
                    sampling_params = SamplingParams(
                        temperature=0.3,
                        max_tokens=2048,
                        stop=["<|im_end|>", "<|endoftext|>"]
                    )
                    
                    messages = [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ]
                    
                    formatted_prompt = qwen_tokenizer.apply_chat_template(
                        messages, tokenize=False, add_generation_prompt=True
                    )
                    
                    outputs = qwen_model.generate([formatted_prompt], sampling_params)
                    result_text = outputs[0].outputs[0].text.strip()
                    
                    try:
                        stage3_tasks = json.loads(result_text)
                    except:
                        stage3_tasks = {"action_items": []}
                        
            except Exception as e:
                logger.error(f"ì—…ë¬´ ìƒì„± ì‹¤íŒ¨: {e}")
                stage3_tasks = None
        
        return EnhancedTwoStageResult(
            success=True,
            triplet_stats=triplet_stats,
            classification_stats=classification_stats,
            stage1_notion=stage1_notion,
            stage2_prd=stage2_prd,
            stage3_tasks=stage3_tasks,
            formatted_notion=format_notion_project(stage1_notion) if stage1_notion else None,
            formatted_prd=format_task_master_prd(stage2_prd) if stage2_prd else None,
            original_transcript_length=len(transcript),
            filtered_transcript_length=len(filtered_transcript),
            noise_reduction_ratio=1.0 - (len(filtered_transcript) / len(transcript)) if transcript else 0,
            processing_time=time.time() - time.time()
        )
        
    except Exception as e:
        logger.error(f"âŒ Text-based 2-stage pipeline error: {e}")
        return EnhancedTwoStageResult(
            success=False,
            error=str(e)
        )

@app.post("/two-stage-pipeline", response_model=EnhancedTwoStageResult)
async def enhanced_two_stage_pipeline(
    audio: UploadFile = File(...),
    enable_bert_filtering: bool = True,
    save_noise_log: bool = True,
    generate_notion: bool = True,
    generate_tasks: bool = True,
    num_tasks: int = 5
):
    """ìµœì¢… ì „ì²´ íŒŒì´í”„ë¼ì¸: ìŒì„± â†’ Triplet í•„í„°ë§ â†’ 2ë‹¨ê³„ ë¶„ì„"""
    try:
        logger.info("ğŸš€ Starting enhanced 2-stage pipeline with Triplets...")
        
        start_time = time.time()
        
        # 1ë‹¨ê³„: í–¥ìƒëœ ì „ì‚¬ (Triplet + BERT)
        logger.info("ğŸ“ Step 1: Enhanced transcription...")
        enhanced_transcribe_result = await transcribe_audio_enhanced(
            audio=audio,
            enable_bert_filtering=enable_bert_filtering,
            save_noise_log=save_noise_log
        )
        
        if not enhanced_transcribe_result.success:
            return EnhancedTwoStageResult(
                success=False,
                error=enhanced_transcribe_result.error
            )
        
        # 2ë‹¨ê³„: í•„í„°ë§ëœ í…ìŠ¤íŠ¸ë¡œ 2ë‹¨ê³„ ë¶„ì„
        logger.info("ğŸ§  Step 2: Running 2-stage analysis on filtered content...")
        filtered_text = enhanced_transcribe_result.filtered_transcript or \
                       enhanced_transcribe_result.transcription["full_text"]
        
        # ğŸ”¥ ì²­í‚¹ í•„ìš”ì„± ì²´í¬ ë° ì²˜ë¦¬
        try:
            from chunking_processor import get_chunking_processor
            chunking_processor = get_chunking_processor(max_context_tokens=32768)
            
            # í•„í„°ë§ëœ í…ìŠ¤íŠ¸ í† í° ìˆ˜ í™•ì¸
            estimated_tokens = chunking_processor.estimate_tokens(filtered_text)
            logger.info(f"ğŸ“Š í•„í„°ë§ëœ í…ìŠ¤íŠ¸ í† í° ìˆ˜: {estimated_tokens}")
            
            # ë…¸ì´ì¦ˆ ì œê±° íš¨ê³¼ ë¡œê·¸
            original_tokens = chunking_processor.estimate_tokens(
                enhanced_transcribe_result.transcription["full_text"]
            )
            token_reduction = ((original_tokens - estimated_tokens) / original_tokens * 100) if original_tokens > 0 else 0
            logger.info(f"ğŸ¯ í† í° ê°ì†Œ íš¨ê³¼: {original_tokens} â†’ {estimated_tokens} ({token_reduction:.1f}% ê°ì†Œ)")
            
        except ImportError:
            logger.warning("âš ï¸ ì²­í‚¹ í”„ë¡œì„¸ì„œë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            estimated_tokens = len(filtered_text) * 1.5  # ëŒ€ëµì  ì¶”ì •
        
        analysis_request = TwoStageAnalysisRequest(
            transcript=filtered_text,
            generate_notion=generate_notion,
            generate_tasks=generate_tasks,
            num_tasks=num_tasks
        )
        analysis_result = await two_stage_analysis(analysis_request)
        
        total_time = time.time() - start_time
        
        # ê²°ê³¼ í†µê³„ ê³„ì‚°
        original_length = len(enhanced_transcribe_result.transcription["full_text"])
        filtered_length = len(filtered_text)
        noise_reduction = 1.0 - (filtered_length / original_length) if original_length > 0 else 0.0
        
        return EnhancedTwoStageResult(
            success=True,
            triplet_stats=enhanced_transcribe_result.triplet_data,
            classification_stats=enhanced_transcribe_result.processing_stats,
            stage1_notion=analysis_result.stage1_notion if analysis_result.success else None,
            stage2_prd=analysis_result.stage2_prd if analysis_result.success else None,
            stage3_tasks=analysis_result.stage3_tasks if analysis_result.success else None,
            formatted_notion=analysis_result.formatted_notion if analysis_result.success else None,
            formatted_prd=analysis_result.formatted_prd if analysis_result.success else None,
            original_transcript_length=original_length,
            filtered_transcript_length=filtered_length,
            noise_reduction_ratio=noise_reduction,
            processing_time=total_time
        )
        
    except Exception as e:
        logger.error(f"âŒ Enhanced 2-stage pipeline error: {e}")
        return EnhancedTwoStageResult(
            success=False,
            error=str(e)
        )

@app.post("/pipeline-final", response_model=Dict[str, Any])
async def final_pipeline(
    audio: UploadFile = File(None),
    transcript: str = None,
    generate_notion: bool = True,
    generate_tasks: bool = True,
    num_tasks: int = 5
):
    """ğŸš€ ìµœì¢… ì „ì²´ íŒŒì´í”„ë¼ì¸: ìŒì„±/í…ìŠ¤íŠ¸ ìë™ ê°ì§€ â†’ VLLM ì´ˆê³ ì† ë¶„ì„"""
    try:
        logger.info("ğŸš€ Starting final pipeline with 2-stage process...")
        
        start_time = time.time()
        
        # ì…ë ¥ íƒ€ì… ìë™ ê°ì§€ ë° BERT í•„í„°ë§
        if transcript:
            # í…ìŠ¤íŠ¸ ì…ë ¥ + BERT í•„í„°ë§
            logger.info("ğŸ“ Text input detected, applying BERT filtering...")
            if TRIPLET_AVAILABLE:
                try:
                    triplet_processor = get_triplet_processor()
                    mock_whisperx_result = {
                        "segments": [{"text": transcript, "start": 0, "end": 60}],
                        "full_text": transcript,
                        "language": "ko"
                    }
                    
                    enhanced_result = triplet_processor.process_whisperx_result(
                        whisperx_result=mock_whisperx_result,
                        enable_bert_filtering=True,
                        save_noise_log=False
                    )
                    
                    if enhanced_result["success"]:
                        full_text = enhanced_result["filtered_transcript"]
                        logger.info(f"âœ… BERT filtering applied: {len(transcript)} â†’ {len(full_text)} chars")
                    else:
                        full_text = transcript
                        logger.warning("BERT filtering failed, using original text")
                except Exception as e:
                    logger.warning(f"BERT filtering error: {e}, using original text")
                    full_text = transcript
            else:
                full_text = transcript
        elif audio and audio.filename:
            # ìŒì„± íŒŒì¼ ì…ë ¥
            logger.info("ğŸ“ Step 1: Transcribing audio...")
            transcribe_result = await transcribe_audio(audio)
            if not transcribe_result.success:
                return {
                    "success": False,
                    "step": "transcription",
                    "error": transcribe_result.error
                }
            
            # ìŒì„± ì „ì‚¬ ê²°ê³¼ì— BERT í•„í„°ë§ ì ìš©
            raw_text = transcribe_result.transcription["full_text"]
            if TRIPLET_AVAILABLE:
                try:
                    triplet_processor = get_triplet_processor()
                    enhanced_result = triplet_processor.process_whisperx_result(
                        whisperx_result=transcribe_result.transcription,
                        enable_bert_filtering=True,
                        save_noise_log=False
                    )
                    
                    if enhanced_result["success"]:
                        full_text = enhanced_result["filtered_transcript"]
                        logger.info(f"âœ… BERT filtering applied to audio: {len(raw_text)} â†’ {len(full_text)} chars")
                    else:
                        full_text = raw_text
                        logger.warning("BERT filtering failed on audio, using original transcription")
                except Exception as e:
                    logger.warning(f"BERT filtering error on audio: {e}, using original transcription")
                    full_text = raw_text
            else:
                full_text = raw_text
        else:
            return {
                "success": False,
                "step": "input",
                "error": "Either transcript or audio file is required"
            }
        
        # 2ë‹¨ê³„: 2ë‹¨ê³„ ë¶„ì„
        logger.info("ğŸ§  Step 2: Running 2-stage analysis...")
        analysis_request = TwoStageAnalysisRequest(
            transcript=full_text,
            generate_notion=generate_notion,
            generate_tasks=generate_tasks,
            num_tasks=num_tasks
        )
        analysis_result = await two_stage_analysis(analysis_request)
        
        total_time = time.time() - start_time
        
        return {
            "success": True,
            "step": "completed",
            "transcription": transcribe_result.transcription,
            "analysis": {
                "notion_project": analysis_result.stage1_notion,
                "task_master_prd": analysis_result.stage2_prd,
                "generated_tasks": analysis_result.stage3_tasks,
                "formatted_notion": analysis_result.formatted_notion,
                "formatted_prd": analysis_result.formatted_prd
            },
            "processing_time": total_time,
            "model_info": {
                "whisperx": "large-v3",
                "qwen3": "Qwen3-32B-AWQ",
                "process": "2-stage-task-master",
                "triplet_available": TRIPLET_AVAILABLE
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ Final pipeline error: {e}")
        return {
            "success": False,
            "step": "pipeline",
            "error": str(e)
        }

if __name__ == "__main__":
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    host = os.getenv("HOST", "0.0.0.0")
    port = int(os.getenv("PORT", "8000"))
    workers = int(os.getenv("WORKERS", "1"))
    
    logger.info(f"ğŸš€ Starting final server with Triplets on {host}:{port}")
    
    uvicorn.run(
        "ai_server_final_with_triplets:app",
        host=host,
        port=port,
        workers=workers,
        reload=False,
        log_level="info"
    )