"""
TtalKkac ìµœì¢… AI ì„œë²„ - Triplet + BERT í†µí•©
íšŒì˜ë¡ â†’ Triplet í•„í„°ë§ â†’ ê¸°íšì•ˆ â†’ Task Master PRD â†’ ì—…ë¬´ìƒì„±
"""

import os
import io
import json
import tempfile
import logging
from typing import Optional, Dict, Any, List
from contextlib import asynccontextmanager
import time

import torch
import numpy as np
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
from task_schemas import (
    TaskItem, MeetingAnalysisResult, ComplexityAnalysis, 
    TaskExpansionRequest, TaskExpansionResult, PipelineRequest, PipelineResult,
    calculate_task_complexity_advanced, validate_task_dependencies, 
    generate_task_id_sequence, TASK_SCHEMA_EXAMPLE
)
from meeting_analysis_prompts import (
    generate_meeting_analysis_system_prompt,
    generate_meeting_analysis_user_prompt,
    generate_task_expansion_system_prompt,
    generate_complexity_analysis_prompt,
    validate_meeting_analysis
)
from prd_generation_prompts import (
    generate_notion_project_prompt,
    generate_task_master_prd_prompt,
    format_notion_project,
    format_task_master_prd,
    validate_notion_project,
    validate_task_master_prd,
    NOTION_PROJECT_SCHEMA,
    TASK_MASTER_PRD_SCHEMA
)

# Triplet + BERT ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from triplet_processor import get_triplet_processor
    from bert_classifier import get_bert_classifier
    TRIPLET_AVAILABLE = True
    logger.info("âœ… Triplet + BERT ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    logger.warning(f"âš ï¸ Triplet + BERT ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    TRIPLET_AVAILABLE = False

import uvicorn

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ê¸€ë¡œë²Œ ëª¨ë¸ ë³€ìˆ˜
whisper_model = None
qwen_model = None
qwen_tokenizer = None

# ìƒˆë¡œìš´ ì‘ë‹µ ëª¨ë¸ë“¤
class NotionProjectResponse(BaseModel):
    success: bool
    notion_project: Optional[Dict[str, Any]] = None
    formatted_notion: Optional[str] = None
    error: Optional[str] = None

class TaskMasterPRDResponse(BaseModel):
    success: bool
    prd_data: Optional[Dict[str, Any]] = None
    formatted_prd: Optional[str] = None
    error: Optional[str] = None

class TwoStageAnalysisRequest(BaseModel):
    transcript: str
    generate_notion: bool = True
    generate_tasks: bool = True
    num_tasks: int = 5
    additional_context: Optional[str] = None

class TwoStageAnalysisResponse(BaseModel):
    success: bool
    stage1_notion: Optional[Dict[str, Any]] = None
    stage2_prd: Optional[Dict[str, Any]] = None
    stage3_tasks: Optional[MeetingAnalysisResult] = None
    formatted_notion: Optional[str] = None
    formatted_prd: Optional[str] = None
    processing_time: Optional[float] = None
    error: Optional[str] = None

# Triplet ê´€ë ¨ ìƒˆë¡œìš´ ì‘ë‹µ ëª¨ë¸ë“¤
class EnhancedTranscriptionResponse(BaseModel):
    success: bool
    transcription: Optional[Dict[str, Any]] = None
    triplet_data: Optional[Dict[str, Any]] = None
    filtered_transcript: Optional[str] = None
    processing_stats: Optional[Dict[str, Any]] = None
    error: Optional[str] = None

class EnhancedTwoStageResult(BaseModel):
    success: bool
    triplet_stats: Optional[Dict[str, Any]] = None
    classification_stats: Optional[Dict[str, Any]] = None
    stage1_notion: Optional[Dict[str, Any]] = None
    stage2_prd: Optional[Dict[str, Any]] = None
    stage3_tasks: Optional[MeetingAnalysisResult] = None
    formatted_notion: Optional[str] = None
    formatted_prd: Optional[str] = None
    original_transcript_length: Optional[int] = None
    filtered_transcript_length: Optional[int] = None
    noise_reduction_ratio: Optional[float] = None
    processing_time: Optional[float] = None
    error: Optional[str] = None

# ê¸°ì¡´ ëª¨ë¸ë“¤
class TranscriptionResponse(BaseModel):
    success: bool
    transcription: Optional[Dict[str, Any]] = None
    error: Optional[str] = None

class AnalysisRequest(BaseModel):
    transcript: str
    num_tasks: int = 5
    additional_context: Optional[str] = None

class AnalysisResponse(BaseModel):
    success: bool
    analysis: Optional[MeetingAnalysisResult] = None
    error: Optional[str] = None

class HealthResponse(BaseModel):
    status: str
    gpu_available: bool
    gpu_count: int
    models_loaded: Dict[str, bool]
    memory_info: Optional[Dict[str, float]] = None

def load_whisperx():
    """WhisperX ëª¨ë¸ ë¡œë”©"""
    global whisper_model
    
    if whisper_model is None:
        logger.info("ğŸ¤ Loading WhisperX large-v3...")
        try:
            import whisperx
            device = "cuda" if torch.cuda.is_available() else "cpu"
            compute_type = "float16" if device == "cuda" else "int8"
            
            whisper_model = whisperx.load_model(
                "large-v3", 
                device, 
                compute_type=compute_type,
                language="ko"
            )
            logger.info("âœ… WhisperX loaded successfully")
            
        except Exception as e:
            logger.error(f"âŒ WhisperX loading failed: {e}")
            raise e
    
    return whisper_model

def load_qwen3():
    """Qwen3-32B-AWQ ëª¨ë¸ ë¡œë”©"""
    global qwen_model, qwen_tokenizer
    
    if qwen_model is None or qwen_tokenizer is None:
        logger.info("ğŸ§  Loading Qwen3-32B-AWQ...")
        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM
            
            model_name = "Qwen/Qwen3-32B-AWQ"
            
            qwen_tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True
            )
            
            qwen_model = AutoModelForCausalLM.from_pretrained(
                model_name,
                device_map="auto",
                torch_dtype=torch.float16,
                trust_remote_code=True
            )
            
            logger.info("âœ… Qwen3-32B-AWQ loaded successfully")
            
        except Exception as e:
            logger.error(f"âŒ Qwen3-32B-AWQ loading failed: {e}")
            raise e
    
    return qwen_model, qwen_tokenizer

def generate_structured_response(
    system_prompt: str, 
    user_prompt: str, 
    response_schema: Dict[str, Any],
    temperature: float = 0.3,
    max_input_tokens: int = 28000,  # Qwen3-32B AWQ ì•ˆì „ ë§ˆì§„ ì ìš©
    enable_chunking: bool = True
) -> Dict[str, Any]:
    """êµ¬ì¡°í™”ëœ ì‘ë‹µ ìƒì„± (ì²­í‚¹ ì§€ì›)"""
    
    # ì²­í‚¹ í•„ìš” ì—¬ë¶€ í™•ì¸
    if enable_chunking:
        try:
            from chunking_processor import get_chunking_processor
            chunking_processor = get_chunking_processor(max_context_tokens=32768)
            
            # ì „ì²´ í”„ë¡¬í”„íŠ¸ í† í° ìˆ˜ ì¶”ì •
            total_prompt = f"{system_prompt}\n{user_prompt}"
            estimated_tokens = chunking_processor.estimate_tokens(total_prompt)
            
            if estimated_tokens > max_input_tokens:
                logger.info(f"ğŸ”„ ì²­í‚¹ í•„ìš” ê°ì§€ (í† í°: {estimated_tokens} > {max_input_tokens})")
                return generate_chunked_response(
                    system_prompt, user_prompt, response_schema, 
                    temperature, chunking_processor
                )
            else:
                logger.info(f"ğŸ“ ë‹¨ì¼ ì²˜ë¦¬ (í† í°: {estimated_tokens})")
        except ImportError:
            logger.warning("âš ï¸ ì²­í‚¹ í”„ë¡œì„¸ì„œë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì²˜ë¦¬ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
    
    # ëª¨ë¸ ë¡œë”©
    qwen_model, qwen_tokenizer = load_qwen3()
    
    # ìŠ¤í‚¤ë§ˆ ì˜ˆì‹œ í¬í•¨ í”„ë¡¬í”„íŠ¸
    schema_prompt = f"""
{system_prompt}

**Response Schema:**
You must respond with a JSON object following this exact structure:
```json
{json.dumps(response_schema, indent=2, ensure_ascii=False)}
```

**Important Rules:**
1. Always return valid JSON format
2. Use Korean for all text content unless technical terms require English
3. Follow the exact schema structure
4. Include all required fields
5. Use appropriate data types for each field

{user_prompt}

**Response:**
```json
"""
    
    # ë©”ì‹œì§€ êµ¬ì„±
    messages = [{"role": "user", "content": schema_prompt}]
    
    # í† í°í™”
    text = qwen_tokenizer.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True
    )
    
    inputs = qwen_tokenizer([text], return_tensors="pt").to(qwen_model.device)
    
    # ì¶”ë¡  ì‹¤í–‰
    with torch.no_grad():
        outputs = qwen_model.generate(
            **inputs,
            max_new_tokens=2048,
            temperature=temperature,
            do_sample=True,
            pad_token_id=qwen_tokenizer.eos_token_id,
            repetition_penalty=1.1,
            top_p=0.9
        )
    
    # ê²°ê³¼ ë””ì½”ë”©
    response = qwen_tokenizer.decode(
        outputs[0][len(inputs["input_ids"][0]):], 
        skip_special_tokens=True
    )
    
    # JSON ì¶”ì¶œ ë° íŒŒì‹±
    try:
        # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
        if "```json" in response:
            json_start = response.find("```json") + 7
            json_end = response.find("```", json_start)
            if json_end == -1:
                json_content = response[json_start:].strip()
            else:
                json_content = response[json_start:json_end].strip()
        else:
            # JSON ë§ˆì»¤ê°€ ì—†ìœ¼ë©´ ì „ì²´ ì‘ë‹µì—ì„œ JSON ì°¾ê¸°
            json_content = response.strip()
        
        # JSON íŒŒì‹±
        parsed_result = json.loads(json_content)
        return parsed_result
        
    except json.JSONDecodeError as e:
        logger.error(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        logger.error(f"Raw response: {response[:500]}...")
        return {
            "error": f"JSON parsing failed: {str(e)}",
            "raw_response": response[:1000]
        }
    except Exception as e:
        logger.error(f"âŒ ì‘ë‹µ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return {
            "error": f"Response processing failed: {str(e)}",
            "raw_response": response[:1000] if 'response' in locals() else "No response"
        }

def generate_chunked_response(
    system_prompt: str,
    user_prompt: str, 
    response_schema: Dict[str, Any],
    temperature: float,
    chunking_processor
) -> Dict[str, Any]:
    """ì²­í‚¹ëœ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬"""
    try:
        logger.info("ğŸš€ ì²­í‚¹ ê¸°ë°˜ ì²˜ë¦¬ ì‹œì‘...")
        start_time = time.time()
        
        # 1. user_promptë¥¼ ì²­í‚¹
        chunks = chunking_processor.create_chunks_with_overlap(user_prompt)
        logger.info(f"ğŸ“Š ì´ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
        
        # 2. ê° ì²­í¬ë³„ë¡œ ì²˜ë¦¬
        chunk_results = []
        for i, chunk in enumerate(chunks):
            logger.info(f"ğŸ”„ ì²­í¬ {i+1}/{len(chunks)} ì²˜ë¦¬ ì¤‘... (í† í°: {chunk['estimated_tokens']})")
            
            # ì²­í¬ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìˆ˜ì •
            chunk_system_prompt = f"""{system_prompt}

**ì²­í‚¹ ì²˜ë¦¬ ì •ë³´:**
- í˜„ì¬ ì²­í¬: {i+1}/{len(chunks)}
- ì´ ì²­í¬ëŠ” ì „ì²´ íšŒì˜ì˜ ì¼ë¶€ì…ë‹ˆë‹¤
- ì´ ì²­í¬ì—ì„œ ë°œê²¬ë˜ëŠ” ë‚´ìš©ë§Œ ë¶„ì„í•˜ì„¸ìš”
- ë‹¤ë¥¸ ì²­í¬ì˜ ë‚´ìš©ì€ ë‚˜ì¤‘ì— í†µí•©ë©ë‹ˆë‹¤"""

            chunk_user_prompt = f"""ë‹¤ìŒì€ ì „ì²´ íšŒì˜ë¡ì˜ ì¼ë¶€ì…ë‹ˆë‹¤:

{chunk['text']}

ìœ„ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì´ ë¶€ë¶„ì—ì„œ ë°œê²¬ë˜ëŠ” ì•¡ì…˜ ì•„ì´í…œ, ê²°ì •ì‚¬í•­, í•µì‹¬ í¬ì¸íŠ¸ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”."""
            
            # ë‹¨ì¼ ì²­í¬ ì²˜ë¦¬
            chunk_result = generate_structured_response(
                system_prompt=chunk_system_prompt,
                user_prompt=chunk_user_prompt,
                response_schema=response_schema,
                temperature=temperature,
                enable_chunking=False  # ì¬ê·€ ë°©ì§€
            )
            
            chunk_results.append(chunk_result)
            logger.info(f"âœ… ì²­í¬ {i+1} ì²˜ë¦¬ ì™„ë£Œ")
        
        # 3. ê²°ê³¼ í†µí•©
        logger.info("ğŸ”„ ì²­í¬ ê²°ê³¼ í†µí•© ì¤‘...")
        merged_result = chunking_processor.merge_chunk_results(chunk_results)
        
        processing_time = time.time() - start_time
        logger.info(f"âœ… ì²­í‚¹ ì²˜ë¦¬ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {processing_time:.2f}ì´ˆ)")
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        merged_result["metadata"] = merged_result.get("metadata", {})
        merged_result["metadata"].update({
            "chunking_applied": True,
            "total_chunks": len(chunks),
            "processing_time": processing_time,
            "original_tokens": chunking_processor.estimate_tokens(user_prompt),
            "chunks_info": [
                {
                    "chunk_id": chunk["chunk_id"],
                    "tokens": chunk["estimated_tokens"],
                    "has_overlap": chunk["has_overlap"]
                }
                for chunk in chunks
            ]
        })
        
        return merged_result
        
    except Exception as e:
        logger.error(f"âŒ ì²­í‚¹ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return {
            "error": f"Chunking processing failed: {str(e)}",
            "fallback_message": "ì²­í‚¹ ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ ì²˜ë¦¬ë¥¼ ì‹œë„í•˜ì„¸ìš”."
        }

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì•± ì‹œì‘/ì¢…ë£Œ ì‹œ ëª¨ë¸ ë¡œë”©/ì •ë¦¬"""
    logger.info("ğŸš€ Starting TtalKkac Final AI Server with Triplets...")
    
    # ëª¨ë¸ë“¤ì„ ë¯¸ë¦¬ ë¡œë”© (ì„ íƒì‚¬í•­)
    try:
        if os.getenv("PRELOAD_MODELS", "false").lower() == "true":
            logger.info("ğŸ“¦ Preloading models...")
            load_whisperx()
            load_qwen3()
            if TRIPLET_AVAILABLE:
                get_bert_classifier()
            logger.info("âœ… Models preloaded successfully")
    except Exception as e:
        logger.warning(f"âš ï¸ Model preloading failed: {e}")
    
    yield
    
    logger.info("ğŸ›‘ Shutting down TtalKkac Final AI Server...")

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="TtalKkac Final AI Server with Triplets",
    description="WhisperX + Triplet + BERT + Qwen3-32B + 2-Stage PRD Process",
    version="3.1.0",
    lifespan=lifespan
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "TtalKkac Final AI Server with Triplets",
        "version": "3.1.0",
        "features": [
            "WhisperX Speech-to-Text",
            "Triplet Context Analysis", 
            "BERT Noise Filtering",
            "2-Stage PRD Process",
            "Notion Project Generation", 
            "Task Master PRD Format",
            "Advanced Task Generation"
        ],
        "workflow": "íšŒì˜ë¡ â†’ Triplet í•„í„°ë§ â†’ ê¸°íšì•ˆ â†’ Task Master PRD â†’ ì—…ë¬´ìƒì„±",
        "docs": "/docs"
    }

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    gpu_available = torch.cuda.is_available()
    gpu_count = torch.cuda.device_count() if gpu_available else 0
    
    memory_info = None
    if gpu_available:
        try:
            memory_info = {
                "allocated_gb": torch.cuda.memory_allocated() / 1024**3,
                "reserved_gb": torch.cuda.memory_reserved() / 1024**3,
                "total_gb": torch.cuda.get_device_properties(0).total_memory / 1024**3
            }
        except:
            pass
    
    return HealthResponse(
        status="healthy",
        gpu_available=gpu_available,
        gpu_count=gpu_count,
        models_loaded={
            "whisperx": whisper_model is not None,
            "qwen3": qwen_model is not None,
            "triplet_bert": TRIPLET_AVAILABLE
        },
        memory_info=memory_info
    )

@app.post("/transcribe", response_model=TranscriptionResponse)
async def transcribe_audio(audio: UploadFile = File(...)):
    """ìŒì„± íŒŒì¼ ì „ì‚¬ (WhisperX)"""
    try:
        logger.info(f"ğŸ¤ Transcribing audio: {audio.filename}")
        
        # ëª¨ë¸ ë¡œë”©
        whisper_model = load_whisperx()
        
        # ì˜¤ë””ì˜¤ íŒŒì¼ ì„ì‹œ ì €ì¥
        audio_content = await audio.read()
        
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_file:
            temp_file.write(audio_content)
            temp_path = temp_file.name
        
        try:
            # WhisperX ì „ì‚¬ ì‹¤í–‰
            result = whisper_model.transcribe(temp_path, batch_size=16)
            
            segments = result.get("segments", [])
            full_text = " ".join([seg.get("text", "") for seg in segments])
            
            logger.info(f"âœ… Transcription completed: {len(full_text)} characters")
            
            return TranscriptionResponse(
                success=True,
                transcription={
                    "segments": segments,
                    "full_text": full_text,
                    "language": result.get("language", "ko"),
                    "duration": sum([seg.get("end", 0) - seg.get("start", 0) for seg in segments])
                }
            )
            
        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            try:
                os.unlink(temp_path)
            except:
                pass
                
    except Exception as e:
        logger.error(f"âŒ Transcription error: {e}")
        return TranscriptionResponse(
            success=False,
            error=str(e)
        )

@app.post("/transcribe-enhanced", response_model=EnhancedTranscriptionResponse)
async def transcribe_audio_enhanced(
    audio: UploadFile = File(...),
    enable_bert_filtering: bool = True,
    save_noise_log: bool = True
):
    """í–¥ìƒëœ ìŒì„± íŒŒì¼ ì „ì‚¬ (Triplet + BERT í•„í„°ë§)"""
    try:
        logger.info(f"ğŸ¤ Enhanced transcribing: {audio.filename}")
        
        # 1. ê¸°ë³¸ WhisperX ì „ì‚¬
        basic_result = await transcribe_audio(audio)
        if not basic_result.success:
            return EnhancedTranscriptionResponse(
                success=False,
                error=basic_result.error
            )
        
        # 2. Triplet í”„ë¡œì„¸ì„œë¡œ ì²˜ë¦¬ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if TRIPLET_AVAILABLE and enable_bert_filtering:
            try:
                triplet_processor = get_triplet_processor()
                
                enhanced_result = triplet_processor.process_whisperx_result(
                    whisperx_result=basic_result.transcription,
                    enable_bert_filtering=enable_bert_filtering,
                    save_noise_log=save_noise_log
                )
                
                if enhanced_result["success"]:
                    logger.info("âœ… Enhanced transcription with Triplets completed")
                    
                    return EnhancedTranscriptionResponse(
                        success=True,
                        transcription=basic_result.transcription,
                        triplet_data=enhanced_result.get("triplet_data"),
                        filtered_transcript=enhanced_result.get("filtered_transcript"),
                        processing_stats=enhanced_result.get("processing_stats")
                    )
                else:
                    logger.warning("âš ï¸ Triplet processing failed, using basic transcription")
            except Exception as e:
                logger.warning(f"âš ï¸ Triplet processing error: {e}")
        
        # 3. Triplet ì‚¬ìš© ë¶ˆê°€ëŠ¥ ì‹œ ê¸°ë³¸ ê²°ê³¼ ë°˜í™˜
        return EnhancedTranscriptionResponse(
            success=True,
            transcription=basic_result.transcription,
            filtered_transcript=basic_result.transcription["full_text"],
            processing_stats={"triplet_available": TRIPLET_AVAILABLE}
        )
        
    except Exception as e:
        logger.error(f"âŒ Enhanced transcription error: {e}")
        return EnhancedTranscriptionResponse(
            success=False,
            error=str(e)
        )

@app.post("/generate-notion-project", response_model=NotionProjectResponse)
async def generate_notion_project(request: AnalysisRequest):
    """1ë‹¨ê³„: íšŒì˜ë¡ â†’ ë…¸ì…˜ ê¸°íšì•ˆ ìƒì„±"""
    try:
        logger.info("ğŸ“ Stage 1: Generating Notion project document...")
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        system_prompt = "ë‹¹ì‹ ì€ íšŒì˜ë¡ì„ ë¶„ì„í•˜ì—¬ ì²´ê³„ì ì¸ í”„ë¡œì íŠ¸ ê¸°íšì•ˆì„ ì‘ì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
        user_prompt = generate_notion_project_prompt(request.transcript)
        
        # êµ¬ì¡°í™”ëœ ì‘ë‹µ ìƒì„±
        result = generate_structured_response(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            response_schema=NOTION_PROJECT_SCHEMA,
            temperature=0.3
        )
        
        if "error" in result:
            return NotionProjectResponse(
                success=False,
                error=result["error"]
            )
        
        # ë°ì´í„° ê²€ì¦
        validated_result = validate_notion_project(result)
        
        # ë…¸ì…˜ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…
        formatted_notion = format_notion_project(validated_result)
        
        logger.info("âœ… Stage 1 completed: Notion project generated")
        
        return NotionProjectResponse(
            success=True,
            notion_project=validated_result,
            formatted_notion=formatted_notion
        )
        
    except Exception as e:
        logger.error(f"âŒ Notion project generation error: {e}")
        return NotionProjectResponse(
            success=False,
            error=str(e)
        )

@app.post("/generate-task-master-prd", response_model=TaskMasterPRDResponse)
async def generate_task_master_prd(notion_project: Dict[str, Any]):
    """2ë‹¨ê³„: ë…¸ì…˜ ê¸°íšì•ˆ â†’ Task Master PRD ë³€í™˜"""
    try:
        logger.info("ğŸ”„ Stage 2: Converting to Task Master PRD format...")
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        system_prompt = "ë‹¹ì‹ ì€ ê¸°íšì•ˆì„ Task Master PRD í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
        user_prompt = generate_task_master_prd_prompt(notion_project)
        
        # êµ¬ì¡°í™”ëœ ì‘ë‹µ ìƒì„±
        result = generate_structured_response(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            response_schema=TASK_MASTER_PRD_SCHEMA,
            temperature=0.3
        )
        
        if "error" in result:
            return TaskMasterPRDResponse(
                success=False,
                error=result["error"]
            )
        
        # ë°ì´í„° ê²€ì¦
        validated_result = validate_task_master_prd(result)
        
        # Task Master PRD í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…
        formatted_prd = format_task_master_prd(validated_result)
        
        logger.info("âœ… Stage 2 completed: Task Master PRD generated")
        
        return TaskMasterPRDResponse(
            success=True,
            prd_data=validated_result,
            formatted_prd=formatted_prd
        )
        
    except Exception as e:
        logger.error(f"âŒ Task Master PRD generation error: {e}")
        return TaskMasterPRDResponse(
            success=False,
            error=str(e)
        )

@app.post("/two-stage-analysis", response_model=TwoStageAnalysisResponse)
async def two_stage_analysis(request: TwoStageAnalysisRequest):
    """2ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤ í†µí•© ë¶„ì„"""
    try:
        logger.info("ğŸš€ Starting 2-stage analysis process...")
        
        start_time = time.time()
        
        # 1ë‹¨ê³„: ë…¸ì…˜ ê¸°íšì•ˆ ìƒì„±
        stage1_result = None
        if request.generate_notion:
            logger.info("ğŸ“ Stage 1: Generating Notion project...")
            notion_request = AnalysisRequest(
                transcript=request.transcript,
                additional_context=request.additional_context
            )
            stage1_response = await generate_notion_project(notion_request)
            
            if not stage1_response.success:
                return TwoStageAnalysisResponse(
                    success=False,
                    error=f"Stage 1 failed: {stage1_response.error}"
                )
            
            stage1_result = stage1_response.notion_project
        
        # 2ë‹¨ê³„: Task Master PRD ë³€í™˜
        stage2_result = None
        if request.generate_tasks and stage1_result:
            logger.info("ğŸ”„ Stage 2: Converting to Task Master PRD...")
            stage2_response = await generate_task_master_prd(stage1_result)
            
            if not stage2_response.success:
                return TwoStageAnalysisResponse(
                    success=False,
                    error=f"Stage 2 failed: {stage2_response.error}"
                )
            
            stage2_result = stage2_response.prd_data
        
        # 3ë‹¨ê³„: íƒœìŠ¤í¬ ìƒì„± (Task Master ë°©ì‹)
        stage3_result = None
        if request.generate_tasks and stage2_result:
            logger.info("ğŸ¯ Stage 3: Generating tasks using Task Master approach...")
            
            # Task Master PRDë¥¼ ì‚¬ìš©í•œ íƒœìŠ¤í¬ ìƒì„±
            prd_content = format_task_master_prd(stage2_result)
            system_prompt = generate_meeting_analysis_system_prompt(request.num_tasks)
            user_prompt = f"""
            ë‹¤ìŒ PRD ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ {request.num_tasks}ê°œì˜ êµ¬ì²´ì ì¸ ê°œë°œ íƒœìŠ¤í¬ë¥¼ ìƒì„±í•˜ì„¸ìš”:
            
            {prd_content}
            
            Task Masterì˜ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì„ ì ìš©í•˜ì—¬ ì²´ê³„ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ íƒœìŠ¤í¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.
            """
            
            result = generate_structured_response(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                response_schema=TASK_SCHEMA_EXAMPLE,
                temperature=0.3
            )
            
            # ê²°ê³¼ í›„ì²˜ë¦¬
            validated_result = validate_meeting_analysis(result)
            
            # TaskItem ê°ì²´ë¡œ ë³€í™˜
            task_items = []
            for i, item in enumerate(validated_result.get("action_items", [])):
                task_item = TaskItem(
                    id=i + 1,
                    title=item.get("task", f"Task {i+1}"),
                    description=item.get("task", ""),
                    priority=item.get("priority", "medium"),
                    assignee=item.get("assignee", "ë¯¸ì§€ì •"),
                    deadline=item.get("deadline", "ë¯¸ì •"),
                    complexity=calculate_task_complexity_advanced(
                        TaskItem(
                            id=i + 1,
                            title=item.get("task", ""),
                            description=item.get("task", ""),
                            priority=item.get("priority", "medium")
                        )
                    ),
                    status="pending"
                )
                task_items.append(task_item)
            
            # ì˜ì¡´ì„± ê²€ì¦
            task_items = validate_task_dependencies(task_items)
            
            # ìµœì¢… ê²°ê³¼ êµ¬ì„±
            stage3_result = MeetingAnalysisResult(
                summary=validated_result.get("summary", ""),
                action_items=task_items,
                decisions=validated_result.get("decisions", []),
                next_steps=validated_result.get("next_steps", []),
                key_points=validated_result.get("key_points", []),
                participants=validated_result.get("participants", []),
                follow_up=validated_result.get("follow_up", {}),
                metadata={
                    "processing_time": time.time() - start_time,
                    "total_tasks": len(task_items),
                    "process_type": "2-stage-task-master"
                }
            )
        
        total_time = time.time() - start_time
        
        return TwoStageAnalysisResponse(
            success=True,
            stage1_notion=stage1_result,
            stage2_prd=stage2_result,
            stage3_tasks=stage3_result,
            formatted_notion=format_notion_project(stage1_result) if stage1_result else None,
            formatted_prd=format_task_master_prd(stage2_result) if stage2_result else None,
            processing_time=total_time
        )
        
    except Exception as e:
        logger.error(f"âŒ 2-stage analysis error: {e}")
        return TwoStageAnalysisResponse(
            success=False,
            error=str(e)
        )

@app.post("/two-stage-pipeline", response_model=EnhancedTwoStageResult)
async def enhanced_two_stage_pipeline(
    audio: UploadFile = File(...),
    enable_bert_filtering: bool = True,
    save_noise_log: bool = True,
    generate_notion: bool = True,
    generate_tasks: bool = True,
    num_tasks: int = 5
):
    """ìµœì¢… ì „ì²´ íŒŒì´í”„ë¼ì¸: ìŒì„± â†’ Triplet í•„í„°ë§ â†’ 2ë‹¨ê³„ ë¶„ì„"""
    try:
        logger.info("ğŸš€ Starting enhanced 2-stage pipeline with Triplets...")
        
        start_time = time.time()
        
        # 1ë‹¨ê³„: í–¥ìƒëœ ì „ì‚¬ (Triplet + BERT)
        logger.info("ğŸ“ Step 1: Enhanced transcription...")
        enhanced_transcribe_result = await transcribe_audio_enhanced(
            audio=audio,
            enable_bert_filtering=enable_bert_filtering,
            save_noise_log=save_noise_log
        )
        
        if not enhanced_transcribe_result.success:
            return EnhancedTwoStageResult(
                success=False,
                error=enhanced_transcribe_result.error
            )
        
        # 2ë‹¨ê³„: í•„í„°ë§ëœ í…ìŠ¤íŠ¸ë¡œ 2ë‹¨ê³„ ë¶„ì„
        logger.info("ğŸ§  Step 2: Running 2-stage analysis on filtered content...")
        filtered_text = enhanced_transcribe_result.filtered_transcript or \
                       enhanced_transcribe_result.transcription["full_text"]
        
        # ğŸ”¥ ì²­í‚¹ í•„ìš”ì„± ì²´í¬ ë° ì²˜ë¦¬
        try:
            from chunking_processor import get_chunking_processor
            chunking_processor = get_chunking_processor(max_context_tokens=32768)
            
            # í•„í„°ë§ëœ í…ìŠ¤íŠ¸ í† í° ìˆ˜ í™•ì¸
            estimated_tokens = chunking_processor.estimate_tokens(filtered_text)
            logger.info(f"ğŸ“Š í•„í„°ë§ëœ í…ìŠ¤íŠ¸ í† í° ìˆ˜: {estimated_tokens}")
            
            # ë…¸ì´ì¦ˆ ì œê±° íš¨ê³¼ ë¡œê·¸
            original_tokens = chunking_processor.estimate_tokens(
                enhanced_transcribe_result.transcription["full_text"]
            )
            token_reduction = ((original_tokens - estimated_tokens) / original_tokens * 100) if original_tokens > 0 else 0
            logger.info(f"ğŸ¯ í† í° ê°ì†Œ íš¨ê³¼: {original_tokens} â†’ {estimated_tokens} ({token_reduction:.1f}% ê°ì†Œ)")
            
        except ImportError:
            logger.warning("âš ï¸ ì²­í‚¹ í”„ë¡œì„¸ì„œë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            estimated_tokens = len(filtered_text) * 1.5  # ëŒ€ëµì  ì¶”ì •
        
        analysis_request = TwoStageAnalysisRequest(
            transcript=filtered_text,
            generate_notion=generate_notion,
            generate_tasks=generate_tasks,
            num_tasks=num_tasks
        )
        analysis_result = await two_stage_analysis(analysis_request)
        
        total_time = time.time() - start_time
        
        # ê²°ê³¼ í†µê³„ ê³„ì‚°
        original_length = len(enhanced_transcribe_result.transcription["full_text"])
        filtered_length = len(filtered_text)
        noise_reduction = 1.0 - (filtered_length / original_length) if original_length > 0 else 0.0
        
        return EnhancedTwoStageResult(
            success=True,
            triplet_stats=enhanced_transcribe_result.triplet_data,
            classification_stats=enhanced_transcribe_result.processing_stats,
            stage1_notion=analysis_result.stage1_notion if analysis_result.success else None,
            stage2_prd=analysis_result.stage2_prd if analysis_result.success else None,
            stage3_tasks=analysis_result.stage3_tasks if analysis_result.success else None,
            formatted_notion=analysis_result.formatted_notion if analysis_result.success else None,
            formatted_prd=analysis_result.formatted_prd if analysis_result.success else None,
            original_transcript_length=original_length,
            filtered_transcript_length=filtered_length,
            noise_reduction_ratio=noise_reduction,
            processing_time=total_time
        )
        
    except Exception as e:
        logger.error(f"âŒ Enhanced 2-stage pipeline error: {e}")
        return EnhancedTwoStageResult(
            success=False,
            error=str(e)
        )

@app.post("/pipeline-final", response_model=Dict[str, Any])
async def final_pipeline(
    audio: UploadFile = File(...),
    generate_notion: bool = True,
    generate_tasks: bool = True,
    num_tasks: int = 5
):
    """ìµœì¢… ì „ì²´ íŒŒì´í”„ë¼ì¸: ìŒì„± â†’ ì „ì‚¬ â†’ 2ë‹¨ê³„ ë¶„ì„ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)"""
    try:
        logger.info("ğŸš€ Starting final pipeline with 2-stage process...")
        
        start_time = time.time()
        
        # 1ë‹¨ê³„: ì „ì‚¬
        logger.info("ğŸ“ Step 1: Transcribing audio...")
        transcribe_result = await transcribe_audio(audio)
        if not transcribe_result.success:
            return {
                "success": False,
                "step": "transcription",
                "error": transcribe_result.error
            }
        
        # 2ë‹¨ê³„: 2ë‹¨ê³„ ë¶„ì„
        logger.info("ğŸ§  Step 2: Running 2-stage analysis...")
        analysis_request = TwoStageAnalysisRequest(
            transcript=transcribe_result.transcription["full_text"],
            generate_notion=generate_notion,
            generate_tasks=generate_tasks,
            num_tasks=num_tasks
        )
        analysis_result = await two_stage_analysis(analysis_request)
        
        total_time = time.time() - start_time
        
        return {
            "success": True,
            "step": "completed",
            "transcription": transcribe_result.transcription,
            "analysis": {
                "notion_project": analysis_result.stage1_notion,
                "task_master_prd": analysis_result.stage2_prd,
                "generated_tasks": analysis_result.stage3_tasks,
                "formatted_notion": analysis_result.formatted_notion,
                "formatted_prd": analysis_result.formatted_prd
            },
            "processing_time": total_time,
            "model_info": {
                "whisperx": "large-v3",
                "qwen3": "Qwen3-32B-AWQ",
                "process": "2-stage-task-master",
                "triplet_available": TRIPLET_AVAILABLE
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ Final pipeline error: {e}")
        return {
            "success": False,
            "step": "pipeline",
            "error": str(e)
        }

if __name__ == "__main__":
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    host = os.getenv("HOST", "0.0.0.0")
    port = int(os.getenv("PORT", "8000"))
    workers = int(os.getenv("WORKERS", "1"))
    
    logger.info(f"ğŸš€ Starting final server with Triplets on {host}:{port}")
    
    uvicorn.run(
        "ai_server_final_with_triplets:app",
        host=host,
        port=port,
        workers=workers,
        reload=False,
        log_level="info"
    )