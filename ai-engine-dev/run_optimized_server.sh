#!/bin/bash

# TtalKkak AI ì„œë²„ VLLM + ìµœì í™” ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

echo "ğŸš€ TtalKkak AI ì„œë²„ VLLM + ìµœì í™” ë²„ì „ ì‹œì‘..."

# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
export PRELOAD_MODELS=true
export USE_VLLM=true  # VLLM í™œì„±í™”
export HOST=0.0.0.0
export PORT=8000
export WORKERS=1

# GPU ë©”ëª¨ë¦¬ ì„¤ì •
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512

# VLLM ìµœì í™” ì„¤ì •
export VLLM_ATTENTION_BACKEND=FLASH_ATTN  # Flash Attention ì‚¬ìš©
export VLLM_USE_MODELSCOPE=false

echo "ğŸ”§ ì„¤ì •ëœ í™˜ê²½ë³€ìˆ˜:"
echo "   - PRELOAD_MODELS=$PRELOAD_MODELS"
echo "   - USE_VLLM=$USE_VLLM"
echo "   - HOST=$HOST"
echo "   - PORT=$PORT"
echo "   - WORKERS=$WORKERS"
echo "   - PYTORCH_CUDA_ALLOC_CONF=$PYTORCH_CUDA_ALLOC_CONF"
echo "   - VLLM_ATTENTION_BACKEND=$VLLM_ATTENTION_BACKEND"

echo ""
echo "ğŸš€ VLLM + ìµœì í™” ê¸°ëŠ¥:"
echo "   âš¡ VLLM ì´ˆê³ ì† ì¶”ë¡  (3-10ë°° ë¹ ë¦„)"
echo "   âœ… ëª¨ë¸ ì‚¬ì „ ë¡œë”© (ë³‘ë ¬ ë¡œë”©)"
echo "   âœ… BERT ì§„ì§œ ë°°ì¹˜ ì²˜ë¦¬ (32ê°œì”©)"
echo "   âœ… GPU ë©”ëª¨ë¦¬ ìµœì í™” (Mixed Precision)"
echo "   âœ… ìë™ ë©”ëª¨ë¦¬ ì •ë¦¬"
echo "   âœ… PagedAttention (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)"
echo "   âœ… ë™ì  ë°°ì¹˜ ì²˜ë¦¬"
echo "   âœ… Flash Attention 2.0"

echo ""
echo "ğŸ“¦ VLLM ì„¤ì¹˜ í™•ì¸ ì¤‘..."
if python -c "import vllm; print(f'VLLM version: {vllm.__version__}')" 2>/dev/null; then
    echo "âœ… VLLM ì„¤ì¹˜ í™•ì¸ë¨"
else
    echo "âŒ VLLM ë¯¸ì„¤ì¹˜ - ì„¤ì¹˜ ì¤‘..."
    pip install vllm>=0.3.0
    if [ $? -eq 0 ]; then
        echo "âœ… VLLM ì„¤ì¹˜ ì™„ë£Œ"
    else
        echo "âš ï¸ VLLM ì„¤ì¹˜ ì‹¤íŒ¨ - Transformers ëª¨ë“œë¡œ ì‹¤í–‰"
        export USE_VLLM=false
    fi
fi

echo ""
echo "ğŸ‰ ì„œë²„ ì‹œì‘ ì¤‘..."

# Python ì„œë²„ ì‹¤í–‰
python ai_server_final_with_triplets.py