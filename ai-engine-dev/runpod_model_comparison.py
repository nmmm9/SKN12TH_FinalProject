"""
ëŸ°íŒŸì—ì„œ ë‹¨ì¼ ëª¨ë¸ë¡œ ì‹¤ì œ íšŒì˜ë¡ ìš”ì•½ í…ŒìŠ¤íŠ¸
ì‹¤ì œ TtalKkac í”„ë¡¬í”„íŠ¸ì™€ ë¡œì§ ì‚¬ìš©
"""

import os
import json
import time
import torch
from typing import Dict, Any, List
from datetime import datetime
import logging
from pathlib import Path
import random

# ì‹¤ì œ í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸
from meeting_analysis_prompts import (
    generate_meeting_analysis_system_prompt,
    generate_meeting_analysis_user_prompt,
    MEETING_ANALYSIS_SCHEMA
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SingleModelTester:
    """ë‹¨ì¼ ëª¨ë¸ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.current_model = None
        self.current_tokenizer = None
        
        # GPU ë©”ëª¨ë¦¬ ì²´í¬
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            logger.info(f"ğŸ® GPU ë©”ëª¨ë¦¬: {gpu_memory:.1f}GB")
        
        # ì‹¤ì œ í”„ë¡œì íŠ¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        self.system_prompt = generate_meeting_analysis_system_prompt()
        
    def load_real_meeting_data(self, data_dir: str = "batch_triplet_results", num_samples: int = 5) -> List[Dict[str, Any]]:
        """ì‹¤ì œ íšŒì˜ë¡ ë°ì´í„° ë¡œë“œ"""
        logger.info(f"ğŸ“ Loading real meeting data from {data_dir}...")
        
        data_path = Path(data_dir)
        if not data_path.exists():
            logger.error(f"âŒ Data directory not found: {data_dir}")
            return []
        
        meeting_data = []
        
        # result_ í´ë”ë“¤ì—ì„œ 05_final_result.json íŒŒì¼ ì°¾ê¸°
        result_folders = [f for f in data_path.iterdir() if f.is_dir() and f.name.startswith("result_")]
        
        if not result_folders:
            logger.error(f"âŒ No result folders found in {data_dir}")
            return []
        
        # ëœë¤í•˜ê²Œ ìƒ˜í”Œ ì„ íƒ
        selected_folders = random.sample(result_folders, min(num_samples, len(result_folders)))
        
        for folder in selected_folders:
            final_result_file = folder / "05_final_result.json"
            
            if final_result_file.exists():
                try:
                    with open(final_result_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    # íšŒì˜ë¡ í…ìŠ¤íŠ¸ êµ¬ì„±
                    meeting_text = ""
                    for item in data:
                        timestamp = item.get('timestamp', 'Unknown')
                        speaker = item.get('speaker', 'Unknown')
                        text = item.get('text', '')
                        meeting_text += f"[{timestamp}] {speaker}: {text}\\n"
                    
                    meeting_data.append({
                        "folder_name": folder.name,
                        "meeting_text": meeting_text.strip(),
                        "total_segments": len(data),
                        "total_length": len(meeting_text)
                    })
                    
                    logger.info(f"âœ… Loaded: {folder.name} ({len(data)} segments, {len(meeting_text)} chars)")
                    
                except Exception as e:
                    logger.error(f"âŒ Failed to load {folder.name}: {e}")
                    continue
            else:
                logger.warning(f"âš ï¸ {folder.name}: 05_final_result.json not found")
        
        logger.info(f"ğŸ“Š Loaded {len(meeting_data)} real meeting samples")
        return meeting_data
        
    def load_model(self, use_vllm: bool = True):
        """ëª¨ë¸ ë¡œë”© (VLLM ìš°ì„ , Transformers ë°±ì—…)"""
        try:
            logger.info(f"ğŸ”„ Loading {self.model_name}...")
            start_time = time.time()
            
            if use_vllm:
                try:
                    from vllm import LLM
                    from transformers import AutoTokenizer
                    
                    # VLLM ëª¨ë¸ ë¡œë”©
                    self.current_model = LLM(
                        model=self.model_name,
                        tensor_parallel_size=1,
                        gpu_memory_utilization=0.8,
                        trust_remote_code=True,
                        max_model_len=8192,  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì ë‹¹í•œ ê¸¸ì´
                        enforce_eager=True,
                        swap_space=2
                    )
                    
                    self.current_tokenizer = AutoTokenizer.from_pretrained(
                        self.model_name, trust_remote_code=True
                    )
                    
                    load_time = time.time() - start_time
                    logger.info(f"âœ… VLLM {self.model_name} loaded in {load_time:.2f}s")
                    return True, "vllm"
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ VLLM failed: {e}")
                    use_vllm = False
            
            if not use_vllm:
                # Transformers ë°±ì—…
                from transformers import AutoTokenizer, AutoModelForCausalLM
                
                self.current_tokenizer = AutoTokenizer.from_pretrained(
                    self.model_name, trust_remote_code=True
                )
                
                if self.current_tokenizer.pad_token is None:
                    self.current_tokenizer.pad_token = self.current_tokenizer.eos_token
                
                self.current_model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    device_map="auto",
                    torch_dtype=torch.float16,
                    trust_remote_code=True
                )
                
                load_time = time.time() - start_time
                logger.info(f"âœ… Transformers {self.model_name} loaded in {load_time:.2f}s")
                return True, "transformers"
                
        except Exception as e:
            logger.error(f"âŒ Failed to load {self.model_name}: {e}")
            return False, str(e)
    
    def generate_with_model(
        self, 
        meeting_text: str, 
        backend: str,
        temperature: float = 0.3
    ) -> Dict[str, Any]:
        """ì‹¤ì œ TtalKkac ë¡œì§ìœ¼ë¡œ ì‘ë‹µ ìƒì„±"""
        try:
            start_time = time.time()
            
            # ì‹¤ì œ í”„ë¡œì íŠ¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
            user_prompt = generate_meeting_analysis_user_prompt(meeting_text)
            
            # ìŠ¤í‚¤ë§ˆ í¬í•¨ í”„ë¡¬í”„íŠ¸ (ì‹¤ì œ generate_structured_response ë¡œì§)
            schema_prompt = f"""
{self.system_prompt}

**Response Schema:**
You must respond with a JSON object following this exact structure:
```json
{json.dumps(MEETING_ANALYSIS_SCHEMA, indent=2, ensure_ascii=False)}
```

**Important Rules:**
1. Always return valid JSON format
2. Use Korean for all text content unless technical terms require English
3. Follow the exact schema structure
4. Include all required fields
5. Use appropriate data types for each field

{user_prompt}

**Response:**
```json
"""
            
            messages = [{"role": "user", "content": schema_prompt}]
            
            if backend == "vllm":
                # VLLM ì¶”ë¡ 
                from vllm import SamplingParams
                
                sampling_params = SamplingParams(
                    max_tokens=2048,
                    temperature=temperature,
                    top_p=0.9,
                    repetition_penalty=1.1
                )
                
                text = self.current_tokenizer.apply_chat_template(
                    messages, tokenize=False, add_generation_prompt=True
                )
                
                outputs = self.current_model.generate([text], sampling_params)
                response = outputs[0].outputs[0].text
                
            else:
                # Transformers ì¶”ë¡ 
                text = self.current_tokenizer.apply_chat_template(
                    messages, tokenize=False, add_generation_prompt=True
                )
                
                inputs = self.current_tokenizer([text], return_tensors="pt").to(self.current_model.device)
                
                with torch.no_grad():
                    outputs = self.current_model.generate(
                        **inputs,
                        max_new_tokens=2048,
                        temperature=temperature,
                        do_sample=True,
                        pad_token_id=self.current_tokenizer.eos_token_id,
                        repetition_penalty=1.1,
                        top_p=0.9
                    )
                
                response = self.current_tokenizer.decode(
                    outputs[0][len(inputs["input_ids"][0]):], 
                    skip_special_tokens=True
                )
            
            inference_time = time.time() - start_time
            
            # JSON ì¶”ì¶œ ë° íŒŒì‹± (ì‹¤ì œ ë¡œì§)
            try:
                if "```json" in response:
                    json_start = response.find("```json") + 7
                    json_end = response.find("```", json_start)
                    if json_end == -1:
                        json_content = response[json_start:].strip()
                    else:
                        json_content = response[json_start:json_end].strip()
                else:
                    json_content = response.strip()
                
                parsed_result = json.loads(json_content)
                
                return {
                    "success": True,
                    "result": parsed_result,
                    "inference_time": inference_time,
                    "raw_response": response,
                    "backend": backend
                }
                
            except json.JSONDecodeError as e:
                return {
                    "success": False,
                    "error": f"JSON parsing failed: {str(e)}",
                    "inference_time": inference_time,
                    "raw_response": response,
                    "backend": backend
                }
                
        except Exception as e:
            return {
                "success": False,
                "error": f"Generation failed: {str(e)}",
                "inference_time": 0,
                "backend": backend
            }
    
    def test_single_meeting(self, meeting_data: Dict[str, Any], backend: str) -> Dict[str, Any]:
        """ë‹¨ì¼ íšŒì˜ë¡ í…ŒìŠ¤íŠ¸"""
        logger.info(f"ğŸ“ Testing: {meeting_data['folder_name']}")
        logger.info(f"   ğŸ“Š {meeting_data['total_segments']} segments, {meeting_data['total_length']} chars")
        
        # ìƒì„± í…ŒìŠ¤íŠ¸
        result = self.generate_with_model(meeting_data['meeting_text'], backend)
        
        return {
            "meeting_info": {
                "folder_name": meeting_data['folder_name'],
                "total_segments": meeting_data['total_segments'],
                "total_length": meeting_data['total_length']
            },
            "generation_result": result
        }
    
    def run_test(self, data_dir: str = "batch_triplet_results", num_samples: int = 3):
        """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info(f"ğŸš€ Starting test with {self.model_name}...")
        
        # ì‹¤ì œ íšŒì˜ë¡ ë°ì´í„° ë¡œë“œ
        meeting_data_list = self.load_real_meeting_data(data_dir, num_samples)
        if not meeting_data_list:
            logger.error("âŒ No meeting data loaded. Test aborted.")
            return
        
        # ëª¨ë¸ ë¡œë”©
        success, backend = self.load_model()
        if not success:
            logger.error(f"âŒ Model loading failed: {backend}")
            return
        
        # ê° íšŒì˜ë¡ì— ëŒ€í•´ í…ŒìŠ¤íŠ¸
        test_results = []
        
        for i, meeting_data in enumerate(meeting_data_list):
            logger.info(f"\\n{'='*60}")
            logger.info(f"ğŸ§ª Test {i+1}/{len(meeting_data_list)}")
            
            result = self.test_single_meeting(meeting_data, backend)
            test_results.append(result)
            
            # ê²°ê³¼ ì¶œë ¥
            if result["generation_result"]["success"]:
                logger.info(f"âœ… Generation successful ({result['generation_result']['inference_time']:.2f}s)")
                logger.info("ğŸ“‹ Generated JSON:")
                print(json.dumps(result["generation_result"]["result"], ensure_ascii=False, indent=2))
            else:
                logger.error(f"âŒ Generation failed: {result['generation_result']['error']}")
                logger.info("ğŸš¨ Raw response:")
                print(result["generation_result"]["raw_response"][:500] + "..." if len(result["generation_result"]["raw_response"]) > 500 else result["generation_result"]["raw_response"])
            
            print("\\n" + "="*60 + "\\n")
        
        # ìµœì¢… ìš”ì•½
        successful_tests = [r for r in test_results if r["generation_result"]["success"]]
        logger.info(f"ğŸ‰ Test completed!")
        logger.info(f"ğŸ“Š Success rate: {len(successful_tests)}/{len(test_results)} ({len(successful_tests)/len(test_results)*100:.1f}%)")
        
        if successful_tests:
            avg_time = sum(r["generation_result"]["inference_time"] for r in successful_tests) / len(successful_tests)
            logger.info(f"â±ï¸  Average inference time: {avg_time:.2f}s")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del self.current_model
        del self.current_tokenizer
        torch.cuda.empty_cache()
        logger.info("ğŸ§¹ Memory cleaned up")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ëª… (ì§ì ‘ ìˆ˜ì •í•´ì„œ ì‚¬ìš©)
    MODEL_NAME = "Qwen/Qwen2.5-7B-Instruct"  # â† ì´ ë¶€ë¶„ì„ ë°”ê¿”ê°€ë©° í…ŒìŠ¤íŠ¸
    
    # ì„¤ì •
    DATA_DIR = "batch_triplet_results"  # ì‹¤ì œ íšŒì˜ë¡ ë°ì´í„° í´ë”
    NUM_SAMPLES = 3  # í…ŒìŠ¤íŠ¸í•  íšŒì˜ë¡ ê°œìˆ˜
    
    logger.info(f"ğŸ¯ Testing model: {MODEL_NAME}")
    logger.info(f"ğŸ“ Data directory: {DATA_DIR}")
    logger.info(f"ğŸ”¢ Number of samples: {NUM_SAMPLES}")
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    tester = SingleModelTester(MODEL_NAME)
    tester.run_test(DATA_DIR, NUM_SAMPLES)

if __name__ == "__main__":
    main()