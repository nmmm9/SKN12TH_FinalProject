#!/usr/bin/env python3
"""
TtalKkak Triplet íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸ ì½”ë“œ
íŒ€ì›ë“¤ì´ ë…ë¦½ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆë„ë¡ 3ê°œ íŒŒì¼ì„ í•˜ë‚˜ë¡œ í†µí•©

ê¸°ëŠ¥:
1. WhisperX JSON íŒŒì‹± (whisperX_parser.py)  
2. Triplet êµ¬ì¡° ìƒì„± (create_triplets.py)
3. Triplet ì „ì²˜ë¦¬ (triplet_preprocessor.py)

ì‚¬ìš©ë²•:
python triplet_pipeline_test.py [options]
"""

import json
import re
from typing import List, Dict
from datetime import timedelta
import argparse
import sys
import os
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ===============================================
# Part 0: BERT ë¶„ë¥˜ ëª¨ë¸
# ===============================================

class TtalkkakBERTClassifier(nn.Module):
    """TtalKkak íšŒì˜ ë°œí™” ì¤‘ìš”ë„ ë¶„ë¥˜ BERT ëª¨ë¸"""
    
    def __init__(self, model_name="klue/bert-base", num_labels=2):
        super().__init__()
        self.bert = AutoModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(0.3)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)
        
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        return logits

class BERTClassificationService:
    """BERT ë¶„ë¥˜ ì„œë¹„ìŠ¤"""
    
    def __init__(self, model_path: str = None):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = None
        self.tokenizer = None
        self.model_path = model_path
        
        if model_path and os.path.exists(model_path):
            self.load_model(model_path)
        else:
            logger.warning(f"BERT ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
            logger.info("ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
    
    def load_model(self, model_path: str):
        """BERT ëª¨ë¸ ë¡œë“œ"""
        try:
            logger.info(f"BERT ëª¨ë¸ ë¡œë”© ì¤‘: {model_path}")
            
            # ëª¨ë¸ í´ë” ê²½ë¡œ ì¶”ì¶œ
            model_dir = os.path.dirname(model_path)
            
            # ì‹¤ì œ ëª¨ë¸ í´ë”ì˜ í† í¬ë‚˜ì´ì €ì™€ config ì‚¬ìš©
            logger.info(f"ëª¨ë¸ í´ë”ì—ì„œ configì™€ tokenizer ë¡œë”©: {model_dir}")
            
            # í† í¬ë‚˜ì´ì € ë¡œë“œ (ëª¨ë¸ í´ë”ì—ì„œ)
            self.tokenizer = AutoTokenizer.from_pretrained(model_dir)
            
            # config ë¡œë“œ (ëª¨ë¸ í´ë”ì—ì„œ)
            from transformers import AutoConfig, AutoModelForSequenceClassification
            config = AutoConfig.from_pretrained(model_dir, num_labels=2)
            
            # ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„±
            self.model = AutoModelForSequenceClassification.from_config(config)
            
            # ì €ì¥ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
            if model_path.endswith('.pt'):
                state_dict = torch.load(model_path, map_location=self.device)
                self.model.load_state_dict(state_dict)
            else:
                logger.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ í˜•ì‹: {model_path}")
                return False
                
            self.model.to(self.device)
            self.model.eval()
            
            logger.info(f"BERT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (Device: {self.device})")
            logger.info(f"Vocab size: {self.tokenizer.vocab_size}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ BERT ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.model = None
            self.tokenizer = None
            return False
    
    def classify_text(self, text: str) -> int:
        """í…ìŠ¤íŠ¸ ë¶„ë¥˜ (0: ì¤‘ìš”, 1: ë…¸ì´ì¦ˆ)"""
        if self.model is None or self.tokenizer is None:
            # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ì‹œë®¬ë ˆì´ì…˜
            return self._simulate_classification(text)
        
        try:
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            text = text.replace("[TGT]", "").replace("[/TGT]", "").strip()
            
            # í† í°í™”
            encoding = self.tokenizer(
                text,
                truncation=True,
                padding=True,
                max_length=512,
                return_tensors='pt'
            )
            
            input_ids = encoding['input_ids'].to(self.device)
            attention_mask = encoding['attention_mask'].to(self.device)
            
            # ì¶”ë¡ 
            with torch.no_grad():
                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                logits = outputs.logits
                probabilities = torch.softmax(logits, dim=-1)
                predicted_label = torch.argmax(probabilities, dim=-1).item()
            
            return predicted_label
            
        except Exception as e:
            logger.error(f"ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜: {e}")
            return self._simulate_classification(text)
    
    def _simulate_classification(self, text: str) -> int:
        """BERT ë¶„ë¥˜ ì‹œë®¬ë ˆì´ì…˜"""
        text = text.lower()
        
        # ì¤‘ìš”í•´ ë³´ì´ëŠ” í‚¤ì›Œë“œë“¤
        important_keywords = [
            "í”„ë¡œì íŠ¸", "ê°œë°œ", "ì¼ì •", "ì™„ë£Œ", "ë‹´ë‹¹", "ì˜ˆì‚°", "ê³„íš", "ê²°ì •", 
            "íšŒì˜", "ë³´ê³ ", "ë¶„ì„", "ê²€í† ", "ìŠ¹ì¸", "ì§„í–‰", "ì—…ë¬´", "ê³¼ì œ", "ë§ˆê°"
        ]
        
        # ë¶ˆí•„ìš”í•´ ë³´ì´ëŠ” í‚¤ì›Œë“œë“¤  
        noise_keywords = [
            "ì•ˆë…•", "ê°ì‚¬", "ìˆ˜ê³ ", "ê·¸ëŸ¼", "ë„¤", "ì•„", "ìŒ", "ì–´", "ì ê¹", 
            "ì•„ë¬´íŠ¼", "ê·¸ëƒ¥", "ë­", "ì¢€", "ì´ì œ", "ê·¸ê±°", "ì´ê±°"
        ]
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
        important_score = sum(1 for kw in important_keywords if kw in text)
        noise_score = sum(1 for kw in noise_keywords if kw in text)
        
        if important_score > noise_score:
            return 0  # ì¤‘ìš”í•œ ë°œí™”
        elif noise_score > important_score:
            return 1  # ë¶ˆí•„ìš”í•œ ë°œí™”
        else:
            # ë¬¸ì¥ ê¸¸ì´ë¡œ íŒë‹¨ (ë„ˆë¬´ ì§§ìœ¼ë©´ ë…¸ì´ì¦ˆë¡œ ê°„ì£¼)
            return 1 if len(text.strip()) < 10 else 0

# ===============================================
# Part 1: WhisperX Parser (whisperX_parser.py)
# ===============================================

def seconds_to_timestamp(seconds: float) -> str:
    """ì´ˆë¥¼ HH:MM:SS í˜•íƒœë¡œ ë³€í™˜"""
    td = timedelta(seconds=seconds)
    return str(td).split('.')[0].zfill(8)

def split_sentences(text: str) -> List[str]:
    """ë¬¸ì¥ë¶€í˜¸ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬"""
    text = re.sub(r'([.?!])(?!\s)', r'\1 ', text)  # ë¬¸ì¥ë¶€í˜¸ ë’¤ì— ê³µë°± ì—†ìœ¼ë©´ ì¶”ê°€
    text = re.sub(r'([.?!])\s{2,}', r'\1 ', text)  # ë¬¸ì¥ë¶€í˜¸ ë’¤ 2ì¹¸ ì´ìƒ ê³µë°±ì„ 1ì¹¸ìœ¼ë¡œ
    return [s.strip() for s in re.split(r'(?<=[.?!])\s+', text.strip()) if s.strip()]

def parse_whisperx_json(json_path: str) -> List[Dict]:
    """WhisperX JSON íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜"""
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    segments = data.get("segments", [])
    entries = []
    global_index = 1
    
    for seg_index, seg in enumerate(segments, 1):
        speaker = seg.get("speaker", "UNKNOWN")
        text = seg.get("text", "").strip()
        start_time = seg.get("start", 0.0)
        timestamp = seconds_to_timestamp(start_time)
        
        sentences = split_sentences(text)
        for i, sent in enumerate(sentences, 1):
            entries.append({
                "timestamp": timestamp,
                "timestamp_order": f"{global_index}-{i}",
                "speaker": speaker,
                "text": sent
            })
        global_index += 1
        
    return entries

def parse_text_input(text: str) -> List[Dict]:
    """ì¼ë°˜ í…ìŠ¤íŠ¸ë¥¼ WhisperX í˜•íƒœë¡œ ë³€í™˜"""
    lines = [line.strip() for line in text.split('\n') if line.strip()]
    entries = []
    
    for idx, line in enumerate(lines, 1):
        # í™”ì: ë‚´ìš© í˜•íƒœë¡œ íŒŒì‹± ì‹œë„
        if ':' in line and not line.startswith('http'):
            parts = line.split(':', 1)
            speaker = parts[0].strip()
            text = parts[1].strip()
        else:
            speaker = "UNKNOWN"
            text = line
            
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
        sentences = split_sentences(text)
        for i, sent in enumerate(sentences, 1):
            entries.append({
                "timestamp": f"00:00:{idx:02d}",  # ê°€ìƒ íƒ€ì„ìŠ¤íƒ¬í”„
                "timestamp_order": f"{idx}-{i}",
                "speaker": speaker,
                "text": sent
            })
    
    return entries

# ===============================================
# Part 2: Triplet Creator (create_triplets.py)
# ===============================================

def create_structured_triplets(data: List[Dict]) -> List[Dict]:
    """WhisperX ë°ì´í„°ë¥¼ triplet êµ¬ì¡°ë¡œ ë³€í™˜"""
    speech_data = [d for d in data if "text" in d]
    
    # ìì—°ìŠ¤ëŸ¬ìš´ ì •ë ¬ (natsort ì—†ì´ êµ¬í˜„)
    def natural_sort_key(item):
        order = item["timestamp_order"]
        parts = order.split('-')
        return (int(parts[0]), int(parts[1]) if len(parts) > 1 else 0)
    
    sorted_data = sorted(speech_data, key=natural_sort_key)
    result = []
    
    for i in range(len(sorted_data)):
        item = sorted_data[i]
        
        # ì´ì „ ë§¥ë½: í˜„ì¬ ë°œí™” ê¸°ì¤€ ì´ì „ 2ê°œ ë°œí™” í…ìŠ¤íŠ¸ ê²°í•©
        prev_1 = sorted_data[i - 2]["text"] if i - 2 >= 0 else ""
        prev_2 = sorted_data[i - 1]["text"] if i - 1 >= 0 else ""
        prev = f"{prev_1} {prev_2}".strip()
        
        # ë‹¤ìŒ ë§¥ë½: í˜„ì¬ ë°œí™” ê¸°ì¤€ ë‹¤ìŒ 2ê°œ ë°œí™” í…ìŠ¤íŠ¸ ê²°í•©
        next_1 = sorted_data[i + 1]["text"] if i + 1 < len(sorted_data) else ""
        next_2 = sorted_data[i + 2]["text"] if i + 2 < len(sorted_data) else ""
        next_ = f"{next_1} {next_2}".strip()
        
        # triplet êµ¬ì¡° ë°ì´í„° ìƒì„±
        result.append({
            "timestamp": item.get("timestamp", ""),
            "timestamp_order": item["timestamp_order"],
            "speaker": item["speaker"],
            "prev": prev,
            "target": f"[TGT] {item['text']} [/TGT]",
            "next": next_,
            "label": None  # ì„ì‹œë¡œ None ì„¤ì •
        })
    
    return result

# ===============================================
# Part 3: Triplet Preprocessor (triplet_preprocessor.py)  
# ===============================================

def preprocess_triplets(triplets_with_labels: List[Dict], log_file_path: str = None) -> List[Dict]:
    """ë¼ë²¨ ê¸°ë°˜ triplet ë¶„ê¸° ì²˜ë¦¬"""
    label_0_items = []
    label_1_items = []
    
    for triplet in triplets_with_labels:
        if triplet.get("label") == 0:
            # label 0: ì¤‘ìš”í•œ ë°œí™” (ìœ ì§€)
            item = {
                "timestamp": triplet.get("timestamp", ""),
                "timestamp_order": triplet["timestamp_order"],
                "speaker": triplet["speaker"],
                "text": triplet["target"]
            }
            label_0_items.append(item)
            
        elif triplet.get("label") == 1:
            # label 1: ë¶ˆí•„ìš”í•œ ë°œí™” (ë¡œê·¸)
            item = {
                "timestamp": triplet.get("timestamp", ""),
                "timestamp_order": triplet["timestamp_order"],
                "speaker": triplet["speaker"],
                "text": triplet["target"],
                "label": triplet["label"]
            }
            label_1_items.append(item)
    
    # label 1 í•­ëª©ë“¤ì„ ë¡œê·¸ íŒŒì¼ì— ì €ì¥
    if log_file_path and label_1_items:
        with open(log_file_path, 'w', encoding='utf-8') as log_file:
            for item in label_1_items:
                log_file.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    return label_0_items

# ===============================================
# Part 4: í†µí•© í…ŒìŠ¤íŠ¸ ë° ì‹œë®¬ë ˆì´ì…˜ í•¨ìˆ˜ë“¤
# ===============================================

def bert_classification(triplets: List[Dict], model_path: str = None) -> List[Dict]:
    """ì‹¤ì œ BERT ëª¨ë¸ì„ ì‚¬ìš©í•œ ë¶„ë¥˜"""
    
    # ê¸°ë³¸ ëª¨ë¸ ê²½ë¡œ ì„¤ì •
    if model_path is None:
        model_path = r"c:\Users\SH\Desktop\TtalKkak\Bertëª¨ë¸\Ttalkkak_model_v2\Ttalkkak_model_v2.pt"
    
    # BERT ë¶„ë¥˜ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    bert_service = BERTClassificationService(model_path)
    
    logger.info(f"ğŸ¤– BERT ë¶„ë¥˜ ì‹œì‘ (ì´ {len(triplets)}ê°œ triplet)")
    
    for i, triplet in enumerate(triplets):
        text = triplet["target"]
        predicted_label = bert_service.classify_text(text)
        triplet["label"] = predicted_label
        
        if (i + 1) % 50 == 0:  # 50ê°œë§ˆë‹¤ ì§„í–‰ë¥  ì¶œë ¥
            logger.info(f"   ì§„í–‰ë¥ : {i+1}/{len(triplets)} ({(i+1)/len(triplets)*100:.1f}%)")
    
    # ë¶„ë¥˜ ê²°ê³¼ í†µê³„
    label_0_count = sum(1 for t in triplets if t["label"] == 0)
    label_1_count = sum(1 for t in triplets if t["label"] == 1)
    
    logger.info(f"âœ… BERT ë¶„ë¥˜ ì™„ë£Œ: ì¤‘ìš” ë°œí™” {label_0_count}ê°œ, ë…¸ì´ì¦ˆ {label_1_count}ê°œ")
    
    return triplets

def run_full_pipeline(input_data: str, input_type: str = "auto", output_dir: str = "./output", model_path: str = None):
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    print("TtalKkak Triplet íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(output_dir, exist_ok=True)
    
    # Step 1: ì…ë ¥ ë°ì´í„° íŒŒì‹±
    print("Step 1: ì…ë ¥ ë°ì´í„° íŒŒì‹±")
    if input_type == "json" or (input_type == "auto" and input_data.endswith('.json')):
        # JSON íŒŒì¼ ì²˜ë¦¬
        parsed_data = parse_whisperx_json(input_data)
        print(f"   WhisperX JSON íŒŒì¼ íŒŒì‹± ì™„ë£Œ: {len(parsed_data)}ê°œ í•­ëª©")
    else:
        # í…ìŠ¤íŠ¸ ì²˜ë¦¬
        if os.path.isfile(input_data):
            with open(input_data, 'r', encoding='utf-8') as f:
                text_content = f.read()
        else:
            text_content = input_data
        parsed_data = parse_text_input(text_content)
        print(f"   í…ìŠ¤íŠ¸ ì…ë ¥ íŒŒì‹± ì™„ë£Œ: {len(parsed_data)}ê°œ í•­ëª©")
    
    # íŒŒì‹± ê²°ê³¼ ì €ì¥
    with open(f"{output_dir}/01_parsed_data.json", 'w', encoding='utf-8') as f:
        json.dump(parsed_data, f, ensure_ascii=False, indent=2)
    
    # Step 2: Triplet êµ¬ì¡° ìƒì„±
    print("\nStep 2: Triplet êµ¬ì¡° ìƒì„±")
    triplets = create_structured_triplets(parsed_data)
    print(f"   Triplet êµ¬ì¡° ìƒì„± ì™„ë£Œ: {len(triplets)}ê°œ triplet")
    
    # Triplet ê²°ê³¼ ì €ì¥
    with open(f"{output_dir}/02_triplets.json", 'w', encoding='utf-8') as f:
        json.dump(triplets, f, ensure_ascii=False, indent=2)
    
    # Step 3: ì‹¤ì œ BERT ë¶„ë¥˜
    print("\nStep 3: ì‹¤ì œ BERT ëª¨ë¸ì„ ì‚¬ìš©í•œ ë¶„ë¥˜")
    classified_triplets = bert_classification(triplets, model_path)
    label_0_count = sum(1 for t in classified_triplets if t["label"] == 0)
    label_1_count = sum(1 for t in classified_triplets if t["label"] == 1)
    print(f"   ë¶„ë¥˜ ì™„ë£Œ: ì¤‘ìš” ë°œí™” {label_0_count}ê°œ, ë…¸ì´ì¦ˆ {label_1_count}ê°œ")
    
    # ë¶„ë¥˜ ê²°ê³¼ ì €ì¥
    with open(f"{output_dir}/03_classified_triplets.json", 'w', encoding='utf-8') as f:
        json.dump(classified_triplets, f, ensure_ascii=False, indent=2)
    
    # Step 4: Triplet ì „ì²˜ë¦¬
    print("\nStep 4: Triplet ì „ì²˜ë¦¬")
    filtered_data = preprocess_triplets(
        classified_triplets, 
        log_file_path=f"{output_dir}/04_noise_log.jsonl"
    )
    print(f"   ì „ì²˜ë¦¬ ì™„ë£Œ: ìµœì¢… {len(filtered_data)}ê°œ ë°œí™” ìœ ì§€")
    
    # ìµœì¢… ê²°ê³¼ ì €ì¥
    with open(f"{output_dir}/05_final_result.json", 'w', encoding='utf-8') as f:
        json.dump(filtered_data, f, ensure_ascii=False, indent=2)
    
    # Step 5: ê²°ê³¼ ìš”ì•½
    print("\nStep 5: ê²°ê³¼ ìš”ì•½")
    print(f"   ì›ë³¸ ë°ì´í„°: {len(parsed_data)}ê°œ í•­ëª©")
    print(f"   ìƒì„±ëœ Triplet: {len(triplets)}ê°œ")
    print(f"   ì¤‘ìš” ë°œí™”: {label_0_count}ê°œ ({label_0_count/len(triplets)*100:.1f}%)")
    print(f"   ë…¸ì´ì¦ˆ ë°œí™”: {label_1_count}ê°œ ({label_1_count/len(triplets)*100:.1f}%)")
    print(f"   ìµœì¢… ìœ ì§€: {len(filtered_data)}ê°œ")
    print(f"   ë…¸ì´ì¦ˆ ì œê±°ìœ¨: {(len(triplets)-len(filtered_data))/len(triplets)*100:.1f}%")
    
    print(f"\níŒŒì´í”„ë¼ì¸ ì™„ë£Œ! ê²°ê³¼ëŠ” '{output_dir}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    return {
        "parsed_data": parsed_data,
        "triplets": triplets,
        "classified_triplets": classified_triplets,
        "filtered_data": filtered_data,
        "stats": {
            "original_count": len(parsed_data),
            "triplet_count": len(triplets),
            "important_count": label_0_count,
            "noise_count": label_1_count,
            "final_count": len(filtered_data),
            "noise_reduction_rate": (len(triplets)-len(filtered_data))/len(triplets)*100
        }
    }

def create_sample_data():
    """ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
    sample_text = """íŒ€ì¥ ê¹€ë¯¼ì¤€: ì•ˆë…•í•˜ì„¸ìš”, ëª¨ë“  ë¶„ë“¤. ì˜¤ëŠ˜ì€ 2025ë…„ 1ì›” 28ì¼ í™”ìš”ì¼ì´ê³ , ìƒˆë¡œìš´ ë§ˆì¼€íŒ… ìº í˜ì¸ ê¸°íš íšŒì˜ë¥¼ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤.
ë§ˆì¼€íŒ… ë§¤ë‹ˆì € ì´ì„œì—°: ë§ˆì¼€íŒ… ë§¤ë‹ˆì € ì´ì„œì—° ì°¸ì„í–ˆìŠµë‹ˆë‹¤.
ë””ìì´ë„ˆ ë°•ì§€í›ˆ: ë””ìì´ë„ˆ ë°•ì§€í›ˆì…ë‹ˆë‹¤.
ë°ì´í„° ë¶„ì„ê°€ ìµœìœ ì§„: ë°ì´í„° ë¶„ì„ê°€ ìµœìœ ì§„ ì°¸ì„í–ˆìŠµë‹ˆë‹¤.
íŒ€ì¥ ê¹€ë¯¼ì¤€: ì¢‹ìŠµë‹ˆë‹¤. ì˜¤ëŠ˜ íšŒì˜ ëª©ì ì€ ë‹¤ìŒ ë¶„ê¸° ì‹ ì œí’ˆ ì¶œì‹œë¥¼ ìœ„í•œ ë§ˆì¼€íŒ… ì „ëµì„ ìˆ˜ë¦½í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
ë§ˆì¼€íŒ… ë§¤ë‹ˆì € ì´ì„œì—°: ë„¤, ê²½ìŸì‚¬ ë¶„ì„ ê²°ê³¼ ìš°ë¦¬ íƒ€ê²Ÿ ê³ ê°ì¸µì¸ 20-30ëŒ€ì—ì„œ ì†Œì…œë¯¸ë””ì–´ ë§ˆì¼€íŒ…ì´ ê°€ì¥ íš¨ê³¼ì ì¸ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.
ë°ì´í„° ë¶„ì„ê°€ ìµœìœ ì§„: ë§ìŠµë‹ˆë‹¤. ì§€ë‚œ ë¶„ê¸° ë°ì´í„°ë¥¼ ë³´ë©´ ì¸ìŠ¤íƒ€ê·¸ë¨ ê´‘ê³ ì˜ ì „í™˜ìœ¨ì´ 12%, í‹±í†¡ì´ 8%ì˜€ìŠµë‹ˆë‹¤.
íŒ€ì¥ ê¹€ë¯¼ì¤€: ê·¸ë ‡ë‹¤ë©´ ì†Œì…œë¯¸ë””ì–´ ì¤‘ì‹¬ìœ¼ë¡œ ì „ëµì„ ì§œëŠ” ê²Œ ì¢‹ê² ë„¤ìš”.
ë””ìì´ë„ˆ ë°•ì§€í›ˆ: íŠ¸ë Œë“œë¥¼ ë¶„ì„í•´ë³´ë‹ˆ ì§§ì€ ì˜ìƒ ì½˜í…ì¸ ì™€ ì¸í„°ë™í‹°ë¸Œí•œ ìŠ¤í† ë¦¬ í˜•íƒœê°€ ì¸ê¸°ê°€ ë§ìŠµë‹ˆë‹¤.
ë§ˆì¼€íŒ… ë§¤ë‹ˆì € ì´ì„œì—°: ì¢‹ì€ ì•„ì´ë””ì–´ë„¤ìš”. ì¸í”Œë£¨ì–¸ì„œ í˜‘ì—…ë„ ê³ ë ¤í•´ë´ì•¼ê² ìŠµë‹ˆë‹¤.
íŒ€ì¥ ê¹€ë¯¼ì¤€: êµ¬ì²´ì ì¸ ì‹¤í–‰ ê³„íšì„ ì„¸ì›Œë´…ì‹œë‹¤. ì´ì„œì—°ë‹˜ì€ ì¸í”Œë£¨ì–¸ì„œ ì„­ì™¸ë¥¼, ë°•ì§€í›ˆë‹˜ì€ í¬ë¦¬ì—ì´í‹°ë¸Œ ì œì‘ì„, ìµœìœ ì§„ë‹˜ì€ ì„±ê³¼ ì¸¡ì • ì²´ê³„ë¥¼ ë‹´ë‹¹í•´ì£¼ì„¸ìš”.
ë§ˆì¼€íŒ… ë§¤ë‹ˆì € ì´ì„œì—°: ì¸í”Œë£¨ì–¸ì„œ ë¦¬ìŠ¤íŠ¸ì—…ì€ ì´ë²ˆ ì£¼ ê¸ˆìš”ì¼ê¹Œì§€ ì™„ë£Œí•˜ê² ìŠµë‹ˆë‹¤.
ë””ìì´ë„ˆ ë°•ì§€í›ˆ: ì‹œì•ˆ ì‘ì—…ì€ ë‹¤ìŒ ì£¼ ìˆ˜ìš”ì¼ê¹Œì§€ ì¤€ë¹„í•˜ê² ìŠµë‹ˆë‹¤.
ë°ì´í„° ë¶„ì„ê°€ ìµœìœ ì§„: ì„±ê³¼ ì¸¡ì • ëŒ€ì‹œë³´ë“œëŠ” ë‹¤ìŒ ì£¼ ì›”ìš”ì¼ê¹Œì§€ êµ¬ì¶•í•˜ê² ìŠµë‹ˆë‹¤.
íŒ€ì¥ ê¹€ë¯¼ì¤€: ì™„ë²½í•©ë‹ˆë‹¤. ê·¸ëŸ¼ 2ì£¼ í›„ì¸ 2ì›” 11ì¼ì— ì¤‘ê°„ ì ê²€ íšŒì˜ë¥¼ ê°–ê² ìŠµë‹ˆë‹¤."""
    
    return sample_text

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='TtalKkak Triplet íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸')
    parser.add_argument('--input', '-i', type=str, help='ì…ë ¥ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” í…ìŠ¤íŠ¸')
    parser.add_argument('--type', '-t', choices=['auto', 'json', 'text'], default='auto', 
                       help='ì…ë ¥ íƒ€ì… (auto: ìë™ê°ì§€, json: WhisperX JSON, text: ì¼ë°˜ í…ìŠ¤íŠ¸)')
    parser.add_argument('--output', '-o', type=str, default='./triplet_test_output', 
                       help='ì¶œë ¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--sample', '-s', action='store_true', 
                       help='ìƒ˜í”Œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸')
    parser.add_argument('--model', '-m', type=str, 
                       default=r"c:\Users\SH\Desktop\TtalKkak\Bertëª¨ë¸\Ttalkkak_model_v2\Ttalkkak_model_v2.pt",
                       help='BERT ëª¨ë¸ íŒŒì¼ ê²½ë¡œ')
    
    args = parser.parse_args()
    
    try:
        if args.sample:
            # ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©
            print("ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
            input_data = create_sample_data()
            result = run_full_pipeline(input_data, input_type='text', output_dir=args.output, model_path=args.model)
            
        elif args.input:
            # ì§€ì •ëœ ì…ë ¥ ì‚¬ìš©
            result = run_full_pipeline(args.input, input_type=args.type, output_dir=args.output, model_path=args.model)
            
        else:
            # ëŒ€í™”í˜• ëª¨ë“œ
            print("TtalKkak Triplet íŒŒì´í”„ë¼ì¸ ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸")
            print("=" * 50)
            print("1. ìƒ˜í”Œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸")
            print("2. íŒŒì¼ ê²½ë¡œ ì…ë ¥")
            print("3. ì§ì ‘ í…ìŠ¤íŠ¸ ì…ë ¥")
            
            while True:
                choice = input("\nì„ íƒí•˜ì„¸ìš” (1-3): ").strip()
                
                if choice == '1':
                    input_data = create_sample_data()
                    result = run_full_pipeline(input_data, input_type='text', output_dir=args.output, model_path=args.model)
                    break
                elif choice == '2':
                    file_path = input("íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
                    if os.path.exists(file_path):
                        result = run_full_pipeline(file_path, input_type=args.type, output_dir=args.output, model_path=args.model)
                        break
                    else:
                        print("íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                elif choice == '3':
                    print("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ë¹ˆ ì¤„ì„ ë‘ ë²ˆ ì…ë ¥í•˜ë©´ ì¢…ë£Œ):")
                    lines = []
                    empty_count = 0
                    while True:
                        line = input()
                        if line.strip() == "":
                            empty_count += 1
                            if empty_count >= 2:
                                break
                        else:
                            empty_count = 0
                            lines.append(line)
                    
                    if lines:
                        input_data = '\n'.join(lines)
                        result = run_full_pipeline(input_data, input_type='text', output_dir=args.output, model_path=args.model)
                        break
                    else:
                        print("ì…ë ¥ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. 1-3 ì¤‘ì—ì„œ ì„ íƒí•˜ì„¸ìš”.")
        
        # ê°„ë‹¨í•œ í†µê³„ ì¶œë ¥
        print("\n" + "="*60)
        print("í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"ê²°ê³¼ íŒŒì¼ë“¤ì´ '{args.output}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("\nìƒì„±ëœ íŒŒì¼ë“¤:")
        print("   01_parsed_data.json      - íŒŒì‹±ëœ ì›ë³¸ ë°ì´í„°")
        print("   02_triplets.json         - ìƒì„±ëœ Triplet êµ¬ì¡°")  
        print("   03_classified_triplets.json - BERT ë¶„ë¥˜ ê²°ê³¼")
        print("   04_noise_log.jsonl       - ì œê±°ëœ ë…¸ì´ì¦ˆ ë°œí™”ë“¤")
        print("   05_final_result.json     - ìµœì¢… í•„í„°ë§ëœ ê²°ê³¼")
        
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()