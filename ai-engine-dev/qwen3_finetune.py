import json
import os
import torch
from typing import List, Dict, Any
from pathlib import Path
from datetime import datetime
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import Dataset
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TtalkkacDatasetConverter:
    """ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” í”„ë¡¬í”„íŠ¸ì™€ ë°ì´í„° í˜•ì‹ìœ¼ë¡œ ê³¨ë“œ ìŠ¤íƒ ë‹¤ë“œë¥¼ ë³€í™˜"""
    
    def __init__(self):
        # ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” í”„ë¡¬í”„íŠ¸
        self.system_prompt = """ë‹¹ì‹ ì€ íšŒì˜ë¡ì„ ë¶„ì„í•˜ì—¬ ì²´ê³„ì ì¸ ë…¸ì…˜ í”„ë¡œì íŠ¸ ê¸°íšì•ˆì„ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
íšŒì˜ ë‚´ìš©ì„ ì •í™•íˆ ë¶„ì„í•˜ê³  ì‹¤ë¬´ì—ì„œ ë°”ë¡œ í™œìš©í•  ìˆ˜ ìˆëŠ” êµ¬ì¡°í™”ëœ í”„ë¡œì íŠ¸ ê³„íšì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”."""

        self.user_prompt_template = """ë‹¤ìŒ íšŒì˜ë¡ì„ ë°”íƒ•ìœ¼ë¡œ ë…¸ì…˜ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì²´ê³„ì ì¸ í”„ë¡œì íŠ¸ ê¸°íšì•ˆì„ JSON í˜•ì‹ìœ¼ë¡œ ìƒì„±í•´ì£¼ì„¸ìš”.

**ìš”êµ¬ì‚¬í•­:**
1. í”„ë¡œì íŠ¸ëª…ê³¼ ëª©ì ì„ ëª…í™•íˆ ì •ì˜
2. í•µì‹¬ ì•„ì´ë””ì–´ì™€ ì‹¤í–‰ ê³„íš ìˆ˜ë¦½  
3. êµ¬ì²´ì ì¸ ëª©í‘œì™€ ê¸°ëŒ€ íš¨ê³¼ ë„ì¶œ
4. ì‹¤ë¬´ì§„ê³¼ ì¼ì • ê³„íš í¬í•¨
5. JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ

**íšŒì˜ë¡:**
{meeting_content}

**ì‘ë‹µ í˜•ì‹:**
JSON í˜•ì‹ìœ¼ë¡œ í”„ë¡œì íŠ¸ ê¸°íšì•ˆì„ ìƒì„±í•´ì£¼ì„¸ìš”."""

    def load_gold_standard_data(self, results_dir: str) -> List[Dict[str, Any]]:
        """ê³¨ë“œ ìŠ¤íƒ ë‹¤ë“œ ê²°ê³¼ í´ë”ì—ì„œ ë°ì´í„° ë¡œë“œ"""
        results_path = Path(results_dir)
        data = []
        
        if not results_path.exists():
            logger.error(f"ê²°ê³¼ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {results_dir}")
            return []
        
        # ì„±ê³µì ìœ¼ë¡œ ìƒì„±ëœ result.json íŒŒì¼ë“¤ ìŠ¤ìº”
        for folder in results_path.iterdir():
            if folder.is_dir() and folder.name.startswith(('train_', 'val_')):
                result_file = folder / "result.json"
                if result_file.exists():
                    try:
                        with open(result_file, 'r', encoding='utf-8') as f:
                            item = json.load(f)
                        data.append(item)
                        logger.info(f"ë¡œë“œ: {folder.name}")
                    except Exception as e:
                        logger.error(f"{folder.name} ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        logger.info(f"ì´ {len(data)}ê°œ ê³¨ë“œ ìŠ¤íƒ ë‹¤ë“œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        return data

    def load_meeting_content(self, source_dir: str, meeting_folder: str) -> str:
        """ì›ë³¸ íšŒì˜ ë‚´ìš© ë¡œë“œ (batch_triplet_resultsì—ì„œ)"""
        # result_ ì ‘ë‘ì‚¬ê°€ ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸
        if meeting_folder.startswith("result_"):
            meeting_path = Path(source_dir) / meeting_folder / "05_final_result.json"
        else:
            meeting_path = Path(source_dir) / f"result_{meeting_folder}" / "05_final_result.json"
        
        if not meeting_path.exists():
            logger.warning(f"íšŒì˜ë¡ íŒŒì¼ ì—†ìŒ: {meeting_path}")
            return ""
        
        try:
            with open(meeting_path, 'r', encoding='utf-8') as f:
                meeting_data = json.load(f)
            
            # ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ íšŒì˜ ë‚´ìš© êµ¬ì„±
            meeting_text = ""
            for item in meeting_data:
                timestamp = item.get('timestamp', 'Unknown')
                speaker = item.get('speaker', 'Unknown')
                text = item.get('text', '')
                meeting_text += f"[{timestamp}] {speaker}: {text}\n"
            
            return meeting_text.strip()
        except Exception as e:
            logger.error(f"íšŒì˜ë¡ ë¡œë“œ ì‹¤íŒ¨ {meeting_path}: {e}")
            return ""

    def convert_to_training_format(self, gold_data: List[Dict[str, Any]], 
                                 source_dir: str = "batch_triplet_results") -> List[Dict[str, str]]:
        """ê³¨ë“œ ìŠ¤íƒ ë‹¤ë“œë¥¼ Qwen3 íŒŒì¸íŠœë‹ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        training_data = []
        
        for item in gold_data:
            try:
                # ì›ë³¸ íšŒì˜ ë‚´ìš© ë¡œë“œ
                metadata = item.get('metadata', {})
                source_file = metadata.get('source_file', '')
                
                # ë””ë²„ê¹…: ë©”íƒ€ë°ì´í„° ì¶œë ¥
                logger.info(f"ë©”íƒ€ë°ì´í„° í™•ì¸: {metadata}")
                
                if not source_file:
                    logger.warning(f"source_file ì—†ìŒ: {item.get('id', 'Unknown')}")
                    continue
                
                # source_fileì—ì„œ ì‹¤ì œ í´ë”ëª… ì¶”ì¶œ
                # '../batch_triplet_results\\result_í´ë”ëª…\\05_final_result.json' â†’ 'í´ë”ëª…'
                if 'result_' in source_file:
                    # result_ ì´í›„ ë¶€ë¶„ì„ ì¶”ì¶œí•˜ê³  \\05_final_result.json ì œê±°
                    temp = source_file.split('result_', 1)[1]
                    # ê²½ë¡œ êµ¬ë¶„ìì™€ íŒŒì¼ëª… ì œê±° (Windows/Linux í˜¸í™˜)
                    source_folder = temp.replace('\\05_final_result.json', '').replace('/05_final_result.json', '')
                else:
                    source_folder = source_file.replace('train_', '').replace('val_', '')
                
                logger.info(f"ì¶”ì¶œëœ í´ë”ëª…: '{source_folder}'")
                
                meeting_content = self.load_meeting_content(source_dir, source_folder)
                if not meeting_content:
                    logger.warning(f"íšŒì˜ ë‚´ìš© ì—†ìŒ: {source_folder}")
                    continue
                
                # ì‹¤ì œ í”„ë¡œì íŠ¸ í”„ë¡¬í”„íŠ¸ í˜•ì‹ìœ¼ë¡œ êµ¬ì„±
                user_message = self.user_prompt_template.format(meeting_content=meeting_content)
                
                # ê³¨ë“œ ìŠ¤íƒ ë‹¤ë“œ ì‘ë‹µ (JSON í˜•ì‹)
                result_data = item.get('result', {})
                if not result_data:
                    logger.warning(f"ê²°ê³¼ ë°ì´í„° ì—†ìŒ: {item}")
                    continue
                    
                assistant_response = json.dumps(result_data, ensure_ascii=False, indent=2)
                
                # Qwen3 ì±„íŒ… í˜•ì‹ìœ¼ë¡œ êµ¬ì„±
                conversation = f"<|im_start|>system\n{self.system_prompt}<|im_end|>\n<|im_start|>user\n{user_message}<|im_end|>\n<|im_start|>assistant\n{assistant_response}<|im_end|>"
                
                training_data.append({
                    "text": conversation,
                    "metadata": {
                        "id": item.get('metadata', {}).get('source_file', ''),
                        "source": source_folder,
                        "quality_score": 8.0,  # ê³¨ë“œ ìŠ¤íƒ ë‹¤ë“œëŠ” ê³ í’ˆì§ˆë¡œ ê°„ì£¼
                        "is_high_quality": True,
                        "dataset_type": "train" if "train_" in str(item.get('metadata', {}).get('source_file', '')) else "val"
                    }
                })
                
                logger.info(f"ë³€í™˜ ì™„ë£Œ: {source_folder}")
                
            except Exception as e:
                logger.error(f"ë³€í™˜ ì‹¤íŒ¨ {item.get('id', 'Unknown')}: {e}")
                continue
        
        return training_data

class QwenFineTuner:
    def __init__(self, model_name: str = "Qwen/Qwen3-14B-AWQ"):
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        
    def setup_model_and_tokenizer(self):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì„¤ì •"""
        logger.info(f"ëª¨ë¸ ë¡œë”©: {self.model_name}")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True,
            padding_side="right"
        )
        
        # íŒ¨ë”© í† í° ì„¤ì •
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # AWQ ëª¨ë¸ ë¡œë“œ
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
            attn_implementation="flash_attention_2" if torch.cuda.is_available() else "eager",
            # AWQ ëª¨ë¸ íŠ¹í™” ì„¤ì •
            use_cache=False,
            low_cpu_mem_usage=True
        )
        
        logger.info("ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ")
    
    def setup_lora_config(self) -> LoraConfig:
        """LoRA ì„¤ì •"""
        return LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=16,  # LoRA rank
            lora_alpha=32,  # LoRA scaling parameter
            lora_dropout=0.1,
            target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
            bias="none",
        )
    
    def prepare_dataset(self, training_data: List[Dict[str, str]], max_length: int = 2048):
        """ë°ì´í„°ì…‹ ì¤€ë¹„ ë° í† í¬ë‚˜ì´ì§•"""
        def tokenize_function(examples):
            # í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•
            tokenized = self.tokenizer(
                examples["text"],
                truncation=True,
                padding=False,
                max_length=max_length,
                return_tensors=None,
            )
            
            # labels = input_ids (ìë™íšŒê·€ ì–¸ì–´ ëª¨ë¸ë§)
            tokenized["labels"] = tokenized["input_ids"].copy()
            return tokenized
        
        # ê³ í’ˆì§ˆ ë°ì´í„°ë§Œ í•„í„°ë§ (7ì  ì´ìƒ)
        high_quality_data = [
            item for item in training_data 
            if item["metadata"]["is_high_quality"]
        ]
        
        logger.info(f"ì „ì²´ ë°ì´í„°: {len(training_data)}ê°œ, ê³ í’ˆì§ˆ ë°ì´í„°: {len(high_quality_data)}ê°œ")
        
        # train/val ë¶„í• 
        train_data = [item for item in high_quality_data if item["metadata"]["dataset_type"] == "train"]
        val_data = [item for item in high_quality_data if item["metadata"]["dataset_type"] == "val"]
        
        logger.info(f"í•™ìŠµ ë°ì´í„°: {len(train_data)}ê°œ, ê²€ì¦ ë°ì´í„°: {len(val_data)}ê°œ")
        
        # Dataset ê°ì²´ ìƒì„±
        train_dataset = Dataset.from_list([{"text": item["text"]} for item in train_data])
        val_dataset = Dataset.from_list([{"text": item["text"]} for item in val_data])
        
        # í† í¬ë‚˜ì´ì§•
        train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=["text"])
        val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=["text"])
        
        return train_dataset, val_dataset
    
    def train(self, train_dataset, val_dataset, output_dir: str = "./qwen3_lora_ttalkkac"):
        """LoRA íŒŒì¸íŠœë‹ ì‹¤í–‰"""
        
        # LoRA ì ìš©
        lora_config = self.setup_lora_config()
        self.model = get_peft_model(self.model, lora_config)
        
        # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ì¶œë ¥
        self.model.print_trainable_parameters()
        
        # í•™ìŠµ ì¸ì ì„¤ì •
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=3,
            per_device_train_batch_size=1,
            per_device_eval_batch_size=1,
            gradient_accumulation_steps=8,
            warmup_steps=100,
            learning_rate=2e-4,
            fp16=True,
            logging_steps=10,
            evaluation_strategy="steps",
            eval_steps=50,
            save_steps=100,
            save_total_limit=3,
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            dataloader_pin_memory=False,
            remove_unused_columns=False,
            gradient_checkpointing=True,
            report_to=None,  # wandb ë“± ë¹„í™œì„±í™”
        )
        
        # ë°ì´í„° ì½œë ˆì´í„°
        data_collator = DataCollatorForSeq2Seq(
            tokenizer=self.tokenizer,
            model=self.model,
            padding=True,
            return_tensors="pt"
        )
        
        # íŠ¸ë ˆì´ë„ˆ ì„¤ì •
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            data_collator=data_collator,
            tokenizer=self.tokenizer,
        )
        
        # í•™ìŠµ ì‹¤í–‰
        logger.info("íŒŒì¸íŠœë‹ ì‹œì‘...")
        train_result = trainer.train()
        
        # ëª¨ë¸ ì €ì¥
        trainer.save_model()
        trainer.save_state()
        
        # í•™ìŠµ ê²°ê³¼ ì €ì¥
        with open(os.path.join(output_dir, "training_results.json"), "w") as f:
            json.dump({
                "train_runtime": train_result.metrics["train_runtime"],
                "train_samples_per_second": train_result.metrics["train_samples_per_second"],
                "train_steps_per_second": train_result.metrics["train_steps_per_second"],
                "total_flos": train_result.metrics["total_flos"],
                "train_loss": train_result.metrics["train_loss"],
            }, f, indent=2)
        
        logger.info(f"íŒŒì¸íŠœë‹ ì™„ë£Œ! ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {output_dir}")
        return trainer

def main():
    print("=" * 60)
    print("ğŸš€ Ttalkkac Qwen3 LoRA íŒŒì¸íŠœë‹ ì‹œì‘")
    print("=" * 60)
    
    # 1. ë°ì´í„° ë³€í™˜
    print("\nğŸ“Š 1. ê³¨ë“œ ìŠ¤íƒ ë‹¤ë“œ ë°ì´í„° ë¡œë“œ ë° ë³€í™˜")
    converter = TtalkkacDatasetConverter()
    
    results_dir = "ttalkkac_gold_standard_results_20250731_104912"
    gold_data = converter.load_gold_standard_data(results_dir)
    
    if not gold_data:
        print("âŒ ê³¨ë“œ ìŠ¤íƒ ë‹¤ë“œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"âœ… ê³¨ë“œ ìŠ¤íƒ ë‹¤ë“œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(gold_data)}ê°œ")
    
    # ì‹¤ì œ í”„ë¡œì íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    training_data = converter.convert_to_training_format(gold_data)
    
    if not training_data:
        print("âŒ ë³€í™˜ëœ í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"âœ… í•™ìŠµ ë°ì´í„° ë³€í™˜ ì™„ë£Œ: {len(training_data)}ê°œ")
    
    # 2. íŒŒì¸íŠœë‹ ì„¤ì • ë° ì‹¤í–‰
    print("\nğŸ¤– 2. Qwen3 ëª¨ë¸ ì„¤ì • ë° LoRA íŒŒì¸íŠœë‹")
    
    # GPU í™•ì¸
    if torch.cuda.is_available():
        print(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name()}")
        print(f"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    else:
        print("âš ï¸ CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    
    # íŒŒì¸íŠœë„ˆ ì´ˆê¸°í™”
    finetuner = QwenFineTuner("Qwen/Qwen3-14B-AWQ")
    finetuner.data_converter = converter
    
    # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì„¤ì •
    finetuner.setup_model_and_tokenizer()
    
    # ë°ì´í„°ì…‹ ì¤€ë¹„
    train_dataset, val_dataset = finetuner.prepare_dataset(training_data, max_length=2048)
    
    # íŒŒì¸íŠœë‹ ì‹¤í–‰
    output_dir = f"./qwen3_lora_ttalkkac_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    trainer = finetuner.train(train_dataset, val_dataset, output_dir)
    
    print("\nğŸ‰ íŒŒì¸íŠœë‹ ì™„ë£Œ!")
    print(f"ğŸ“ ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {output_dir}")
    print("\nğŸ’¡ ëª¨ë¸ ì‚¬ìš© ë°©ë²•:")
    print("1. LoRA ì–´ëŒ‘í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("2. ì¶”ë¡  ì‹œì—ëŠ” ë² ì´ìŠ¤ ëª¨ë¸ + LoRA ì–´ëŒ‘í„°ë¥¼ í•¨ê»˜ ë¡œë“œí•˜ì„¸ìš”.")
    print("3. ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” í”„ë¡¬í”„íŠ¸ í˜•ì‹ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.")

if __name__ == "__main__":
    main()