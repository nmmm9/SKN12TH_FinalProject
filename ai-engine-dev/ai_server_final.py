"""
TtalKkak ìµœì¢… AI ì„œë²„ - 2ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤ êµ¬í˜„
íšŒì˜ë¡ â†’ ê¸°íšì•ˆ â†’ Task Master PRD â†’ ì—…ë¬´ìƒì„±
"""

import os
import io
import json
import tempfile
import logging
from typing import Optional, Dict, Any, List
from contextlib import asynccontextmanager
import time

import torch
import numpy as np
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
from task_schemas import (
    TaskItem, MeetingAnalysisResult, ComplexityAnalysis, 
    TaskExpansionRequest, TaskExpansionResult, PipelineRequest, PipelineResult,
    calculate_task_complexity_advanced, validate_task_dependencies, 
    generate_task_id_sequence, TASK_SCHEMA_EXAMPLE
)
from meeting_analysis_prompts import (
    generate_meeting_analysis_system_prompt,
    generate_meeting_analysis_user_prompt,
    generate_task_expansion_system_prompt,
    generate_complexity_analysis_prompt,
    validate_meeting_analysis
)
from prd_generation_prompts import (
    generate_notion_project_prompt,
    generate_task_master_prd_prompt,
    format_notion_project,
    format_task_master_prd,
    validate_notion_project,
    validate_task_master_prd,
    NOTION_PROJECT_SCHEMA,
    TASK_MASTER_PRD_SCHEMA
)

import uvicorn

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ê¸€ë¡œë²Œ ëª¨ë¸ ë³€ìˆ˜
whisper_model = None
qwen_model = None
qwen_tokenizer = None

# ìƒˆë¡œìš´ ì‘ë‹µ ëª¨ë¸ë“¤
class NotionProjectResponse(BaseModel):
    success: bool
    notion_project: Optional[Dict[str, Any]] = None
    formatted_notion: Optional[str] = None
    error: Optional[str] = None

class TaskMasterPRDResponse(BaseModel):
    success: bool
    prd_data: Optional[Dict[str, Any]] = None
    formatted_prd: Optional[str] = None
    error: Optional[str] = None

class TwoStageAnalysisRequest(BaseModel):
    transcript: str
    generate_notion: bool = True
    generate_tasks: bool = True
    num_tasks: int = 5
    additional_context: Optional[str] = None

class TwoStageAnalysisResponse(BaseModel):
    success: bool
    stage1_notion: Optional[Dict[str, Any]] = None
    stage2_prd: Optional[Dict[str, Any]] = None
    stage3_tasks: Optional[MeetingAnalysisResult] = None
    formatted_notion: Optional[str] = None
    formatted_prd: Optional[str] = None
    processing_time: Optional[float] = None
    error: Optional[str] = None

# ê¸°ì¡´ ëª¨ë¸ë“¤
class TranscriptionResponse(BaseModel):
    success: bool
    transcription: Optional[Dict[str, Any]] = None
    error: Optional[str] = None

class AnalysisRequest(BaseModel):
    transcript: str
    num_tasks: int = 5
    additional_context: Optional[str] = None

class AnalysisResponse(BaseModel):
    success: bool
    analysis: Optional[MeetingAnalysisResult] = None
    error: Optional[str] = None

class HealthResponse(BaseModel):
    status: str
    gpu_available: bool
    gpu_count: int
    models_loaded: Dict[str, bool]
    memory_info: Optional[Dict[str, float]] = None

def load_whisperx():
    """WhisperX ëª¨ë¸ ë¡œë”©"""
    global whisper_model
    
    if whisper_model is None:
        logger.info("ğŸ¤ Loading WhisperX large-v3...")
        try:
            import whisperx
            device = "cuda" if torch.cuda.is_available() else "cpu"
            compute_type = "float16" if device == "cuda" else "int8"
            
            whisper_model = whisperx.load_model(
                "large-v3", 
                device, 
                compute_type=compute_type,
                language="ko"
            )
            logger.info("âœ… WhisperX loaded successfully")
            
        except Exception as e:
            logger.error(f"âŒ WhisperX loading failed: {e}")
            raise e
    
    return whisper_model

def load_qwen3():
    """Qwen3-32B-AWQ ëª¨ë¸ ë¡œë”©"""
    global qwen_model, qwen_tokenizer
    
    if qwen_model is None or qwen_tokenizer is None:
        logger.info("ğŸ§  Loading Qwen3-32B-AWQ...")
        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM
            
            model_name = "Qwen/Qwen3-32B-AWQ"
            
            qwen_tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True
            )
            
            qwen_model = AutoModelForCausalLM.from_pretrained(
                model_name,
                device_map="auto",
                torch_dtype=torch.float16,
                trust_remote_code=True
            )
            
            logger.info("âœ… Qwen3-32B-AWQ loaded successfully")
            
        except Exception as e:
            logger.error(f"âŒ Qwen3-32B-AWQ loading failed: {e}")
            raise e
    
    return qwen_model, qwen_tokenizer

def generate_structured_response(
    system_prompt: str, 
    user_prompt: str, 
    response_schema: Dict[str, Any],
    temperature: float = 0.3
) -> Dict[str, Any]:
    """êµ¬ì¡°í™”ëœ ì‘ë‹µ ìƒì„±"""
    
    # ëª¨ë¸ ë¡œë”©
    qwen_model, qwen_tokenizer = load_qwen3()
    
    # ìŠ¤í‚¤ë§ˆ ì˜ˆì‹œ í¬í•¨ í”„ë¡¬í”„íŠ¸
    schema_prompt = f"""
{system_prompt}

**Response Schema:**
You must respond with a JSON object following this exact structure:
```json
{json.dumps(response_schema, indent=2, ensure_ascii=False)}
```

**Important Rules:**
1. Always return valid JSON format
2. Use Korean for all text content unless technical terms require English
3. Follow the exact schema structure
4. Include all required fields
5. Use appropriate data types for each field

{user_prompt}

**Response:**
```json
"""
    
    # ë©”ì‹œì§€ êµ¬ì„±
    messages = [{"role": "user", "content": schema_prompt}]
    
    # í† í°í™”
    text = qwen_tokenizer.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True
    )
    
    inputs = qwen_tokenizer([text], return_tensors="pt").to(qwen_model.device)
    
    # ì¶”ë¡  ì‹¤í–‰
    with torch.no_grad():
        outputs = qwen_model.generate(
            **inputs,
            max_new_tokens=2048,
            temperature=temperature,
            do_sample=True,
            pad_token_id=qwen_tokenizer.eos_token_id,
            repetition_penalty=1.1,
            top_p=0.9
        )
    
    # ê²°ê³¼ ë””ì½”ë”©
    response = qwen_tokenizer.decode(
        outputs[0][len(inputs["input_ids"][0]):], 
        skip_special_tokens=True
    )
    
    # JSON ì¶”ì¶œ ë° íŒŒì‹±
    try:
        # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
        if "```json" in response:
            json_start = response.find("```json") + 7
            json_end = response.find("```", json_start)
            if json_end == -1:
                json_end = len(response)
            json_text = response[json_start:json_end].strip()
        elif "{" in response and "}" in response:
            json_start = response.find("{")
            json_end = response.rfind("}") + 1
            json_text = response[json_start:json_end]
        else:
            json_text = response
        
        # JSON íŒŒì‹±
        parsed_result = json.loads(json_text)
        return parsed_result
        
    except json.JSONDecodeError as e:
        logger.warning(f"âš ï¸ JSON parsing failed: {e}")
        logger.warning(f"Raw response: {response}")
        
        # ê¸°ë³¸ êµ¬ì¡° ë°˜í™˜
        return {
            "error": "AI ì‘ë‹µì„ íŒŒì‹±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            "raw_response": response
        }

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì•± ì‹œì‘/ì¢…ë£Œ ì‹œ ëª¨ë¸ ë¡œë”©/ì •ë¦¬"""
    logger.info("ğŸš€ Starting TtalKkak Final AI Server...")
    
    # ëª¨ë¸ë“¤ì„ ë¯¸ë¦¬ ë¡œë”© (ì„ íƒì‚¬í•­)
    try:
        if os.getenv("PRELOAD_MODELS", "false").lower() == "true":
            logger.info("ğŸ“¦ Preloading models...")
            load_whisperx()
            load_qwen3()
            logger.info("âœ… Models preloaded successfully")
    except Exception as e:
        logger.warning(f"âš ï¸ Model preloading failed: {e}")
    
    yield
    
    logger.info("ğŸ›‘ Shutting down TtalKkak Final AI Server...")

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="TtalKkak Final AI Server",
    description="WhisperX + Qwen3-14B + 2-Stage PRD Process",
    version="3.0.0",
    lifespan=lifespan
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/", response_model=Dict[str, str])
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "TtalKkak Final AI Server",
        "version": "3.0.0",
        "features": [
            "WhisperX Speech-to-Text",
            "2-Stage PRD Process",
            "Notion Project Generation", 
            "Task Master PRD Format",
            "Advanced Task Generation"
        ],
        "workflow": "íšŒì˜ë¡ â†’ ê¸°íšì•ˆ â†’ Task Master PRD â†’ ì—…ë¬´ìƒì„±",
        "docs": "/docs"
    }

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    gpu_available = torch.cuda.is_available()
    gpu_count = torch.cuda.device_count() if gpu_available else 0
    
    memory_info = None
    if gpu_available:
        try:
            memory_info = {
                "allocated_gb": torch.cuda.memory_allocated() / 1024**3,
                "reserved_gb": torch.cuda.memory_reserved() / 1024**3,
                "total_gb": torch.cuda.get_device_properties(0).total_memory / 1024**3
            }
        except:
            pass
    
    return HealthResponse(
        status="healthy",
        gpu_available=gpu_available,
        gpu_count=gpu_count,
        models_loaded={
            "whisperx": whisper_model is not None,
            "qwen3": qwen_model is not None
        },
        memory_info=memory_info
    )

@app.post("/transcribe", response_model=TranscriptionResponse)
async def transcribe_audio(audio: UploadFile = File(...)):
    """ìŒì„± íŒŒì¼ ì „ì‚¬ (WhisperX)"""
    try:
        logger.info(f"ğŸ¤ Transcribing audio: {audio.filename}")
        
        # ëª¨ë¸ ë¡œë”©
        whisper_model = load_whisperx()
        
        # ì˜¤ë””ì˜¤ íŒŒì¼ ì„ì‹œ ì €ì¥
        audio_content = await audio.read()
        
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_file:
            temp_file.write(audio_content)
            temp_path = temp_file.name
        
        try:
            # WhisperX ì „ì‚¬ ì‹¤í–‰
            result = whisper_model.transcribe(temp_path, batch_size=16)
            
            segments = result.get("segments", [])
            full_text = " ".join([seg.get("text", "") for seg in segments])
            
            logger.info(f"âœ… Transcription completed: {len(full_text)} characters")
            
            return TranscriptionResponse(
                success=True,
                transcription={
                    "segments": segments,
                    "full_text": full_text,
                    "language": result.get("language", "ko"),
                    "duration": sum([seg.get("end", 0) - seg.get("start", 0) for seg in segments])
                }
            )
            
        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            try:
                os.unlink(temp_path)
            except:
                pass
                
    except Exception as e:
        logger.error(f"âŒ Transcription error: {e}")
        return TranscriptionResponse(
            success=False,
            error=str(e)
        )

@app.post("/generate-notion-project", response_model=NotionProjectResponse)
async def generate_notion_project(request: AnalysisRequest):
    """1ë‹¨ê³„: íšŒì˜ë¡ â†’ ë…¸ì…˜ ê¸°íšì•ˆ ìƒì„±"""
    try:
        logger.info("ğŸ“ Stage 1: Generating Notion project document...")
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        system_prompt = "ë‹¹ì‹ ì€ íšŒì˜ë¡ì„ ë¶„ì„í•˜ì—¬ ì²´ê³„ì ì¸ í”„ë¡œì íŠ¸ ê¸°íšì•ˆì„ ì‘ì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
        user_prompt = generate_notion_project_prompt(request.transcript)
        
        # êµ¬ì¡°í™”ëœ ì‘ë‹µ ìƒì„±
        result = generate_structured_response(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            response_schema=NOTION_PROJECT_SCHEMA,
            temperature=0.4
        )
        
        if "error" in result:
            return NotionProjectResponse(
                success=False,
                error=result["error"]
            )
        
        # ë°ì´í„° ê²€ì¦
        validated_result = validate_notion_project(result)
        
        # ë…¸ì…˜ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…
        formatted_notion = format_notion_project(validated_result)
        
        logger.info("âœ… Stage 1 completed: Notion project generated")
        
        return NotionProjectResponse(
            success=True,
            notion_project=validated_result,
            formatted_notion=formatted_notion
        )
        
    except Exception as e:
        logger.error(f"âŒ Notion project generation error: {e}")
        return NotionProjectResponse(
            success=False,
            error=str(e)
        )

@app.post("/generate-task-master-prd", response_model=TaskMasterPRDResponse)
async def generate_task_master_prd(notion_project: Dict[str, Any]):
    """2ë‹¨ê³„: ë…¸ì…˜ ê¸°íšì•ˆ â†’ Task Master PRD ë³€í™˜"""
    try:
        logger.info("ğŸ”„ Stage 2: Converting to Task Master PRD format...")
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        system_prompt = "ë‹¹ì‹ ì€ ê¸°íšì•ˆì„ Task Master PRD í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
        user_prompt = generate_task_master_prd_prompt(notion_project)
        
        # êµ¬ì¡°í™”ëœ ì‘ë‹µ ìƒì„±
        result = generate_structured_response(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            response_schema=TASK_MASTER_PRD_SCHEMA,
            temperature=0.3
        )
        
        if "error" in result:
            return TaskMasterPRDResponse(
                success=False,
                error=result["error"]
            )
        
        # ë°ì´í„° ê²€ì¦
        validated_result = validate_task_master_prd(result)
        
        # Task Master PRD í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…
        formatted_prd = format_task_master_prd(validated_result)
        
        logger.info("âœ… Stage 2 completed: Task Master PRD generated")
        
        return TaskMasterPRDResponse(
            success=True,
            prd_data=validated_result,
            formatted_prd=formatted_prd
        )
        
    except Exception as e:
        logger.error(f"âŒ Task Master PRD generation error: {e}")
        return TaskMasterPRDResponse(
            success=False,
            error=str(e)
        )

@app.post("/two-stage-analysis", response_model=TwoStageAnalysisResponse)
async def two_stage_analysis(request: TwoStageAnalysisRequest):
    """2ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤ í†µí•© ë¶„ì„"""
    try:
        logger.info("ğŸš€ Starting 2-stage analysis process...")
        
        start_time = time.time()
        
        # 1ë‹¨ê³„: ë…¸ì…˜ ê¸°íšì•ˆ ìƒì„±
        stage1_result = None
        if request.generate_notion:
            logger.info("ğŸ“ Stage 1: Generating Notion project...")
            notion_request = AnalysisRequest(
                transcript=request.transcript,
                additional_context=request.additional_context
            )
            stage1_response = await generate_notion_project(notion_request)
            
            if not stage1_response.success:
                return TwoStageAnalysisResponse(
                    success=False,
                    error=f"Stage 1 failed: {stage1_response.error}"
                )
            
            stage1_result = stage1_response.notion_project
        
        # 2ë‹¨ê³„: Task Master PRD ë³€í™˜
        stage2_result = None
        if request.generate_tasks and stage1_result:
            logger.info("ğŸ”„ Stage 2: Converting to Task Master PRD...")
            stage2_response = await generate_task_master_prd(stage1_result)
            
            if not stage2_response.success:
                return TwoStageAnalysisResponse(
                    success=False,
                    error=f"Stage 2 failed: {stage2_response.error}"
                )
            
            stage2_result = stage2_response.prd_data
        
        # 3ë‹¨ê³„: íƒœìŠ¤í¬ ìƒì„± (Task Master ë°©ì‹)
        stage3_result = None
        if request.generate_tasks and stage2_result:
            logger.info("ğŸ¯ Stage 3: Generating tasks using Task Master approach...")
            
            # Task Master PRDë¥¼ ì‚¬ìš©í•œ íƒœìŠ¤í¬ ìƒì„±
            prd_content = format_task_master_prd(stage2_result)
            system_prompt = generate_meeting_analysis_system_prompt(request.num_tasks)
            user_prompt = f"""
            ë‹¤ìŒ PRD ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ {request.num_tasks}ê°œì˜ êµ¬ì²´ì ì¸ ê°œë°œ íƒœìŠ¤í¬ë¥¼ ìƒì„±í•˜ì„¸ìš”:
            
            {prd_content}
            
            Task Masterì˜ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì„ ì ìš©í•˜ì—¬ ì²´ê³„ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ íƒœìŠ¤í¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.
            """
            
            result = generate_structured_response(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                response_schema=TASK_SCHEMA_EXAMPLE,
                temperature=0.3
            )
            
            # ê²°ê³¼ í›„ì²˜ë¦¬
            validated_result = validate_meeting_analysis(result)
            
            # TaskItem ê°ì²´ë¡œ ë³€í™˜
            task_items = []
            for i, item in enumerate(validated_result.get("action_items", [])):
                task_item = TaskItem(
                    id=i + 1,
                    title=item.get("task", f"Task {i+1}"),
                    description=item.get("task", ""),
                    priority=item.get("priority", "medium"),
                    assignee=item.get("assignee", "ë¯¸ì§€ì •"),
                    deadline=item.get("deadline", "ë¯¸ì •"),
                    complexity=calculate_task_complexity_advanced(
                        TaskItem(
                            id=i + 1,
                            title=item.get("task", ""),
                            description=item.get("task", ""),
                            priority=item.get("priority", "medium")
                        )
                    ),
                    status="pending"
                )
                task_items.append(task_item)
            
            # ì˜ì¡´ì„± ê²€ì¦
            task_items = validate_task_dependencies(task_items)
            
            # ìµœì¢… ê²°ê³¼ êµ¬ì„±
            stage3_result = MeetingAnalysisResult(
                summary=validated_result.get("summary", ""),
                action_items=task_items,
                decisions=validated_result.get("decisions", []),
                next_steps=validated_result.get("next_steps", []),
                key_points=validated_result.get("key_points", []),
                participants=validated_result.get("participants", []),
                follow_up=validated_result.get("follow_up", {}),
                metadata={
                    "processing_time": time.time() - start_time,
                    "total_tasks": len(task_items),
                    "process_type": "2-stage-task-master"
                }
            )
        
        total_time = time.time() - start_time
        
        return TwoStageAnalysisResponse(
            success=True,
            stage1_notion=stage1_result,
            stage2_prd=stage2_result,
            stage3_tasks=stage3_result,
            formatted_notion=format_notion_project(stage1_result) if stage1_result else None,
            formatted_prd=format_task_master_prd(stage2_result) if stage2_result else None,
            processing_time=total_time
        )
        
    except Exception as e:
        logger.error(f"âŒ 2-stage analysis error: {e}")
        return TwoStageAnalysisResponse(
            success=False,
            error=str(e)
        )

@app.post("/pipeline-final", response_model=Dict[str, Any])
async def final_pipeline(
    audio: UploadFile = File(...),
    generate_notion: bool = True,
    generate_tasks: bool = True,
    num_tasks: int = 5
):
    """ìµœì¢… ì „ì²´ íŒŒì´í”„ë¼ì¸: ìŒì„± â†’ ì „ì‚¬ â†’ 2ë‹¨ê³„ ë¶„ì„"""
    try:
        logger.info("ğŸš€ Starting final pipeline with 2-stage process...")
        
        start_time = time.time()
        
        # 1ë‹¨ê³„: ì „ì‚¬
        logger.info("ğŸ“ Step 1: Transcribing audio...")
        transcribe_result = await transcribe_audio(audio)
        if not transcribe_result.success:
            return {
                "success": False,
                "step": "transcription",
                "error": transcribe_result.error
            }
        
        # 2ë‹¨ê³„: 2ë‹¨ê³„ ë¶„ì„
        logger.info("ğŸ§  Step 2: Running 2-stage analysis...")
        analysis_request = TwoStageAnalysisRequest(
            transcript=transcribe_result.transcription["full_text"],
            generate_notion=generate_notion,
            generate_tasks=generate_tasks,
            num_tasks=num_tasks
        )
        analysis_result = await two_stage_analysis(analysis_request)
        
        total_time = time.time() - start_time
        
        return {
            "success": True,
            "step": "completed",
            "transcription": transcribe_result.transcription,
            "analysis": {
                "notion_project": analysis_result.stage1_notion,
                "task_master_prd": analysis_result.stage2_prd,
                "generated_tasks": analysis_result.stage3_tasks,
                "formatted_notion": analysis_result.formatted_notion,
                "formatted_prd": analysis_result.formatted_prd
            },
            "processing_time": total_time,
            "model_info": {
                "whisperx": "large-v3",
                "qwen3": "unsloth/Qwen3-14B-unsloth-bnb-4bit",
                "process": "2-stage-task-master"
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ Final pipeline error: {e}")
        return {
            "success": False,
            "step": "pipeline",
            "error": str(e)
        }

if __name__ == "__main__":
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    host = os.getenv("HOST", "0.0.0.0")
    port = int(os.getenv("PORT", "8000"))
    workers = int(os.getenv("WORKERS", "1"))
    
    logger.info(f"ğŸš€ Starting final server on {host}:{port}")
    
    uvicorn.run(
        "ai_server_final:app",
        host=host,
        port=port,
        workers=workers,
        reload=False,
        log_level="info"
    )